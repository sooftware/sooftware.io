{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/deepspeech/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Deep Speech: Scaling up end-to-end speech recognition</h1>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjNfMTQ1/MDAxNTgyMzkwNTAyMzI5.qYtgph7nxA4sOZlHd8-dw9dOmXeEZvz3zifBjyMYNaUg.uX_2ZheLYRPxPHJogipB50IrpYX7yYi5jNPWbWGv2sog.PNG.sooftware/image.png?type=w773\" alt=\"title\"></p>\n<p><a href=\"https://arxiv.org/pdf/1412.5567.pdf\">https://arxiv.org/pdf/1412.5567.pdf</a> (Awni Hannun et al. 2014)</p>\n<h2>Abstract</h2>\n<p>본 논문은 2014년도에 나온 논문이다. 논문을 읽어본 결과, 당시에는 음성인식에 End-to-End 방식의 딥러닝을 적용한 사례가 없던 모양이다. (또는 주목할만한 성과가 나오지 않았던 모양이다.) 본 논문에서는 End-to-End 방식으로 Switchboard Hub5’00 데이터셋에서 16.0%의 Error Rate를 기록하며 <strong>State-Of-The-Art</strong> (SOTA) 를 달성한 성과를 밝히고 있다. 기존 음성인식의 traditional한 방식은 전처리 과정이 상당히 많이 필요했지만, 이러한 과정 없이 데이터와 레이블만을 이용한 End-to-End 방식으로 이러한 성과를 냈음을 거듭 강조하고 있다. 그리고 이전 방식으로는 노이즈가 있는 환경에서 급격히 떨어졌는데 반하여, 본 논문 방식으로는 노이즈가 있는 환경에서도 좋은 성능을 기록했다고 한다. 즉, 기존 방식보다 더 간단하면서도 좋은 성능을 기록한 End-to-End 방식의 Speech-Recognition 모델을 소개하는 논문이다.</p>\n<h2>Introduction</h2>\n<p>Abstract에서 강조한 내용을 다시 한번 강조한다. 기존 traditional한 방식은 전문가들의 손이 많이 가야했다. (노이즈 필터링 등..) 하지만 그러한 노력에도 불구하고, 실제 노이즈가 낀 상황에서의 인식률은 좋지 못했다. 하지만 End-to-End 방식으로 이러한 2가지의 단점을 개선할 수 있다고 주장한다.</p>\n<p>물론 End-to-End 방식으로 가기 위해서는 몇가지 고려해야할 사항들이 있었지만, 본 논문에서는 기존 연구 결과들을 참고하여 End-to-End 방식을 시도할 수 있었다고 말한다. 그리고, 본 논문에서는 RNN 모델을 사용했다. 뒤에 더 자세히 설명하겠지만 본 논문에서는 학습 시간을 단축시키기 위해 많은 고려를 한 것으로 보인다.</p>\n<h2>RNN Training Setup</h2>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjNfMjcy/MDAxNTgyMzkwNTUyMDky.Btvd1sq4aVjkLzhMoXlSY5zrjNa8WyfqUAHi7NeFtcog.mFCsfZv-R3VDe_Gt-PZYr7R-Ymc3x84GWlcRX7-SZyIg.PNG.sooftware/image.png?type=w773\" alt=\"model\"></p>\n<p>해당 챕터에서는 자신들이 어떤 식으로 모델을 구성했는지에 대해 설명한다.<br>\n모델의 핵심은 RNN으로 구성되어 있으며, 트레이닝 셋은 ${(x_1, y_1), (x_2, y_2), … (x_t, y_t)}$ 와 같은 딕셔너리 형식으로 구성했다고 한다. (x는 스펙트로그램, y는 문자로 구성 )</p>\n<p>모델은 총 5개의 히든 레이어로 구성했다고 한다.\n논문을 읽으면서 모델 아키텍쳐가 상당히 특이하다고 생각했다.</p>\n<p>현재 내가 알고있는 방식과는 사뭇 다른 방식이였는데, 히든 레이어 중 1, 2, 3번째 레이어는 병렬적 (Parallel) 하게 처리하기 위해 서로 독립적으로 포워딩 된다고 한다. RNN 아키텍쳐를 사용하게 되면 이전 셀의 아웃풋이 필요하기 때문에 어쩔 수 없이 병렬처리의 한계점이 있기 때문에 학습 속도 개선을 위해 각 인풋을 독립적으로 처리했다고 한다.<br>\n여기서도 나는 본 논문이 학습 시간을 단축시키기 위해 상당히 노력했다는 인상을 받았다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjNfMTgz/MDAxNTgyMzkwNTIyMDQ0.Tq2hNX4U6a4mmT4V3f2sWemnruUnzCuLJYP5v5x8LlEg.8oF99BbkoyhYbmWQL0dD5LiWhNl61qe33P8OnGOG_Hsg.PNG.sooftware/image.png?type=w773\" alt=\"forward\"></p>\n<p>위의 수식을 통해 포워딩이 진행되는데, 여기서 g는 최소 0, 최대 20의 값을 가지는 ReLU 함수이다.<br>\n그리고 4번째 레이어에서는 Bidirectional-RNN으로 구성했다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjNfMjI0/MDAxNTgyMzkwNTMyNzM2.yNOOR59yjQmH7I1v8h-YtNJ-S8JSf5q9oSiMHn1JGcAg.mrjf5ofIgIa2CQBbzZZG0ncV7JxxxR9pRcXY5ZzaG00g.PNG.sooftware/image.png?type=w773\" alt=\"Bi-RNN\"></p>\n<p>(f)는 정방향 (forward), (b)는 역방향 (backward)을 표현한 것이다.<br>\n이때 주의할 점으로는, forward는 t = 1 에서 t = T 방향으로 흐르고, backward는 t = T에서 t = 1 방향으로 흐른다는 점이다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjVfMzkg/MDAxNTgyNjIxNjY0MzMw.CdMnrGnCNt8FrFb102hZFmRldRA1Xp_0kpWoFUbUv4sg.-VSEzem8ZH_3ry2W29awhSRASuz4Cb0MjIJnmwp6zcEg.PNG.sooftware/image.png?type=w773\" alt=\"5-layer\"></p>\n<p>그리고 마지막 5번째 레이어는 이렇게 forward, backward의 결과에 웨이트를 주고 1, 2, 3 번째 레이어와 동일한 ReLU를 활성화 함수로 사용했다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjNfODIg/MDAxNTgyMzkwNTQxMjI0.bn6ow6MHo58KaEaVu33JM8rzSvIYZqxjRpJMeKauUgsg.QumDhUt_Kx0Gqv_psT9xcHXmr44HLcGtELb7BkphlKsg.PNG.sooftware/image.png?type=w773\" alt=\"softmax\"></p>\n<p>그리고 이렇게 나온 결과는 Softmax 함수에 넣어서 최종적으로 Classfication을 진행한다. 또한 loss 계산시에는 CTC loss를 사용했다.</p>\n<p>여기서 또 특이했던 점으로, LSTM이 아닌 기본 RNN을 사용했다는 점이다.<br>\n그 이유로 본 논문에서는 LSTM의 단점은 메모리가 많이 소요되고, 학습이 오래걸린다는 점을 꼽았다.</p>\n<p>총 5개의 히든 레이어 중 실제 RNN 계층은 1개 뿐이라는 점과, LSTM이 아닌 RNN을 사용했다는 점이 인상깊었다.<br>\n최근 연구에서는 기본 RNN을 사용하는 모습을 거의 볼 수 없는데, 당시에는 아직 GPU의 성능이 그리 좋지 않던 때라 그런지 학습 속도에 대해 굉장히 고려를 많이한 모습이 보였다.</p>\n<h3>Regularization</h3>\n<p>본 논문은 학습 시, 드랍아웃 비율을 5 - 10%정도를 유지했다고 한다.<br>\n그리고 Spectrogram의 프레임 길이는 10ms, 포워딩은 5ms를 사용했다.<br>\n(해당 부분은 오류가 있을수도 있습니다)</p>\n<p>통상적으로 음성 인식에서 프레임 길이는 20 - 40ms를 사용하기 때문에 프레임 길이가 상당히 짧다고 생각했다.<br>\n해당 부분은 다른 이유가 있어서 짧게 한 건지, 당시에 프레임 길이에 대한 연구가 현재보다 덜 발달해서 그런 것인지는 확인을 해봐야 할 듯 하다.</p>\n<h3>Language Model</h3>\n<p>본 논문의 모델은 성능 테스트시에, 정확히 맞추거나 그럴싸하게 틀렸다고 한다.<br>\n<img src=\"https://postfiles.pstatic.net/MjAyMDAyMjNfOTEg/MDAxNTgyMzkwNTYzMjQ4.k-V_c6x3VVmGMV8CBoxQ4c-pDxUPDKVaYr0ibScfYmUg.cLZNNBPSqZJiRrMTRtX9PTyn6IMS0k4v5o2tp8Y4_4Qg.PNG.sooftware/image.png?type=w773\" alt=\"performance-test\"></p>\n<p>arther => are there, n tickets => any tickets 등 꽤나 말이 되도록 틀린 것을 볼 수 있다.<br>\n본 논문은 이보다 더 정확한 인식을 위하여 N-gram Language Model을 사용했다고 한다.</p>\n<p>매우 방대한 텍스트 Corpus로 N-gram language model을 학습시켰으며, 해당 언어 모델은 다음 공식에 사용됐다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjNfMTY1/MDAxNTgyMzkwNTcxMjQ0.cLKWNiiavUqrxHaBijz7yBT90bD-7QwqpA9My0qtc6Ug.67kMZDDcCx0HnvPj6hKRbAmAsuXhRhTT1Lihbx-TBKcg.PNG.sooftware/image.png?type=w773\" alt=\"scoring\"></p>\n<p>여기서 알파, 베타는 설정 가능한 파라미터이다.<br>\n본 논문에서는 성능을 향상시키기 위해 빔서치를 사용했는데, 이때 빔 사이즈를 1,000 - 8,000으로 상당히 크게 준 것을 볼 수 있었다.<br>\n이후에 나온 논문들을 봤을 때, 빔 사이즈 단위는 기껏 해봐야 수십 정도였는데 본 논문은 상당히 큰 빔 사이즈를 사용한 것을 볼 수 있었다.</p>\n<h2>Optimizations</h2>\n<p>해당 장에서는 어떻게 최적화를 했는지에 대해 설명하고 있다.<br>\n주로 빠른 학습을 시키기 위해 어떤 노력을 했는지를 설명했다.</p>\n<h3>Data Parallelism</h3>\n<p>데이터를 효과적으로 처리하기 위해 2-level data parallelism을 사용했다고 한다.</p>\n<p>미니배치 단위로 처리를 했는데, 이때 배치의 크기를 GPU 메모리 한계까지 사용했다고 한다.</p>\n<p>또한, 학습을 빨리하기 위해 NMT와 같은 Text-NLP에서 많이 사용되는, 길이 순으로 정렬해서 비슷한 길이끼리 배치로 묶었다고 한다. 이렇게 비슷한 길이끼리 배치로 묶게 되면, 배치 안에서 Max Length를 맞추기 위해 PAD token을 최소화 할 수 있다.</p>\n<h1></h1>\n<p>본 논문은 음성 인식의 기반을 다진 논문인지라 논문에 소개된 대부분의 내용이 음성인식 튜토리얼 내용과 비슷했습니다.<br>\n해당 논문 리뷰는 여기까지만 하겠습니다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Deep Speech: Scaling up end-to-end speech recognition"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjNfMTQ1/MDAxNTgyMzkwNTAyMzI5.qYtgph7nxA4sOZlHd8-dw9dOmXeEZvz3zifBjyMYNaUg.uX_2ZheLYRPxPHJogipB50IrpYX7yYi5jNPWbWGv2sog.PNG.sooftware/image.png?type=w773","alt":"title"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1412.5567.pdf"},"children":[{"type":"text","value":"https://arxiv.org/pdf/1412.5567.pdf"}]},{"type":"text","value":" (Awni Hannun et al. 2014)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 2014년도에 나온 논문이다. 논문을 읽어본 결과, 당시에는 음성인식에 End-to-End 방식의 딥러닝을 적용한 사례가 없던 모양이다. (또는 주목할만한 성과가 나오지 않았던 모양이다.) 본 논문에서는 End-to-End 방식으로 Switchboard Hub5’00 데이터셋에서 16.0%의 Error Rate를 기록하며 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"State-Of-The-Art"}]},{"type":"text","value":" (SOTA) 를 달성한 성과를 밝히고 있다. 기존 음성인식의 traditional한 방식은 전처리 과정이 상당히 많이 필요했지만, 이러한 과정 없이 데이터와 레이블만을 이용한 End-to-End 방식으로 이러한 성과를 냈음을 거듭 강조하고 있다. 그리고 이전 방식으로는 노이즈가 있는 환경에서 급격히 떨어졌는데 반하여, 본 논문 방식으로는 노이즈가 있는 환경에서도 좋은 성능을 기록했다고 한다. 즉, 기존 방식보다 더 간단하면서도 좋은 성능을 기록한 End-to-End 방식의 Speech-Recognition 모델을 소개하는 논문이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Introduction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Abstract에서 강조한 내용을 다시 한번 강조한다. 기존 traditional한 방식은 전문가들의 손이 많이 가야했다. (노이즈 필터링 등..) 하지만 그러한 노력에도 불구하고, 실제 노이즈가 낀 상황에서의 인식률은 좋지 못했다. 하지만 End-to-End 방식으로 이러한 2가지의 단점을 개선할 수 있다고 주장한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"물론 End-to-End 방식으로 가기 위해서는 몇가지 고려해야할 사항들이 있었지만, 본 논문에서는 기존 연구 결과들을 참고하여 End-to-End 방식을 시도할 수 있었다고 말한다. 그리고, 본 논문에서는 RNN 모델을 사용했다. 뒤에 더 자세히 설명하겠지만 본 논문에서는 학습 시간을 단축시키기 위해 많은 고려를 한 것으로 보인다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"RNN Training Setup"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjNfMjcy/MDAxNTgyMzkwNTUyMDky.Btvd1sq4aVjkLzhMoXlSY5zrjNa8WyfqUAHi7NeFtcog.mFCsfZv-R3VDe_Gt-PZYr7R-Ymc3x84GWlcRX7-SZyIg.PNG.sooftware/image.png?type=w773","alt":"model"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"해당 챕터에서는 자신들이 어떤 식으로 모델을 구성했는지에 대해 설명한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n모델의 핵심은 RNN으로 구성되어 있으며, 트레이닝 셋은 ${(x_1, y_1), (x_2, y_2), … (x_t, y_t)}$ 와 같은 딕셔너리 형식으로 구성했다고 한다. (x는 스펙트로그램, y는 문자로 구성 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델은 총 5개의 히든 레이어로 구성했다고 한다.\n논문을 읽으면서 모델 아키텍쳐가 상당히 특이하다고 생각했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"현재 내가 알고있는 방식과는 사뭇 다른 방식이였는데, 히든 레이어 중 1, 2, 3번째 레이어는 병렬적 (Parallel) 하게 처리하기 위해 서로 독립적으로 포워딩 된다고 한다. RNN 아키텍쳐를 사용하게 되면 이전 셀의 아웃풋이 필요하기 때문에 어쩔 수 없이 병렬처리의 한계점이 있기 때문에 학습 속도 개선을 위해 각 인풋을 독립적으로 처리했다고 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n여기서도 나는 본 논문이 학습 시간을 단축시키기 위해 상당히 노력했다는 인상을 받았다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjNfMTgz/MDAxNTgyMzkwNTIyMDQ0.Tq2hNX4U6a4mmT4V3f2sWemnruUnzCuLJYP5v5x8LlEg.8oF99BbkoyhYbmWQL0dD5LiWhNl61qe33P8OnGOG_Hsg.PNG.sooftware/image.png?type=w773","alt":"forward"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 수식을 통해 포워딩이 진행되는데, 여기서 g는 최소 0, 최대 20의 값을 가지는 ReLU 함수이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 4번째 레이어에서는 Bidirectional-RNN으로 구성했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjNfMjI0/MDAxNTgyMzkwNTMyNzM2.yNOOR59yjQmH7I1v8h-YtNJ-S8JSf5q9oSiMHn1JGcAg.mrjf5ofIgIa2CQBbzZZG0ncV7JxxxR9pRcXY5ZzaG00g.PNG.sooftware/image.png?type=w773","alt":"Bi-RNN"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"(f)는 정방향 (forward), (b)는 역방향 (backward)을 표현한 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이때 주의할 점으로는, forward는 t = 1 에서 t = T 방향으로 흐르고, backward는 t = T에서 t = 1 방향으로 흐른다는 점이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjVfMzkg/MDAxNTgyNjIxNjY0MzMw.CdMnrGnCNt8FrFb102hZFmRldRA1Xp_0kpWoFUbUv4sg.-VSEzem8ZH_3ry2W29awhSRASuz4Cb0MjIJnmwp6zcEg.PNG.sooftware/image.png?type=w773","alt":"5-layer"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 마지막 5번째 레이어는 이렇게 forward, backward의 결과에 웨이트를 주고 1, 2, 3 번째 레이어와 동일한 ReLU를 활성화 함수로 사용했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjNfODIg/MDAxNTgyMzkwNTQxMjI0.bn6ow6MHo58KaEaVu33JM8rzSvIYZqxjRpJMeKauUgsg.QumDhUt_Kx0Gqv_psT9xcHXmr44HLcGtELb7BkphlKsg.PNG.sooftware/image.png?type=w773","alt":"softmax"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 이렇게 나온 결과는 Softmax 함수에 넣어서 최종적으로 Classfication을 진행한다. 또한 loss 계산시에는 CTC loss를 사용했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 또 특이했던 점으로, LSTM이 아닌 기본 RNN을 사용했다는 점이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그 이유로 본 논문에서는 LSTM의 단점은 메모리가 많이 소요되고, 학습이 오래걸린다는 점을 꼽았다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"총 5개의 히든 레이어 중 실제 RNN 계층은 1개 뿐이라는 점과, LSTM이 아닌 RNN을 사용했다는 점이 인상깊었다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n최근 연구에서는 기본 RNN을 사용하는 모습을 거의 볼 수 없는데, 당시에는 아직 GPU의 성능이 그리 좋지 않던 때라 그런지 학습 속도에 대해 굉장히 고려를 많이한 모습이 보였다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Regularization"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 학습 시, 드랍아웃 비율을 5 - 10%정도를 유지했다고 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그리고 Spectrogram의 프레임 길이는 10ms, 포워딩은 5ms를 사용했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(해당 부분은 오류가 있을수도 있습니다)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"통상적으로 음성 인식에서 프레임 길이는 20 - 40ms를 사용하기 때문에 프레임 길이가 상당히 짧다고 생각했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해당 부분은 다른 이유가 있어서 짧게 한 건지, 당시에 프레임 길이에 대한 연구가 현재보다 덜 발달해서 그런 것인지는 확인을 해봐야 할 듯 하다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Language Model"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문의 모델은 성능 테스트시에, 정확히 맞추거나 그럴싸하게 틀렸다고 한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjNfOTEg/MDAxNTgyMzkwNTYzMjQ4.k-V_c6x3VVmGMV8CBoxQ4c-pDxUPDKVaYr0ibScfYmUg.cLZNNBPSqZJiRrMTRtX9PTyn6IMS0k4v5o2tp8Y4_4Qg.PNG.sooftware/image.png?type=w773","alt":"performance-test"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"arther => are there, n tickets => any tickets 등 꽤나 말이 되도록 틀린 것을 볼 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n본 논문은 이보다 더 정확한 인식을 위하여 N-gram Language Model을 사용했다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"매우 방대한 텍스트 Corpus로 N-gram language model을 학습시켰으며, 해당 언어 모델은 다음 공식에 사용됐다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjNfMTY1/MDAxNTgyMzkwNTcxMjQ0.cLKWNiiavUqrxHaBijz7yBT90bD-7QwqpA9My0qtc6Ug.67kMZDDcCx0HnvPj6hKRbAmAsuXhRhTT1Lihbx-TBKcg.PNG.sooftware/image.png?type=w773","alt":"scoring"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기서 알파, 베타는 설정 가능한 파라미터이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n본 논문에서는 성능을 향상시키기 위해 빔서치를 사용했는데, 이때 빔 사이즈를 1,000 - 8,000으로 상당히 크게 준 것을 볼 수 있었다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이후에 나온 논문들을 봤을 때, 빔 사이즈 단위는 기껏 해봐야 수십 정도였는데 본 논문은 상당히 큰 빔 사이즈를 사용한 것을 볼 수 있었다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Optimizations"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"해당 장에서는 어떻게 최적화를 했는지에 대해 설명하고 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n주로 빠른 학습을 시키기 위해 어떤 노력을 했는지를 설명했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Data Parallelism"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"데이터를 효과적으로 처리하기 위해 2-level data parallelism을 사용했다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"미니배치 단위로 처리를 했는데, 이때 배치의 크기를 GPU 메모리 한계까지 사용했다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한, 학습을 빨리하기 위해 NMT와 같은 Text-NLP에서 많이 사용되는, 길이 순으로 정렬해서 비슷한 길이끼리 배치로 묶었다고 한다. 이렇게 비슷한 길이끼리 배치로 묶게 되면, 배치 안에서 Max Length를 맞추기 위해 PAD token을 최소화 할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 음성 인식의 기반을 다진 논문인지라 논문에 소개된 대부분의 내용이 음성인식 튜토리얼 내용과 비슷했습니다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해당 논문 리뷰는 여기까지만 하겠습니다."}]}],"data":{"quirksMode":false}},"excerpt":"Deep Speech: Scaling up end-to-end speech recognition title https://arxiv.org/pdf/1412.5567.pdf (Awni Hannun et al. 2014) Abstract…","fields":{"readingTime":{"text":"10 min read"}},"frontmatter":{"title":"DeepSpeech Paper Review","userDate":"11 November 2019","date":"2019-11-11T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png","srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png 541w","sizes":"100vw"},"sources":[{"srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/1edf8/deepspeech.webp 541w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9149722735674677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"19ded62e-3e91-5733-9329-a1c7bdcf859b","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"35eadfc7-646b-5194-a711-ce20e840ba58","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"fd3185b8-63e4-5e3d-aeb3-e67ed1343af9","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"aad087b1-4b0f-5956-ab2f-d7ab33fdb8c4","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/deepspeech/","prev":{"excerpt":"「Listen, Attend and Spell」 Review title https://arxiv.org/abs/1508.01211  (William Chan et al. 2015)  Introduction 어텐션 기반 Seq2seq…","frontmatter":{"title":"Listen, Attend and Spell Paper Review","tags":["speech","paper"],"date":"2019-09-20T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAYCAYAAAD6S912AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC8UlEQVQ4y42VCU9iQRCE3///ORjQqOCRGBXiGcUT5HE/lENAwLN2vo7NrkbJklTmrq6p6X5EcaWiuFrVaDTScDhciMfHx7BvrNfXV728vFhr/ednG7+9vSna2NjQysqKNjc3lU6nlc3llAtIZ9JiDTBmfWkppb29PdXrNeULeV1cXKh4ealOp6N+v2/k0cHBgW3OZDLa39+3/tramvXX19fnWF1d1fb2tk5OTuxwq91SkiRKAtlwNNT7+7v4RcfHx8rn8zo6OjISxoeHh/O+A6Ld3V0Vi0X99vv4+FBElDiO1Wg0dHd3p3a7Ha5Ut7ler2e+DQYDQyX4/fDwYIdRBMG/MIWQXF9fB6JE5+fnRgzK5bKazeYcBLoMfrG2UOHOzo5SqZSy2ax5h5e5z4fBT8fW1patY0eStHVzc2OKuQ2CaqE1hYVCwUyHFI+Wl5cNHCaIgz0EwU8Ik06i6XSq2WyqSWhJGyMkEkZz9dPTU4tWDXkJ/PqAK7Neq9UWX7nVaun29lZXV1cGiJy02+1aigAeiOD+KN8fZP4oGI4XEPhD+KNACBEgGwhMEv9GaArv7+8tKiog4wDwuX5/YMSkDcSU4KJf5HXpwGjPPQ43GnU1W00RGJVuQbfXtUCA/QjgfPRTlOdQ7OPx2D4Y5VJJJbOioUocrKlVLektbUILIXsRYLX8E+FsNtNkMjGF9L0q/ucXEYGcQi4HIUIhBERkjNKnpyezgzFB/PPFGT5bgH6EVDfdW78uoI9SX8Mv8zCM8c3rnTn2Rv4ATOANifsdlFfl80MMCQ8EoSc/HBB/IWQjeYbZbIQEQEggPgyl8ECQeaqxjwrzlzdCrkP+8aXximETkSGk9UBUEOukD4SAPawxN1dIZBRQr1QI5JASiM2MXS11H3+mC0q5KkqZ4+EiSM7Oziw6ZBBxyPsQEowx4H8ElXwD3E8vS4JYLXMQOAlK6Ht9M+YWwOcgdJW0KKQfeWp4ruHp9/ZrGv2dY92BdeToH6F88yKPhpP1AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png","srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png 625w","sizes":"100vw"},"sources":[{"srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/09f70/las.webp 625w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.192}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"8 min read"},"layout":"","slug":"/las/"}},"next":{"excerpt":"SpecAugment: 「A Simple Data Augmentation Method for Automatic Speech Recognition」  Review title https://arxiv.org/abs/1904.08779 Abstract…","frontmatter":{"title":"SpecAugment Paper Review","tags":["speech","paper"],"date":"2020-01-12T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC/0lEQVQoz1WT309TZxzG3+mN280SExcXFxNvdONC/AP2D+xCE3exG4UNYeBoik4FrD9Q1DmGYawwTWMW3VAQSpWyGk0czDisECZQREpbBHranv5mFTaXbFI++562u+Ak33yf93ne87zv8805KptdxXieT2hcfzLBT8+n6Hrqodv9jI4pD/f6fudBzygdnkk6J0T3TtA1Ool9aJLu0XFuejz0DHtYevl3zkf9+3olB44fvoH6yISqOonaV4vaXY8qP8b29SXsWFeCOnIYtf8Qql6qQrSSY6IfRJ0wo0xfMj6j5w1XVrI5cLd3hNKrvZg7XFTdcmK+3kf1NRdfmW9ywdLFFw47pmt3qLjl4oCzj8rBTqrsDiru9/Dpz7fREstrDZ39Y+yzOSjtlxe6nVT+2E/FDacY3OGiqYsaq4PPevspczoo73Ri+t7F5523KbOLuRyux1+ujVzf0I3ac5Q3qs9KlNOsLzvFutIGNuw9TvFbB3nzk6MyDolafQJlbkDVWKTXoSwyHks9Y3OxvGE2m7/hiNtHs2uItsERrL+6aR9w0zYwjK3vCfb2h9juD3F58BGtg8O0PnJz5ekArY+HsI4I9/g3EplXecPXhRu2XvmFbaYWPmiysbOxjV21l3O148wPfFhiZZelheJTVopsTbzfbGXnkasUN35LUfslii5Y8QdTBcPCDBu/u8uG8nNsrmvh3ZpmtlZe4j2pTSYr2z9uYkvNebaYmnin9hs2WoSvaxb8NZsazvP26YtMaom1kTPLr1hIZNAX/yL8x59EUkvSlwkvSqWX0Q1Oeji5RGhRKpMhZOiZvP5PIalKpZJMT0+TTCZICw4F59HDGpFIiHhUz+GY9Hg0wvzcbA6nk3GikUhBl32xKKFQiFA4jIrH4/gDAeYXFvD6fPhnA8z4Zng2NSV4lmmvF5/fR0B4Q/MH/LyYe4F3xit4NsdrYhbUgkT0CCqdTuWcDUKPRtGFNMyNdSwWIyxaVHhN1sY+owc1Db8cous6/3/Hq6v5X/g/WhniovCPOZEAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/a5b61e2900f2360216061fe9e8f8f640/4df46/specaugment.png","srcSet":"/static/a5b61e2900f2360216061fe9e8f8f640/4df46/specaugment.png 620w","sizes":"100vw"},"sources":[{"srcSet":"/static/a5b61e2900f2360216061fe9e8f8f640/cd871/specaugment.webp 620w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5919354838709677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"20 min read"},"layout":"","slug":"/specaugment/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}