{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/electra/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<p>Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs.</p>\n<hr>\n<h1>ELECTRA</h1>\n<ul>\n<li><a href=\"https://openreview.net/forum?id=r1xMH1BtvB\">paper</a> / <a href=\"https://github.com/google-research/electra\">code</a></li>\n<li>ICLR 2020</li>\n</ul>\n<h2>Abstract</h2>\n<ul>\n<li>BERT에서 제안한 Masked Language Modeling(MLM)은 좋은 성능을 보여줬지만, 전체 데이터의 15%만을 마스킹해서 학습 효율 측면에서 좋지 않음.</li>\n<li>ELECTRA는 <strong>모델의 성능</strong>과 함께 <strong>학습의 효율성</strong>도 개선할 수 있는 방법을 제안함.</li>\n<li>Replaced Token Detection(RTD)라는 새로운 pre-training 태스크 제안.</li>\n<li>ELECTRA의 장점은 Small 모델에서 두드러짐. 1개의 GPU로 4일만 학습한 모델로 계산량이 30배인 GPT를 능가.</li>\n</ul>\n<h2>Replaced Token Detection (RTD)</h2>\n<img src=\"https://blog.pingpong.us/images/2020.05.08.electra/figure2.png\" width=\"700\">\n<ul>\n<li>Generator: BERT의 MLM\n<ul>\n<li>입력된 인풋 중 15%의 토큰을 [MASK]로 가림</li>\n<li>[MASK]로 가려진 인풋의 원래 토큰을 예측</li>\n</ul>\n</li>\n<li>Discriminator\n<ul>\n<li>입력 토큰 시퀀스에 대해서 각 토큰이 original인지 replaced인지 이진 분류로 학습</li>\n<li>학습 과정\n<ol>\n<li>Generator에서 마스킹 된 입력 토큰들을 예측</li>\n<li>마스킹할 위치의 토큰에 대해 generator가 예측했던 softmax 분포에서 높은 순위의 토큰 중 하나로 치환 (1위: cooked, 2위: ate, 3위: … 이였으면 MLM은 cooked를 선택하지만 해당 과정에서 ate를 가져오는 방식)</li>\n</ol>\n<ul>\n<li>Original input : [the, chef, cooked, the, meal]</li>\n<li>Input for generator : [[MASK], chef, [MASK], the, meal]</li>\n<li>Input for discriminator : [the, chef, ate, the, meal]</li>\n</ul>\n<ol start=\"3\">\n<li>치환된 입력에 대해 discriminator는 원래 입력과 동일한지 치환된 것인지를 이진 분류로 예측</li>\n</ol>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Training Algorithm</h2>\n<ul>\n<li>Jointly 학습\n<ul>\n<li>Generator와 Discriminator를 동시에 학습시키는 방법</li>\n</ul>\n</li>\n<li>Two-stage 학습\n<ol>\n<li>Generator만 MLM으로 N 스텝동안 학습</li>\n<li>뒤이어 해당 모델을 Discriminator로 N 스텝동안 학습시키는 방식 (이때 Generator의 웨이트는 고정)</li>\n</ol>\n</li>\n<li>Adversarial 학습\n<ul>\n<li>Adversarial training을 모사해서 학습시키는 방식 (jointly보다 좋지 않아서 자세히 안 봤습니다.)</li>\n</ul>\n</li>\n</ul>\n<h2>Result</h2>\n<h3>Performance &#x26; Efficiency</h3>\n<img src=\"https://blog.pingpong.us/images/2020.05.08.electra/figure1.png\" width=\"700\">\n<ul>\n<li>다른 모델들에 비해 매우 빠르게 성능이 향상되는 것을 볼 수 있음</li>\n<li>그럼에도 불구하고, 기존 BERT보다 더 좋은 성느을 기록함.</li>\n</ul>\n<h3>Weight Sharing</h3>\n<ul>\n<li>Generator와 discriminator는 모두 트랜스포머의 인코더 구조.</li>\n<li>그렇기 때문에 3가지 선택사항이 생김.\n<ol>\n<li>Generator, Discriminator가 서로 독립적으로 학습 (83.5)</li>\n<li>임베딩 레이어의 웨이트만 공유 (84.3)</li>\n<li>모든 레이어의 웨이트를 공유 (84.4)</li>\n</ol>\n</li>\n<li>결과적으로 모든 웨이트를 공유하는 것이 가장 좋은 성능을 보임.</li>\n</ul>\n<h3>Training Algorithm</h3>\n<img src=\"https://blog.pingpong.us/images/2020.05.08.electra/figure3.png\" width=\"700\">\n<ul>\n<li>Jointly 방식이 가장 성능이 좋았음 (왼쪽은 discriminator와 generator의 사이즈에 따른 실험)</li>\n</ul>\n<h2>Conclusion</h2>\n<ul>\n<li>더 효율적이고 효과도 좋은 Replaced Token Detection (RTD) 제안</li>\n<li>메인 아이디어는 Generator가 만들어 낸 질 좋은 negative sample로 학습함으로써 더 적은 리소스로 모델을 더욱 견고하게 만드는 것.</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"ELECTRA"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://openreview.net/forum?id=r1xMH1BtvB"},"children":[{"type":"text","value":"paper"}]},{"type":"text","value":" / "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/google-research/electra"},"children":[{"type":"text","value":"code"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"ICLR 2020"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT에서 제안한 Masked Language Modeling(MLM)은 좋은 성능을 보여줬지만, 전체 데이터의 15%만을 마스킹해서 학습 효율 측면에서 좋지 않음."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"ELECTRA는 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"모델의 성능"}]},{"type":"text","value":"과 함께 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"학습의 효율성"}]},{"type":"text","value":"도 개선할 수 있는 방법을 제안함."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Replaced Token Detection(RTD)라는 새로운 pre-training 태스크 제안."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"ELECTRA의 장점은 Small 모델에서 두드러짐. 1개의 GPU로 4일만 학습한 모델로 계산량이 30배인 GPT를 능가."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Replaced Token Detection (RTD)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://blog.pingpong.us/images/2020.05.08.electra/figure2.png","width":700},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Generator: BERT의 MLM\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"입력된 인풋 중 15%의 토큰을 [MASK]로 가림"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"[MASK]로 가려진 인풋의 원래 토큰을 예측"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Discriminator\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"입력 토큰 시퀀스에 대해서 각 토큰이 original인지 replaced인지 이진 분류로 학습"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"학습 과정\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Generator에서 마스킹 된 입력 토큰들을 예측"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"마스킹할 위치의 토큰에 대해 generator가 예측했던 softmax 분포에서 높은 순위의 토큰 중 하나로 치환 (1위: cooked, 2위: ate, 3위: … 이였으면 MLM은 cooked를 선택하지만 해당 과정에서 ate를 가져오는 방식)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Original input : [the, chef, cooked, the, meal]"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Input for generator : [[MASK], chef, [MASK], the, meal]"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Input for discriminator : [the, chef, ate, the, meal]"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{"start":3},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"치환된 입력에 대해 discriminator는 원래 입력과 동일한지 치환된 것인지를 이진 분류로 예측"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Training Algorithm"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Jointly 학습\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Generator와 Discriminator를 동시에 학습시키는 방법"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Two-stage 학습\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Generator만 MLM으로 N 스텝동안 학습"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"뒤이어 해당 모델을 Discriminator로 N 스텝동안 학습시키는 방식 (이때 Generator의 웨이트는 고정)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Adversarial 학습\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Adversarial training을 모사해서 학습시키는 방식 (jointly보다 좋지 않아서 자세히 안 봤습니다.)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Result"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Performance & Efficiency"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://blog.pingpong.us/images/2020.05.08.electra/figure1.png","width":700},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"다른 모델들에 비해 매우 빠르게 성능이 향상되는 것을 볼 수 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"그럼에도 불구하고, 기존 BERT보다 더 좋은 성느을 기록함."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Weight Sharing"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Generator와 discriminator는 모두 트랜스포머의 인코더 구조."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"그렇기 때문에 3가지 선택사항이 생김.\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Generator, Discriminator가 서로 독립적으로 학습 (83.5)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"임베딩 레이어의 웨이트만 공유 (84.3)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"모든 레이어의 웨이트를 공유 (84.4)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결과적으로 모든 웨이트를 공유하는 것이 가장 좋은 성능을 보임."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Training Algorithm"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://blog.pingpong.us/images/2020.05.08.electra/figure3.png","width":700},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Jointly 방식이 가장 성능이 좋았음 (왼쪽은 discriminator와 generator의 사이즈에 따른 실험)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conclusion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"더 효율적이고 효과도 좋은 Replaced Token Detection (RTD) 제안"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"메인 아이디어는 Generator가 만들어 낸 질 좋은 negative sample로 학습함으로써 더 적은 리소스로 모델을 더욱 견고하게 만드는 것."}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"Electra Paper Review","userDate":"23 September 2020","date":"2020-09-23T07:03:47.149Z","tags":["nlp","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png","srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/c68af/electra.png 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/87f65/electra.png 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/d464a/electra.png 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/9fb02/electra.webp 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/cd76f/electra.webp 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/b7397/electra.webp 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/507b8/electra.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4479166666666667}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":11,"edges":[{"node":{"id":"f8857a8d-9121-5e23-91bc-3fbe4f090418","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"3b2d6ea5-69c9-5459-815d-b59e3dbb34f8","excerpt":"Tokenization 문장에서 의미있는 단위로 나누는 작업을 라고 한다. 문자 단위 토큰화 문자 단위로 토큰화를 하는 것이다. 한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 1…","frontmatter":{"title":"Tokenizer","date":"2021-09-13T23:46:37.121Z"},"fields":{"readingTime":{"text":"10 min read"},"slug":"/tokenizer/"}}},{"node":{"id":"476f2558-a27e-5d36-bc5e-d8f8a5ca05e0","excerpt":"정규 표현식 정규표현식(regular expression)은 일종의 문자를 표현하는 공식으로, 특정 규칙이 있는 문자열 집합을 추출할 때 자주 사용되는 기법입니다. 주로 Prograaming Language나 Text Editor…","frontmatter":{"title":"정규표현식 (regex)","date":"2021-09-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"30 min read"},"slug":"/regex/"}}},{"node":{"id":"eaa0dc8a-25ba-5f4e-9a44-0022cc38776d","excerpt":"최근 NLP 토크나이저를 만드는데 가장 많이 사용되는  라이브러와 실제 사용이 가장 많이 되는  라이브러리로의 변환에 대한 코드를 담고 있습니다. 해당 내용은  버젼에서 수행되었습니다. Train 아래 코드는 wordpiece, char-bpe…","frontmatter":{"title":"Hugging Face Tokenizers","date":"2021-08-11T15:11:55.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/tokenizers/"}}},{"node":{"id":"261863c4-58d3-5883-b410-c0a7cc3d8319","excerpt":"GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…","frontmatter":{"title":"P-Tuning Paper Review","date":"2021-05-13T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/p_tuning/"}}}]}},"pageContext":{"slug":"/electra/","prev":{"excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","tags":["speech","paper"],"date":"2020-09-12T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACV0lEQVQoz22S6U8TURTF+ycqQUOQlkJAgygBEYVQFELZbGLiJ/8AEv1mNNEYazQhwabsSy1LF+xC95YuM21n6/x8M/2oL7mT985958x9514H/1mm2RFfK0zMTgfTUKiUizSbMqrS5vo6TbGQIZ/PinOry+kYgmfisMilUolEIkG73Rak7oWyBLmakDUhnGxRlTTklkalKtv5WM5AFf+0Il7odEVFOBQhkkwmicfjlCsVFEVFb8Qop3dJRwO0iwckLn9RzexxI7BMdAu1ckIsvIVcOLIjFQmg3Zyiay0c/z4X2uFlOO5D3XNiHDjhZJBWsJ/CjzsUf96ltt1nY+axC/PIJfYu9EM3Sj3ZFdQ0TVSm2B5Ygq34Jkp4ifqJF/ViHSnkpfl7ldL+EqltD3JoRZy9aJcbKOdrtMIrqJc+VDmPw/Lt/PyM01CIYrFIrVZD0w272shVmLrcQGlLXFyd2difVIxcKQ+GRiRxga5rwqos6ULazjskSSISiRCNRcnmsjQakp0IJxpsh3Jip/Nlp0C13hAkGf9+xnoT/oMK14UbarLK52CGjtFtpqPZbNpNSaVSdlOslSrr+L7Baz98PYX5j/DhEN5+b+J5XxaYyYLANgPwLggLn2AvYTXAEB4K0zpi1nRdt8Oau8DOIeu+N3hXfUzOvGBpeZVHU3MMjYwzMTlNv3sMl3uE++NTOIfHGBwaZdazLPhCUNNUew7z+TzVahXDMNjdCfJqY40FzzxzszMsLr5k4vE4zoF7TD+Zwj04QG9vDw/HHjA6MkzP7Vs8f/bULugvq76654K/U9cAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/038c6/wav2vec2.png","srcSet":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/e6cc4/wav2vec2.png 750w,\n/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/038c6/wav2vec2.png 842w","sizes":"100vw"},"sources":[{"srcSet":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/79bef/wav2vec2.webp 750w,\n/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/daf06/wav2vec2.webp 842w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5106888361045131}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"5 min read"},"layout":"","slug":"/wav2vec2/"}},"next":{"excerpt":"RoBERTa paper / code Abstract BERT를 제대로 학습시키는 법을 제안 BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음 BERT…","frontmatter":{"title":"RoBERTa Paper Review","tags":["nlp","paper"],"date":"2020-10-11T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAADh0lEQVQ4y61USY/iRhTmhybXSJGSa/IDkiiHUTQdzaGTQzZpcunTqGemmW4aMOANDF4w2CwNGDfQzb7ZZufLc9EdjeaalPT0Xr3vq6+eq1wvEgQB3HYbs9kM4Tgej//JItPJGJwkwel18X+MSLA/oHObwFgvwtsesFz4WC49LJZL8mEcYOkFzC9YvKJ49YQ9c1aE+VivN4gUag5iL14i/vtrXMplRNMKPgh1xLIuoryNq4yCd6kC+QKuRR1vuTzLRXkLN7LLfJRXcJnUkdOqiOS1IpIVAXFTAKelUGjy4PQOlAbAl1oQSjdIFJKIF65xm/+AmBKDUueQNquQKgfiNMCbUcRVDXnTRUQ167iWG0jk20hq97SwSaQpsrUNUsUhYrk6CTm4ydYRFauIU5xQW0gTFnIy5gi3CnEKXRTMFiJ6uQnBXhG4JttCrq4h2h6b5xs7aA6gtQE9NBeUOxK2g0y4VAmIv2LCoUa+2DgJpksLqmqGTGmOlDGGYC3xhqvh1Z9XeHF+gVd/vMfPv13il79juBLbDM+YM/DlJYtFi9ZbPgk2T4K85dFOAatKqvjMUsYEb5JVXFwX8TbdYEJXgkPnO6Lz3bGqwuok24dc8Z4EqUKNBJPGHJwxJZvR4Y5JbEq7EqnsUQU+WUCxzxalzAWS+oRxT5wl5ejLTI8uhSo0jByama/hSN/ijv8G48KXkLUS+BqdnZpEL/MZqtkzhj3KX8CWznAvfoVG9kdkKsSh2++Ln6Os/AWl1KNb1lQoiZdQuHPo/K+oSD8hrd4hZQOyqqLIfQ9FvICWOUfu5gdk+QuY/BlU6TVS5R0kJQuT+w6i+B4y3Vyk+zBCrtSBaj+QPdIuXRQr9zCsBnS7w+Z69ZFhBasHvTZAvky/iEU8qwmj2kWe8vmSC7dDFR53W/RVBcOSgWHZQFcWsA989i63szGcTBLjehUjwjuEzVt36GRFDO0S46zHQzjpJOZu6/SW156PtiCgnc3hPpdDRxAxGwxxJHDe6+HuNg6XXkGXMCedRkfX4RL/wTBwCDndLmqxGIbV2kkwbDn7wwEH8pvtFj61M9/3Eba11WrFmsRuv2f4s63WawSEhbyQs/Q8lmeCn7af+XyO0WjE+mO44PhE/Hh4JDCZTDAYDDCdTrHZbP7tpZGPm2M4wmZrWRbq9Tr6/cEp/wmn3+/Dtm3UajU4jsO+5lnwH3sJDqGy7C/mAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png","srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/5dae1/roberta.png 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/35cd7/roberta.png 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png 1134w","sizes":"100vw"},"sources":[{"srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/76436/roberta.webp 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/ce7b4/roberta.webp 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/03f03/roberta.webp 1134w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8835978835978836}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/roberta/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}