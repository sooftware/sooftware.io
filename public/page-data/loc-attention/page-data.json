{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/loc-attention/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Attention-Based Models for Speech Recognition Paper Review</h1>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMjAw/MDAxNTgxODY4NTQ4MDYw.h3fmR1DnirrDCC-wkSrHptgHrlPX2GPQsnIhI1ulGecg.bIlam2xHjyx9Fdet1be9FvurzHMMNIfxLa2_cY2hapsg.PNG.sooftware/image.png?type=w773\" alt=\"title\"></p>\n<p><a href=\"http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf\">http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf</a></p>\n<h2>Introduction</h2>\n<p>본 논문에서는 최근 도입된 (당시는 최근이였음) 어텐션 매커니즘이 여러 분야에서 좋은 성능을 보였지만, 음성 인식 분야의 특성을 충분히 반영한 매커니즘은 없었다고 주장한다.</p>\n<p>음성 인식은 NMT 등의 task에 비해 상당히 긴 input sequence를 가진다.<br>\n단어 단위로 수개에서 수십개의 인풋을 가지는 NMT에 비해 음성 인식에서는 20 ~ 40ms로 자른 프레임들이 수백~수천개의 인풋으로 들어가게 된다</p>\n<p>본 논문은 이러한 음성 인식 분야의 특성에 맞게 새로운 어텐션 매커니즘을 제안한다.</p>\n<h1></h1>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMjAz/MDAxNTgxODY5MjE1NTMx.xNh4qwldRqCtKonZGRH8c1E0yk22yEvYOHIlwAPNbzcg.nZZBOnwPrdlpsKWSqnqVqXslTHfw8noeqGN59MPh2Rwg.PNG.sooftware/image.png?type=w773\" alt=\"2paper\"></p>\n<p>참고로 본 논문은 2015년 당시 음성 인식 분야에서 “Listen, Attend and Spell” 논문과 함께 Innovation이라고 불릴만큼 큰 파장을 준 논문이였다.\n기존 CTC 방식이 압도적이였던 당시에, End-to-End 방식의 포문을 열어준 논문이였기 때문이다.</p>\n<h1></h1>\n<h2>General Framework</h2>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfNiAg/MDAxNTgxODY4NTU3Nzg4.YCElS0j5R6lXK_ryX4m_jkAzjFrZPsEekbDna6LCr4Qg.RxewbBWd3eAVDFcasyCT8VafU2oDZqo_yIyh6RNosbIg.PNG.sooftware/image.png?type=w773\" alt=\"base_attention\"></p>\n<p>기본적인 어텐션에 대한 큰 그림이다.<br>\n(본 논문에서는 α는 alignment, g는 glimpse라고 칭함 )</p>\n<p>어떠한 매커니즘을 거쳐서 alignment (α) 를 구하고 나면, alignment와 인코더의 아웃풋들을 곱해서 glimpse를 구한다.</p>\n<h1></h1>\n<h3>Attention의 개념</h3>\n<p>본 논문에서는 나와 있지 않지만 간단하게 개념을 정리하고 가자면, “alignment는 어떤 인코더를 고려해야 할까?”를 수치화해준 벡터이고, glimpse는 수치화 된 alignment와 인코더의 아웃풋들을 각각 곱해서 현재 디코딩에 필요한 인코더의 정보를 압축한 벡터이다. 그리고 glimpse와 디코더의 아웃풋을 고려해서 현재 스텝의 값을 예측한다.</p>\n<h1></h1>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMTIw/MDAxNTgxODY4NTY1MDcy.94IsqEbUQd9q_A2RBdgwWIzrN2ngrPqGdAJAaUVOQDog.ScXnpprsxqBELeteJFikGmXv3RoH2DcxECcDxSkoRe8g.PNG.sooftware/image.png?type=w773\" alt=\"alignment\"></p>\n<p>그럼 alignment는 어떻게 구하지?<br>\n란 물음에 답해주는 부분이다.</p>\n<p>특정 방식으로 Score를 구한 뒤, 해당 점수를 Softmax 함수에 넣어서 전체 값을 0~1의 값으로, 전체 합을 1로 만들어 준다.<br>\n=> 각 인코더 아웃풋을 얼마씩 참고할지를 수치화하는 것이다.</p>\n<p>그럼 Score를 구하는 특정 방식은 무엇이냐??</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfODUg/MDAxNTgxODY4NTY5Mzgy.Nbp_BKh56TwrUeOA9GuBNny_OwX2ZfRzbHYz-Oag4dYg.BGNBoYP9qwvFL26yr5AZhQ3RlE17GAER9pBnA7dblhEg.PNG.sooftware/image.png?type=w773\" alt=\"score_func\"></p>\n<p>어텐션 스코어를 구하는 방법은 위와 같이 다양하다. 사실 위는 정말 몇 개만 뽑아온 것이다.</p>\n<p>어텐션 매커니즘의 종류는 이 스코어 함수가 무엇이냐에 따라 달라진다.</p>\n<p>그리고, 본 논문은 새로운 “스코어 함수”를 제안한 논문인 것이다.</p>\n<h1></h1>\n<p>본 논문에서는 2가지 어텐션 방식에 주목했다.</p>\n<ol>\n<li>Content-Based Attention</li>\n<li>Location-Based Attention</li>\n</ol>\n<h2>Content-Based Attention</h2>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfNjEg/MDAxNTgxODY4NTkxNzY4.pP5KJnqRAe0qT2fp90t59QJnh7q1cRjETJhDhEdxr4Mg.wK2gs1u_poWfTkwxjPPStLauOB_jVN6Itkz6wrGQHnYg.PNG.sooftware/image.png?type=w773\" alt=\"content-based\"></p>\n<p>아마 어텐션을 처음 공부할 때에 대부분 Dot-Product Attention으로 배웠을 것이다.</p>\n<p>해당 스텝의 디코더의 출력과 인코더의 모든 출력들을 내적하여 어텐션 스코어를 구하는 방식이다.</p>\n<p>Content-Based Attention은 Dot-Product보다 조금 더 복잡한 수식으로 점수를 낸다.</p>\n<p>단순한 내적이 아닌, 해당 스텝의 디코더의 출력과 인코더의 모든 출력들에 웨이트를 준다.</p>\n<p>그리고 편향 및 Hyperbolic tangent를 걸어주고, 마지막으로 웨이트를 다시 걸어준다.</p>\n<p>Dot-Product Attention에 비해서는 진보된 방법이지만, Content-Based 방식의 문제점은 시퀀스에서의 자신의 위치에 상관없이 스코어링을 한다는 점이다.<br>\n이를 “similar speech fragments” 문제라고 한다고 한다.</p>\n<h2>Location-Based Attention</h2>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMTcg/MDAxNTgxODY4NjAyMzU5.csBoosFKJVgVBeHsRCD3fSHokS4MYajHh4lssnQ2bHMg.9P1RL402Y4qyMp6Vfex01uiBTWmUxgAu9zpEArFFvi8g.PNG.sooftware/image.png?type=w773\" alt=\"location-based\"></p>\n<p>그럼 이번에는 Location-Based 방식을 살펴보자.<br>\n이 방식은 alignment 계산시, 해당 스텝 디코어의 출력과, 이전 alignment를 고려해줌으로써, 현재 시퀀스에서 어느 위치인지를 알 수 있게끔 해주는 방식이다.</p>\n<p>하지만 이 방식은 인코더의 아웃풋을 전혀 고려하지 않고, 디코더의 아웃풋만을 가지고 예측하기 때문에 분명한 한계점이 존재한다.</p>\n<h2>Hybrid Attention</h2>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMTQz/MDAxNTgxODY4NjE1Mzc3.SoD_ilkO_r7XfC-kNb36iNq7wR78iAGl7_HBfznB0VIg.czrqHp7DSqJfOuccfHUq91HPXcdpu2MouEDDg4jWfVog.PNG.sooftware/image.png?type=w773\" alt=\"hybrid\"></p>\n<p>본 논문은 이러한 2 방식의 어텐션을 적절히 결합한 음성 인식용 어텐션을 제안한다.</p>\n<p>( 해당 어텐션을 Hybrid, Location-Aware, Location-Sensitive 등 여러 이름으로 불린다 )</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMjg2/MDAxNTgxODY4NjI5MzA4.IHXwfdQs3EC_YQ4mafpM0XLYbDtcPTg2eGjFDjQnTX8g.Dl63xBYsbahH4MyrwGjmbQgI57WTAbXXsAFjFgHhEJ0g.PNG.sooftware/image.png?type=w773\" alt=\"hybrid-attention\"></p>\n<p>기존 Content-Based 방식에서 약간의 수식만이 추가됐을 뿐이다.</p>\n<p>기존 Content-Based 방식에서 이전 스텝의 alignment를 고려해준다.</p>\n<p>이때 이전 alignment에 웨이트를 주기 이전에, Convolution으로 1xC의 형상에서 KxC의 형상으로 늘려준다. (C: Classfication Number)</p>\n<p>그리고 해당 행렬에 웨이트를 주어서 Content + location 방식을 완성한다.</p>\n<h2>3 Potential Issue</h2>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMTI0/MDAxNTgxODcxNTc5MzU0.9AzhrTq5SoHmgIIrCcmYBB837p3yL8K09QVH3P6gDrkg.YWKb3qLysPBy6oGkm4MBQa1ty9u7-ktEmPgyjRbee7gg.PNG.sooftware/image.png?type=w773\" alt=\"Eq6\"></p>\n<p>앞에서 살펴봤던 위의 수식에는 3가지의 이슈가 있다.</p>\n<ol>\n<li>인풋 시퀀스가 길다면, glimpse에는 노이즈가 섞여있을 가능성이 크다.</li>\n</ol>\n<p>만약 인풋 시퀀스가 길다면, 어떤 시점 t에서 멀리 떨어져 있는 t + k라는 시점에서의 음성과는 서로 관련이 없을 것이다. 하지만 Softmax 함수 특성상, 모든 인풋들에 값을 부여한다. 이러한 Softmax의 특성에 의해 많은 관련없는(irrelevant) 인코더의 출력들이 고려될 것이다. 이는 Noise로 작용된다.</p>\n<ol start=\"2\">\n<li>시간 복잡도가 크다.</li>\n</ol>\n<p>인풋 시퀀스의 길이가 L이라고 할 때, 디코더는 매 타임 스텝마다 이 L개의 frame을 고려해주어야 한다. 그리고, 디코딩 길이를 T라 할 때, 위의 과정을 T만큼 반복하게 된다.  이는 O(LT) 라는 높은 시간 복잡도를 만들게 된다.</p>\n<ol start=\"3\">\n<li>Softmax 함수는 Single Vector에만 집중 (focus) 하는 경향이 있다.</li>\n</ol>\n<p>이러한 경향은 top-score를 받은 여러 프레임을 고려할 수 없게 한다.</p>\n<h3><strong>Sharpening</strong> &#x26; <strong>Windowing</strong></h3>\n<p>본 논문은 위의 문제를 간단하게 해결하기 위해 “Sharpening”이라는 개념의 제안했다. Softmax 수식을 약간 수정하는 것이다.<br>\n<img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMjMz/MDAxNTgxODcyNDMyNjU0.drdyx8zV1DOSV-6rezOYqgDQmcFpiqe4U04da8kcvWcg.w_vqXCxu1nd-uCZ-VW395v4AX76Z5hNuK1HYh15c8iMg.PNG.sooftware/image.png?type=w773\" alt=\"sharpening\"><br>\nwhen, β > 1</p>\n<p>본 논문에서는 inverse temperature를 걸어준다고 표현했다.<br>\n위의 수식이 왜 1번 문제를 해결해 주는지에 대해서는 아직 이해를 하지 못하였다.</p>\n<p>그리고 본 논문은 위의 방식이거나, top-k개의 프레임만을 뽑아서 re-normalization을 해주는 방식으로도 해결 가능하다고 말한다.<br>\n하지만, 위의 2 방식 모두 2번째 시간복잡도의 문제는 해결하지 못했으며, 2번째 방법의 경우는 오히려 시간 복잡도를 더 늘리게 된다.</p>\n<p>그리고 Windowing이라는 방법이 나오게 되는데, 이전 alignment의 중간값(median)을 기준으로 윈도우 크기 만큼만 고려해주는 방식이다. 해당 방법은 O(L+T)로 시간 복잡도를 낮춰준다.</p>\n<h1></h1>\n<p>Sharpening은 long-utterance (긴 발화)에서의 퍼포먼스는 개선했지만, 전체적인 퍼포먼스면에서는 좋지 못한 결과로 이어졌다.<br>\n(짧은 발화에서는 퍼포먼스가 별로였다)<br>\n하지만 해당 실험은 최상위 점수를 받은 프레임들을 선택하여 집계하는 방식이 좋을 것이라는 가정을 하도록 만들었다고 한다.</p>\n<h3><strong>Smoothing</strong></h3>\n<p>그래서 나오게 된 방법이 Smoothing 방법이다.<br>\n<img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfNjIg/MDAxNTgxODY4NjM0ODMz.jYTFOEd93R5-IagaKWOyTg3i07Pk7Rwdl1LxsbZAPS8g.r7A0IGeNFdXeSUUl9EB_QWM6EOzb_6N1eHuDGz8_JrIg.PNG.sooftware/image.png?type=w773\" alt=\"smoothing\"></p>\n<p>위의 식처럼 기존 Softmax 식에 Sigmoid를 추가해준 방식이다.<br>\nSigmoid로 Top-k frame과 아닌 frame들을 구분해주는 방식이라고 나는 이해했다.<br>\n이러한 방식은 다양성을 가져온다고 본 논문은 말한다.</p>\n<h2>Result</h2>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMTdfMjkw/MDAxNTgxODY4NjcwNDM4.hknmkkv3qrF8llD9vB2AUALkhuYkUHcuNewXoHv-R-gg.vPQyt_knw2_429fP4jUbdUFU4aMsyexsNCQ7iJi4xb0g.PNG.sooftware/image.png?type=w773\" alt=\"result\"></p>\n<p>본 논문에서 진행한 실험의 결과이다.<br>\n기본 모델보다는 Convolution을 적용한 모델이 더 좋은 결과를 내었고,<br>\nSmoothing까지 적용한 모델이 최상의 성적을 내었다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Attention-Based Models for Speech Recognition Paper Review"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMjAw/MDAxNTgxODY4NTQ4MDYw.h3fmR1DnirrDCC-wkSrHptgHrlPX2GPQsnIhI1ulGecg.bIlam2xHjyx9Fdet1be9FvurzHMMNIfxLa2_cY2hapsg.PNG.sooftware/image.png?type=w773","alt":"title"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf"},"children":[{"type":"text","value":"http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Introduction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 최근 도입된 (당시는 최근이였음) 어텐션 매커니즘이 여러 분야에서 좋은 성능을 보였지만, 음성 인식 분야의 특성을 충분히 반영한 매커니즘은 없었다고 주장한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"음성 인식은 NMT 등의 task에 비해 상당히 긴 input sequence를 가진다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n단어 단위로 수개에서 수십개의 인풋을 가지는 NMT에 비해 음성 인식에서는 20 ~ 40ms로 자른 프레임들이 수백~수천개의 인풋으로 들어가게 된다"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 이러한 음성 인식 분야의 특성에 맞게 새로운 어텐션 매커니즘을 제안한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMjAz/MDAxNTgxODY5MjE1NTMx.xNh4qwldRqCtKonZGRH8c1E0yk22yEvYOHIlwAPNbzcg.nZZBOnwPrdlpsKWSqnqVqXslTHfw8noeqGN59MPh2Rwg.PNG.sooftware/image.png?type=w773","alt":"2paper"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"참고로 본 논문은 2015년 당시 음성 인식 분야에서 “Listen, Attend and Spell” 논문과 함께 Innovation이라고 불릴만큼 큰 파장을 준 논문이였다.\n기존 CTC 방식이 압도적이였던 당시에, End-to-End 방식의 포문을 열어준 논문이였기 때문이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"General Framework"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfNiAg/MDAxNTgxODY4NTU3Nzg4.YCElS0j5R6lXK_ryX4m_jkAzjFrZPsEekbDna6LCr4Qg.RxewbBWd3eAVDFcasyCT8VafU2oDZqo_yIyh6RNosbIg.PNG.sooftware/image.png?type=w773","alt":"base_attention"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기본적인 어텐션에 대한 큰 그림이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(본 논문에서는 α는 alignment, g는 glimpse라고 칭함 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어떠한 매커니즘을 거쳐서 alignment (α) 를 구하고 나면, alignment와 인코더의 아웃풋들을 곱해서 glimpse를 구한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Attention의 개념"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 나와 있지 않지만 간단하게 개념을 정리하고 가자면, “alignment는 어떤 인코더를 고려해야 할까?”를 수치화해준 벡터이고, glimpse는 수치화 된 alignment와 인코더의 아웃풋들을 각각 곱해서 현재 디코딩에 필요한 인코더의 정보를 압축한 벡터이다. 그리고 glimpse와 디코더의 아웃풋을 고려해서 현재 스텝의 값을 예측한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMTIw/MDAxNTgxODY4NTY1MDcy.94IsqEbUQd9q_A2RBdgwWIzrN2ngrPqGdAJAaUVOQDog.ScXnpprsxqBELeteJFikGmXv3RoH2DcxECcDxSkoRe8g.PNG.sooftware/image.png?type=w773","alt":"alignment"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 alignment는 어떻게 구하지?"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n란 물음에 답해주는 부분이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"특정 방식으로 Score를 구한 뒤, 해당 점수를 Softmax 함수에 넣어서 전체 값을 0~1의 값으로, 전체 합을 1로 만들어 준다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n=> 각 인코더 아웃풋을 얼마씩 참고할지를 수치화하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 Score를 구하는 특정 방식은 무엇이냐??"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfODUg/MDAxNTgxODY4NTY5Mzgy.Nbp_BKh56TwrUeOA9GuBNny_OwX2ZfRzbHYz-Oag4dYg.BGNBoYP9qwvFL26yr5AZhQ3RlE17GAER9pBnA7dblhEg.PNG.sooftware/image.png?type=w773","alt":"score_func"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어텐션 스코어를 구하는 방법은 위와 같이 다양하다. 사실 위는 정말 몇 개만 뽑아온 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어텐션 매커니즘의 종류는 이 스코어 함수가 무엇이냐에 따라 달라진다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고, 본 논문은 새로운 “스코어 함수”를 제안한 논문인 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 2가지 어텐션 방식에 주목했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Content-Based Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Location-Based Attention"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Content-Based Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfNjEg/MDAxNTgxODY4NTkxNzY4.pP5KJnqRAe0qT2fp90t59QJnh7q1cRjETJhDhEdxr4Mg.wK2gs1u_poWfTkwxjPPStLauOB_jVN6Itkz6wrGQHnYg.PNG.sooftware/image.png?type=w773","alt":"content-based"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"아마 어텐션을 처음 공부할 때에 대부분 Dot-Product Attention으로 배웠을 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"해당 스텝의 디코더의 출력과 인코더의 모든 출력들을 내적하여 어텐션 스코어를 구하는 방식이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Content-Based Attention은 Dot-Product보다 조금 더 복잡한 수식으로 점수를 낸다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"단순한 내적이 아닌, 해당 스텝의 디코더의 출력과 인코더의 모든 출력들에 웨이트를 준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 편향 및 Hyperbolic tangent를 걸어주고, 마지막으로 웨이트를 다시 걸어준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Dot-Product Attention에 비해서는 진보된 방법이지만, Content-Based 방식의 문제점은 시퀀스에서의 자신의 위치에 상관없이 스코어링을 한다는 점이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이를 “similar speech fragments” 문제라고 한다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Location-Based Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMTcg/MDAxNTgxODY4NjAyMzU5.csBoosFKJVgVBeHsRCD3fSHokS4MYajHh4lssnQ2bHMg.9P1RL402Y4qyMp6Vfex01uiBTWmUxgAu9zpEArFFvi8g.PNG.sooftware/image.png?type=w773","alt":"location-based"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그럼 이번에는 Location-Based 방식을 살펴보자."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이 방식은 alignment 계산시, 해당 스텝 디코어의 출력과, 이전 alignment를 고려해줌으로써, 현재 시퀀스에서 어느 위치인지를 알 수 있게끔 해주는 방식이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만 이 방식은 인코더의 아웃풋을 전혀 고려하지 않고, 디코더의 아웃풋만을 가지고 예측하기 때문에 분명한 한계점이 존재한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Hybrid Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMTQz/MDAxNTgxODY4NjE1Mzc3.SoD_ilkO_r7XfC-kNb36iNq7wR78iAGl7_HBfznB0VIg.czrqHp7DSqJfOuccfHUq91HPXcdpu2MouEDDg4jWfVog.PNG.sooftware/image.png?type=w773","alt":"hybrid"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 이러한 2 방식의 어텐션을 적절히 결합한 음성 인식용 어텐션을 제안한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"( 해당 어텐션을 Hybrid, Location-Aware, Location-Sensitive 등 여러 이름으로 불린다 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMjg2/MDAxNTgxODY4NjI5MzA4.IHXwfdQs3EC_YQ4mafpM0XLYbDtcPTg2eGjFDjQnTX8g.Dl63xBYsbahH4MyrwGjmbQgI57WTAbXXsAFjFgHhEJ0g.PNG.sooftware/image.png?type=w773","alt":"hybrid-attention"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기존 Content-Based 방식에서 약간의 수식만이 추가됐을 뿐이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기존 Content-Based 방식에서 이전 스텝의 alignment를 고려해준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이때 이전 alignment에 웨이트를 주기 이전에, Convolution으로 1xC의 형상에서 KxC의 형상으로 늘려준다. (C: Classfication Number)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 해당 행렬에 웨이트를 주어서 Content + location 방식을 완성한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"3 Potential Issue"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMTI0/MDAxNTgxODcxNTc5MzU0.9AzhrTq5SoHmgIIrCcmYBB837p3yL8K09QVH3P6gDrkg.YWKb3qLysPBy6oGkm4MBQa1ty9u7-ktEmPgyjRbee7gg.PNG.sooftware/image.png?type=w773","alt":"Eq6"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞에서 살펴봤던 위의 수식에는 3가지의 이슈가 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인풋 시퀀스가 길다면, glimpse에는 노이즈가 섞여있을 가능성이 크다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"만약 인풋 시퀀스가 길다면, 어떤 시점 t에서 멀리 떨어져 있는 t + k라는 시점에서의 음성과는 서로 관련이 없을 것이다. 하지만 Softmax 함수 특성상, 모든 인풋들에 값을 부여한다. 이러한 Softmax의 특성에 의해 많은 관련없는(irrelevant) 인코더의 출력들이 고려될 것이다. 이는 Noise로 작용된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{"start":2},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"시간 복잡도가 크다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"인풋 시퀀스의 길이가 L이라고 할 때, 디코더는 매 타임 스텝마다 이 L개의 frame을 고려해주어야 한다. 그리고, 디코딩 길이를 T라 할 때, 위의 과정을 T만큼 반복하게 된다.  이는 O(LT) 라는 높은 시간 복잡도를 만들게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{"start":3},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Softmax 함수는 Single Vector에만 집중 (focus) 하는 경향이 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이러한 경향은 top-score를 받은 여러 프레임을 고려할 수 없게 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Sharpening"}]},{"type":"text","value":" & "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Windowing"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 위의 문제를 간단하게 해결하기 위해 “Sharpening”이라는 개념의 제안했다. Softmax 수식을 약간 수정하는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMjMz/MDAxNTgxODcyNDMyNjU0.drdyx8zV1DOSV-6rezOYqgDQmcFpiqe4U04da8kcvWcg.w_vqXCxu1nd-uCZ-VW395v4AX76Z5hNuK1HYh15c8iMg.PNG.sooftware/image.png?type=w773","alt":"sharpening"},"children":[]},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nwhen, β > 1"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 inverse temperature를 걸어준다고 표현했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n위의 수식이 왜 1번 문제를 해결해 주는지에 대해서는 아직 이해를 하지 못하였다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 본 논문은 위의 방식이거나, top-k개의 프레임만을 뽑아서 re-normalization을 해주는 방식으로도 해결 가능하다고 말한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n하지만, 위의 2 방식 모두 2번째 시간복잡도의 문제는 해결하지 못했으며, 2번째 방법의 경우는 오히려 시간 복잡도를 더 늘리게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 Windowing이라는 방법이 나오게 되는데, 이전 alignment의 중간값(median)을 기준으로 윈도우 크기 만큼만 고려해주는 방식이다. 해당 방법은 O(L+T)로 시간 복잡도를 낮춰준다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Sharpening은 long-utterance (긴 발화)에서의 퍼포먼스는 개선했지만, 전체적인 퍼포먼스면에서는 좋지 못한 결과로 이어졌다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(짧은 발화에서는 퍼포먼스가 별로였다)"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n하지만 해당 실험은 최상위 점수를 받은 프레임들을 선택하여 집계하는 방식이 좋을 것이라는 가정을 하도록 만들었다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Smoothing"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그래서 나오게 된 방법이 Smoothing 방법이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfNjIg/MDAxNTgxODY4NjM0ODMz.jYTFOEd93R5-IagaKWOyTg3i07Pk7Rwdl1LxsbZAPS8g.r7A0IGeNFdXeSUUl9EB_QWM6EOzb_6N1eHuDGz8_JrIg.PNG.sooftware/image.png?type=w773","alt":"smoothing"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 식처럼 기존 Softmax 식에 Sigmoid를 추가해준 방식이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nSigmoid로 Top-k frame과 아닌 frame들을 구분해주는 방식이라고 나는 이해했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이러한 방식은 다양성을 가져온다고 본 논문은 말한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Result"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMTdfMjkw/MDAxNTgxODY4NjcwNDM4.hknmkkv3qrF8llD9vB2AUALkhuYkUHcuNewXoHv-R-gg.vPQyt_knw2_429fP4jUbdUFU4aMsyexsNCQ7iJi4xb0g.PNG.sooftware/image.png?type=w773","alt":"result"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 진행한 실험의 결과이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n기본 모델보다는 Convolution을 적용한 모델이 더 좋은 결과를 내었고,"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nSmoothing까지 적용한 모델이 최상의 성적을 내었다."}]}],"data":{"quirksMode":false}},"excerpt":"Attention-Based Models for Speech Recognition Paper Review title http://papers.nips.cc/paper/5847-attention-based-models-for-speech…","fields":{"readingTime":{"text":"11 min read"}},"frontmatter":{"title":"Attention-Based Models for Speech Recognition Paper Review","userDate":"20 January 2020","date":"2020-01-20T10:00:00.000Z","tags":["speech","attention","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/1402657e6eeeaf54425d3d5cf34998c8/0f4e3/loc-attention.png","srcSet":"/static/1402657e6eeeaf54425d3d5cf34998c8/0f4e3/loc-attention.png 681w","sizes":"100vw"},"sources":[{"srcSet":"/static/1402657e6eeeaf54425d3d5cf34998c8/c0309/loc-attention.webp 681w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4552129221732746}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"19ded62e-3e91-5733-9329-a1c7bdcf859b","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"35eadfc7-646b-5194-a711-ce20e840ba58","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"fd3185b8-63e4-5e3d-aeb3-e67ed1343af9","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"aad087b1-4b0f-5956-ab2f-d7ab33fdb8c4","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/loc-attention/","prev":{"excerpt":"SpecAugment: 「A Simple Data Augmentation Method for Automatic Speech Recognition」  Review title https://arxiv.org/abs/1904.08779 Abstract…","frontmatter":{"title":"SpecAugment Paper Review","tags":["speech","paper"],"date":"2020-01-12T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC/0lEQVQoz1WT309TZxzG3+mN280SExcXFxNvdONC/AP2D+xCE3exG4UNYeBoik4FrD9Q1DmGYawwTWMW3VAQSpWyGk0czDisECZQREpbBHranv5mFTaXbFI++562u+Ak33yf93ne87zv8805KptdxXieT2hcfzLBT8+n6Hrqodv9jI4pD/f6fudBzygdnkk6J0T3TtA1Ool9aJLu0XFuejz0DHtYevl3zkf9+3olB44fvoH6yISqOonaV4vaXY8qP8b29SXsWFeCOnIYtf8Qql6qQrSSY6IfRJ0wo0xfMj6j5w1XVrI5cLd3hNKrvZg7XFTdcmK+3kf1NRdfmW9ywdLFFw47pmt3qLjl4oCzj8rBTqrsDiru9/Dpz7fREstrDZ39Y+yzOSjtlxe6nVT+2E/FDacY3OGiqYsaq4PPevspczoo73Ri+t7F5523KbOLuRyux1+ujVzf0I3ac5Q3qs9KlNOsLzvFutIGNuw9TvFbB3nzk6MyDolafQJlbkDVWKTXoSwyHks9Y3OxvGE2m7/hiNtHs2uItsERrL+6aR9w0zYwjK3vCfb2h9juD3F58BGtg8O0PnJz5ekArY+HsI4I9/g3EplXecPXhRu2XvmFbaYWPmiysbOxjV21l3O148wPfFhiZZelheJTVopsTbzfbGXnkasUN35LUfslii5Y8QdTBcPCDBu/u8uG8nNsrmvh3ZpmtlZe4j2pTSYr2z9uYkvNebaYmnin9hs2WoSvaxb8NZsazvP26YtMaom1kTPLr1hIZNAX/yL8x59EUkvSlwkvSqWX0Q1Oeji5RGhRKpMhZOiZvP5PIalKpZJMT0+TTCZICw4F59HDGpFIiHhUz+GY9Hg0wvzcbA6nk3GikUhBl32xKKFQiFA4jIrH4/gDAeYXFvD6fPhnA8z4Zng2NSV4lmmvF5/fR0B4Q/MH/LyYe4F3xit4NsdrYhbUgkT0CCqdTuWcDUKPRtGFNMyNdSwWIyxaVHhN1sY+owc1Db8cous6/3/Hq6v5X/g/WhniovCPOZEAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/a5b61e2900f2360216061fe9e8f8f640/4df46/specaugment.png","srcSet":"/static/a5b61e2900f2360216061fe9e8f8f640/4df46/specaugment.png 620w","sizes":"100vw"},"sources":[{"srcSet":"/static/a5b61e2900f2360216061fe9e8f8f640/cd871/specaugment.webp 620w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5919354838709677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"20 min read"},"layout":"","slug":"/specaugment/"}},"next":{"excerpt":"「STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODEL」 Review title https://arxiv.org/abs/1712.0176…","frontmatter":{"title":"STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODEL Paper Review","tags":["speech","paper"],"date":"2020-02-03T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABaklEQVQ4y51TaUvDQBDN//8fKn6xLfip4PGlRaigItL0yGV6YY1Nu1eyOZ7ZrS1p6IEddpnJm5fHzO6sgZLleb71m1hZmqbgjO/NVc2oAuUfpJSIogjBTwDf93fyh0SNQ2KEEkgm0WVDXAYNXAe3aNHngqTWGYKT6QThPMRr8IGrrwYuxjU8sZfzBfW5CQ5OOCSRmHlTpCI9eY7GPlCT/yoRnoB748CpWSBvRGN6/bfCbaVxgpTGGL73YJsDMMaOVnnwltfcHK4r0GyGqNd9dDqL0y2Xx2B3rwVHfoTH+wUe7r5hdkWBZTpX7WYTG9XZyrKstNekOKYIw7mOFa55Jf6O4LGp35iUCShlJ3lKy+CMotVuwx+NMCtmr9frwbIt9Ad9eJ8eXM+D4ziYjEdwXBeWNSy8o1+Oim3bhmmaiOJ4U2GGIAjAOUeSSCyXSyxXKxBC9I0KIbRXT1DFlFKwgqswtUnxrTDVurJffDf0mCFVnMAAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/ccbbeff79541c85288ad06b096472221/8e70e/sota_speech.png","srcSet":"/static/ccbbeff79541c85288ad06b096472221/8e70e/sota_speech.png 546w","sizes":"100vw"},"sources":[{"srcSet":"/static/ccbbeff79541c85288ad06b096472221/dc201/sota_speech.webp 546w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6465201465201466}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"12 min read"},"layout":"","slug":"/sota_sr_speech/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}