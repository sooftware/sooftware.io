{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/efficient-attention/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Efficient Attention: Attention with Linear Complexities</h1>\n<ul>\n<li>Shen Zhuoran et al.</li>\n</ul>\n<h2>Abstract</h2>\n<ul>\n<li>Dot-product attention은 들어오는 인풋 길이에 따라 memory &#x26; computation cost가 quadratically하게 증가함</li>\n<li>어텐션 매커니즘을 조금 수정해서 memory &#x26; computation cost를 상당히 줄이는 방법 제안</li>\n</ul>\n<h2>Method</h2>\n<img src=\"https://www.pragmatic.ml/content/images/2020/06/image-13.png\">\n<ul>\n<li>기존 Dot-product로 similarty를 구하는 방식과 다르게, Key와 value를 곱하는 방식 사용</li>\n<li>Dot-product:</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/121996703-0da3ac80-cde4-11eb-9870-e710b6b13c53.png\">\n<ul>\n<li>Efficient:</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/121996782-2f9d2f00-cde4-11eb-8c73-823f775a42f7.png\">\n<h2>Experiment</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/121996832-4774b300-cde4-11eb-8050-b0f7e00f343d.png\">\n<ul>\n<li>기존 attention과 제안된 attention 비교 => 상당히 효율적으로 변한것을 확인 가능</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/121997009-90c50280-cde4-11eb-9387-4b4819fcb251.png\">\n<ul>\n<li>성능 면에서도 더 좋은 결과가 나왔다는 표</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Efficient Attention: Attention with Linear Complexities"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Shen Zhuoran et al."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Dot-product attention은 들어오는 인풋 길이에 따라 memory & computation cost가 quadratically하게 증가함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"어텐션 매커니즘을 조금 수정해서 memory & computation cost를 상당히 줄이는 방법 제안"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Method"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://www.pragmatic.ml/content/images/2020/06/image-13.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존 Dot-product로 similarty를 구하는 방식과 다르게, Key와 value를 곱하는 방식 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Dot-product:"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/121996703-0da3ac80-cde4-11eb-9870-e710b6b13c53.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Efficient:"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/121996782-2f9d2f00-cde4-11eb-8c73-823f775a42f7.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/121996832-4774b300-cde4-11eb-8050-b0f7e00f343d.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존 attention과 제안된 attention 비교 => 상당히 효율적으로 변한것을 확인 가능"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/121997009-90c50280-cde4-11eb-9387-4b4819fcb251.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"성능 면에서도 더 좋은 결과가 나왔다는 표"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"Efficient Attention: Attention with Linear Complexities Shen Zhuoran et al. Abstract Dot-product attention은 들어오는 인풋 길이에 따라 memory…","fields":{"readingTime":{"text":"1 min read"}},"frontmatter":{"title":"Efficient Attention Paper Review","userDate":"17 July 2021","date":"2021-07-17T10:00:00.000Z","tags":["attention","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/2cdc9b7481cd5e517d106c8618db52d3/a2d8d/efficient_attention.png","srcSet":"/static/2cdc9b7481cd5e517d106c8618db52d3/a74b9/efficient_attention.png 750w,\n/static/2cdc9b7481cd5e517d106c8618db52d3/63493/efficient_attention.png 1080w,\n/static/2cdc9b7481cd5e517d106c8618db52d3/a2d8d/efficient_attention.png 1211w","sizes":"100vw"},"sources":[{"srcSet":"/static/2cdc9b7481cd5e517d106c8618db52d3/ba69e/efficient_attention.webp 750w,\n/static/2cdc9b7481cd5e517d106c8618db52d3/f8adb/efficient_attention.webp 1080w,\n/static/2cdc9b7481cd5e517d106c8618db52d3/a7f95/efficient_attention.webp 1211w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4706853839801816}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":3,"edges":[{"node":{"id":"0d4b1242-b522-54bd-aa71-d8d3b16db784","excerpt":"Efficient Attention: Attention with Linear Complexities Shen Zhuoran et al. Abstract Dot-product attention은 들어오는 인풋 길이에 따라 memory…","frontmatter":{"title":"Efficient Attention Paper Review","date":"2021-07-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"1 min read"},"slug":"/efficient-attention/"}}},{"node":{"id":"44f69d57-8b08-5c25-8c1c-812917e8c950","excerpt":"Luna: Linear Unified Nested Attention USC + CMU + Facebook AI 2021.06 code Abstract 트랜스포머의 Multi Headed Self Attention…","frontmatter":{"title":"Luna: Linear Unified Nested Attention","date":"2021-07-03T23:46:37.121Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/luna/"}}},{"node":{"id":"189dffaa-f30b-5e46-b6cb-005c06f0a6f3","excerpt":"Attention-Based Models for Speech Recognition Paper Review title http://papers.nips.cc/paper/5847-attention-based-models-for-speech…","frontmatter":{"title":"Attention-Based Models for Speech Recognition Paper Review","date":"2020-01-20T10:00:00.000Z"},"fields":{"readingTime":{"text":"11 min read"},"slug":"/loc-attention/"}}}]}},"pageContext":{"slug":"/efficient-attention/","prev":{"excerpt":"2021 AI 온라인 경진대회 1위 이번에 열린 2021 인공지능 온라인 경진대회 대화 감성 분류 태스크에 회사 대표로 참가 Public / Private / Final 리더보드에서 모두…","frontmatter":{"title":"2021 AI 온라인 경진대회 1위","tags":["competition","tunib"],"date":"2021-07-12T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABpElEQVQoz3WT227bMAyG/f5v1AG97cV2tWEoArRb1yZrEx9kWY7Osv+RjJ06FxNA0KKtT+RPumptxkfdQGuNnDNZQSkF8zxfbWKbpotfYoX2X74f8LA7woaA0Qe4mFBNU0FKCZGMQWwMFcBivIo8f4L5cmCWdz5EOcMXVRyIZYZSmsAZzsfP7BaYVh1en3do65PsGcoZRYIItESE4C/AziaoXpMNN0Beq/d0+DyQJCle4wxLlOW+HfHt6QhlrFxUnQaPtm0xjuOi4cW2+m3XNh5zwuNe4Y60bIxDJrmqVZ8byEa/1SIJvtWRs1v1JQK4F1IyvcdgKcuuR6BDo/U3YF5vL7/we/cTXVPLnjPR9F0ufFHEYAwiacir+jsEfBxP6JSSTrGO83/KvGq7xM+k92r3P97x9alGxbPjnKMuRYHxCLFfSy1LllzJvBkfHq9IZSdKwhPj0J7RUD9Ew0AA7/xNZ7fPdjTouxbO2uvYOEpgkltmmYJ5ypeSWVw9WtRNB0O+k/FJ0iT2jNy//sHh5Rm67wXGg6zMWaCB/hJDGhqaEv7+H2OXW0Ff8iIeAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/21d62c8654d10e5d942b1712a230ff6e/93f02/1st_ranked.png","srcSet":"/static/21d62c8654d10e5d942b1712a230ff6e/02437/1st_ranked.png 750w,\n/static/21d62c8654d10e5d942b1712a230ff6e/e8771/1st_ranked.png 1080w,\n/static/21d62c8654d10e5d942b1712a230ff6e/44ee1/1st_ranked.png 1366w,\n/static/21d62c8654d10e5d942b1712a230ff6e/93f02/1st_ranked.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/21d62c8654d10e5d942b1712a230ff6e/06597/1st_ranked.webp 750w,\n/static/21d62c8654d10e5d942b1712a230ff6e/94e4c/1st_ranked.webp 1080w,\n/static/21d62c8654d10e5d942b1712a230ff6e/4094c/1st_ranked.webp 1366w,\n/static/21d62c8654d10e5d942b1712a230ff6e/da0c3/1st_ranked.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5645833333333333}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/2021_ai_online_competition/"}},"next":{"excerpt":"최근 NLP 토크나이저를 만드는데 가장 많이 사용되는  라이브러와 실제 사용이 가장 많이 되는  라이브러리로의 변환에 대한 코드를 담고 있습니다. 해당 내용은  버젼에서 수행되었습니다. Train 아래 코드는 wordpiece, char-bpe…","frontmatter":{"title":"Hugging Face Tokenizers","tags":["huggingface","nlp"],"date":"2021-08-11T15:11:55.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsTAAALEwEAmpwYAAAD1klEQVQ4y2WUXWyTZRTHKx+DgW6MjrHZj5V9qFMiisSEEGI0JgPkSuIFiRfGG70y6o0XJl5w443GBBMSY0w0fqHRZK7SdVvXso5t3Qd267puc2wgUVk6YWYwGGnP+ft/n6erHbzJkz593/f5vef8z/8cF3ipqll2n8PapZKDLs9BF/uh/wxDV/76/5k5ky/uUTjvWgcTC9O7/0InT0GjT0PDFZDu7ZCubdDOamj8Bej8lyXgEugacB0s28uDjZCIC3LRDUn7IJP1/EA9ZMILGeIHwi6CX4TeuXYf1LUOttADCW6BjlQhn2lGbiJAiB9aWHmuXLoBMtVE4GZo12OEZgswWYvQbvTOArTDAx1lVFONwIwHuMT/ToQTPgPGtBeY4/1JRjr9CLR3E/TCcZTWwVWMLvkOJLaBsGZgyovBH1oQ/GxvAUjYlA9Xehrx3Sf7sJQIWGiG0nRsgP7ZVpTNpnx3ieEzvbFao1Vu3I9jrc9j3/5WXI0y2ozPRPvR+wfg9h7Hj6efNJHm0gQOlEMHTxS1tMBsH6RzK8UPmPScqPq+bcHlSBNWk/XIp6gd1/JIAANn7X0n/fwECzW2GxppNkEZDQ3wyjeQnq3QzB4L5MKM16bqaDZb0O53/k76jCSS8plCSdpLa1VBb/1RApz/ChJldWcajUWQ8WMlGcDrJw/j4w+eQf9Pj6KbEb/52iGcObWfcB8dwOgydmm4EnpzvgS4NAYJMeXIg5DBauSHa4HULrz31kE80fQSXj3cipefPYKWlqOIf/84MO5GbqQOcoGO6KJ9onuL1rPAuTOQszRyrILWKUe+vQwIl+HvmAeJt5/D4oljWHjlKEZPH0Bu2A09twXyy2Z20EPUnpm11UCvhQo+XJ6lmTdBevkwWslO4MPzlcj3VwORcmrJovTSyIkGo6WGCBtmIaIV5ld6+BsjNExNc7cJXIhA2jey/DVc1ZB4FeE72C27ke/cThgFn6H40+wUJyIni8QuSB/fi++AJGrMXkPs81uXCZz7nPo9QO34NR4WgiTltd0x7oH8yoiSHnuQckjabyps3mE2EnfbsyH6cXGAwPkveIjNPsoXh+roQb9djn2mG2xKI7UswE4e3mmcYHrbGRjsdR1ywAS2k3EjSeDNS5wuTZBzLErfNhr1YROFOiZnAeRn3g8zylAZpI37UaaZZsSpOruPUf+OMuhvb5S23nWa+kMKfRAapE5BVjC4kXtqM/upHa43LrIBvuY7T7FNWZRufjR+CDr2Lp8PFedjcdoUB+bKVTuhs3Hoahb3XmaK3+ZkWr1eHKpr48v5/x8wDq9nFnJf6AAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/ccbb378a7f6bb27d48ab79b74b1b4d28/5eb66/huggingface.png","srcSet":"/static/ccbb378a7f6bb27d48ab79b74b1b4d28/c65f6/huggingface.png 750w,\n/static/ccbb378a7f6bb27d48ab79b74b1b4d28/5eb66/huggingface.png 798w","sizes":"100vw"},"sources":[{"srcSet":"/static/ccbb378a7f6bb27d48ab79b74b1b4d28/2b9c0/huggingface.webp 750w,\n/static/ccbb378a7f6bb27d48ab79b74b1b4d28/1b08c/huggingface.webp 798w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9511278195488723}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"post","slug":"/tokenizers/"}},"primaryTag":"attention"}},
    "staticQueryHashes": ["3170763342","3229353822"]}