{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/wav2vec2/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations</h1>\n<p>Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli<br>\nFacebook AI Research (FAIR)<br>\narXiv (2020.06)</p>\n<h2>References</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2006.11477\">arXiv</a></li>\n<li><a href=\"https://github.com/pytorch/fairseq/tree/master/examples/wav2vec\">source code</a></li>\n<li><a href=\"https://arxiv.org/abs/1810.04805\">BERT</a></li>\n<li><a href=\"https://arxiv.org/pdf/1904.05862.pdf\">Wav2vec</a></li>\n<li><a href=\"https://arxiv.org/pdf/1910.05453.pdf\">VQ-Wav2vec</a></li>\n<li><a href=\"https://ratsgo.github.io/speechbook/docs/neuralfe/wav2vec\">speech book</a></li>\n</ul>\n<h2>Summary</h2>\n<ul>\n<li>BERT in Speech Recognition</li>\n<li>Representation learning with 53,000 hours of unlabeled speech data.</li>\n<li>Excellent speech recognition performance with only 40 sentences (10 minutes) of labeled data</li>\n</ul>\n<h2>Abstract</h2>\n<ul>\n<li>음성 데이터 자체만으로 학습한 후 Fine-tuning하는 간단한 방법으로 semi-supervised 방법보다 좋은 성능을 냄</li>\n<li>TIMIT, LibriSpeech 100h 데이터셋에서 State-Of-The-Art (SOTA) 를 달성</li>\n<li>LibriSpeech 100h은 단 1시간의 데이터만으로 기존 SOTA보다 높은 성능을 보임</li>\n<li>단 10분의 데이터 (40문장) 으로 LibriSpeech clean 5.7 / noisy 10.1 Word Error Rate (WER) 를 기록</li>\n<li>LibriSpeech의 전체 학습셋을 사용했을 때 (960h) clean 1.9 / noisy 3.5 WER을 기록 (현재 <a href=\"https://github.com/syhw/wer_are_we\">wer-are-we</a> SOTA보다 높은 기록)</li>\n</ul>\n<h2>Wav2vec</h2>\n<ul>\n<li>현재 어느 정도 수준 이상의 음성인식기를 만들기 위해서는 대량의 데이터가 필요함 (수천 시간)</li>\n<li>세상에는 7,000개 이상의 언어가 존재하는데 모든 언어에 대해 이 정도 수준의 데이터를 구축하기는 어려움</li>\n<li>영아 (infant) 들은 단순히듣는 것만으로 음성에 대해 학습함</li>\n</ul>\n<h3>Wav2vec 1.0 (previous work)</h3>\n<img src=\"https://i.imgur.com/H9X1HiX.png\" width=\"500\">    \n<ul>\n<li>wav2vec은 크게 <em>encoder</em> network <em>f</em>와 <em>context</em> network <em>g</em> 두개의 파트로 구성 (둘 모두 convolution neural network)</li>\n<li>wav2vec은 해당 입력이 Positive인지 Negative인지 이진 분류(Binary Classification)하는 과정에서 학습</li>\n<li>Positive : (C<sub>i</sub>, Z<sub>i+1</sub>), Negative : otherwise</li>\n<li>Negative 쌍은 입력 음성의 i번째 context representation C<sub>i</sub>와 다른 음성의 hidden representation들 중 랜덤 추출</li>\n<li>즉, 2개의 네트워크는 입력 음성의 다음 시퀀스가 무엇일지에 관한 정보를 음성 피처에 녹여내도록 학습</li>\n</ul>\n<h3>VQ-Wav2vec (previous work)</h3>\n<img src=\"https://i.imgur.com/ivviYL1.png\" width=\"500\">  \n<ul>\n<li>Wav2vec 아키텍처 중간에 <strong>Vector Quantization</strong> 모듈을 추가한 구조</li>\n<li>VQ 모듈 : continuous representation Z를 discrete representation Z<sup>^</sup>로 변환</li>\n<li>Discretization(이산화)는 discrete한 input을 필요로하는 NLP 알고리즘들을 바로 적용할 수 있다는 장점이 있음</li>\n</ul>\n<h4>Vector Quantization</h4>\n<img src=\"https://i.imgur.com/y15Qu5Z.png\" width=\"300\">  \n<ol>\n<li>\n<p>Z를 선형변환하여 logit을 만듦</p>\n</li>\n<li>\n<p>여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦</p>\n</li>\n<li>\n<p>이후 Embedding matrix를 내적해 Z<sup>^</sup>를 만듦</p>\n</li>\n</ol>\n<h3>Wav2vec 2.0</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/92450554-8a22b280-f1f6-11ea-8f66-0616b29d8c94.png\" width=\"500\">  \n<ul>\n<li>VQ-wav2vec의 모델을 Transformer로 대체</li>\n<li>Pre-training 이후 labeled data로 fine-tuning (Connectionist Temporal Classification (CTC) loss 사용)</li>\n<li>이전 연구들과의 차이점으로 Filter-Bank와 같은 피쳐추출 과정이 없음</li>\n</ul>\n<h2>Model</h2>\n<p>VQ-Wav2vec와 비교하여 Transformer를 사용했다는 특징이 있음</p>\n<h3>Feature Encoder</h3>\n<ul>\n<li>\n<p>N x [Conv1d, Dropout, GroupNorm, GELU]</p>\n</li>\n</ul>\n<h3>Transformer</h3>\n<ul>\n<li>Positional Encoding을 conv1d로 대체</li>\n<li>Convoulation Layer 뒷단에 layer normalization 적용</li>\n</ul>\n<h3>Quantization module</h3>\n<ul>\n<li>Vector Quantization 부분 참고</li>\n</ul>\n<h2>Training</h2>\n<p>BERT의 masked language modeling (MLM)과 유사하게 latent speech의 일부분을 masking하며, 모델은 quantized latent audio representation을 맞추는 방식으로 트레이닝이 진행됨. 이렇게 학습한 후 labeled 된 데이터로 fine-tuning 진행.</p>\n<h3>Masking</h3>\n<ul>\n<li>Maksing 방법</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1. 전체 오디오 구간 중 6.5%를 랜덤하게 선택  \n  \n2. 선택된 구간부터 10 time-step만큼 masking (masking은 중복될 수 있음)   </code></pre></div>\n<ul>\n<li>전체 오디오 구간 중 약 49% 정도가 masking (평균 14.7 timestep (299ms))</li>\n</ul>\n<h3>Objective</h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/93301173-0474b780-f833-11ea-8206-5ab40cf418a5.png\" alt=\"image\"></p>\n<p>L<sub>m</sub> : Contrastive Loss, L<sub>d</sub> : Diversity Loss, L<sub>f</sub> : L2 penalty, {alpha, beta} : hyperparameter</p>\n<ul>\n<li>Contrastive Loss</li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/92514957-c5040500-f24d-11ea-95c3-1183fa1145b2.png\" alt=\"contrastive-loss\"></p>\n<ul>\n<li>Diversity Loss</li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/92514997-df3de300-f24d-11ea-835c-f1367aef5ebe.png\" alt=\"diversity-loss\"></p>\n<h3>Fine-tuning</h3>\n<ul>\n<li>\n<p>이렇게 Pre-train 된 모델을 ASR 태스크로 Fine-tuning (+ projection layer)</p>\n</li>\n<li>\n<p>29 character token</p>\n</li>\n<li>\n<p>CTC Loss</p>\n</li>\n<li>\n<p>SpecAugment</p>\n</li>\n</ul>\n<h2>Experiment</h2>\n<h3>Datasets</h3>\n<ul>\n<li>Unlabeled data 1 : LibriVox-60k (전처리하여 53.2k 사용) [<a href=\"https://arxiv.org/abs/1912.07875\">Reference</a>]</li>\n<li>Unlabeled data 2 : LibriSpeech 960h</li>\n<li>train-10min, train-1h, train-10h, train-100h, train-960h 설정 (LibriSpeech)</li>\n</ul>\n<h3>Result</h3>\n<ul>\n<li>WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled</li>\n</ul>\n<p>data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/92516234-de0db580-f24f-11ea-88f3-485ee579bfda.png\" alt=\"image\"></p>\n<ul>\n<li>WER on Librispeech when using all labeled data of 960 hours</li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/92516409-262cd800-f250-11ea-8e77-42fdc8761d2c.png\" alt=\"image\"></p>\n<h2>Conclusion</h2>\n<ul>\n<li>적은 비용으로도 좋은 성능의 음성인식기를 만들 수 있는 연구 방향을 제시함</li>\n<li>Seq2seq 구조 혹은 word-piece 단위로의 변경을 통해 성능 향상 기대</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nFacebook AI Research (FAIR)"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\narXiv (2020.06)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"References"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/2006.11477"},"children":[{"type":"text","value":"arXiv"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/pytorch/fairseq/tree/master/examples/wav2vec"},"children":[{"type":"text","value":"source code"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1810.04805"},"children":[{"type":"text","value":"BERT"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1904.05862.pdf"},"children":[{"type":"text","value":"Wav2vec"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1910.05453.pdf"},"children":[{"type":"text","value":"VQ-Wav2vec"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://ratsgo.github.io/speechbook/docs/neuralfe/wav2vec"},"children":[{"type":"text","value":"speech book"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Summary"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT in Speech Recognition"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Representation learning with 53,000 hours of unlabeled speech data."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Excellent speech recognition performance with only 40 sentences (10 minutes) of labeled data"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"음성 데이터 자체만으로 학습한 후 Fine-tuning하는 간단한 방법으로 semi-supervised 방법보다 좋은 성능을 냄"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"TIMIT, LibriSpeech 100h 데이터셋에서 State-Of-The-Art (SOTA) 를 달성"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"LibriSpeech 100h은 단 1시간의 데이터만으로 기존 SOTA보다 높은 성능을 보임"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"단 10분의 데이터 (40문장) 으로 LibriSpeech clean 5.7 / noisy 10.1 Word Error Rate (WER) 를 기록"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"LibriSpeech의 전체 학습셋을 사용했을 때 (960h) clean 1.9 / noisy 3.5 WER을 기록 (현재 "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/syhw/wer_are_we"},"children":[{"type":"text","value":"wer-are-we"}]},{"type":"text","value":" SOTA보다 높은 기록)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Wav2vec"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"현재 어느 정도 수준 이상의 음성인식기를 만들기 위해서는 대량의 데이터가 필요함 (수천 시간)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"세상에는 7,000개 이상의 언어가 존재하는데 모든 언어에 대해 이 정도 수준의 데이터를 구축하기는 어려움"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"영아 (infant) 들은 단순히듣는 것만으로 음성에 대해 학습함"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Wav2vec 1.0 (previous work)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://i.imgur.com/H9X1HiX.png","width":500},"children":[]},{"type":"text","value":"    \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"wav2vec은 크게 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"encoder"}]},{"type":"text","value":" network "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"f"}]},{"type":"text","value":"와 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"context"}]},{"type":"text","value":" network "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"g"}]},{"type":"text","value":" 두개의 파트로 구성 (둘 모두 convolution neural network)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"wav2vec은 해당 입력이 Positive인지 Negative인지 이진 분류(Binary Classification)하는 과정에서 학습"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Positive : (C"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"i"}]},{"type":"text","value":", Z"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"i+1"}]},{"type":"text","value":"), Negative : otherwise"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Negative 쌍은 입력 음성의 i번째 context representation C"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"i"}]},{"type":"text","value":"와 다른 음성의 hidden representation들 중 랜덤 추출"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"즉, 2개의 네트워크는 입력 음성의 다음 시퀀스가 무엇일지에 관한 정보를 음성 피처에 녹여내도록 학습"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"VQ-Wav2vec (previous work)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://i.imgur.com/ivviYL1.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Wav2vec 아키텍처 중간에 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Vector Quantization"}]},{"type":"text","value":" 모듈을 추가한 구조"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"VQ 모듈 : continuous representation Z를 discrete representation Z"},{"type":"element","tagName":"sup","properties":{},"children":[{"type":"text","value":"^"}]},{"type":"text","value":"로 변환"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Discretization(이산화)는 discrete한 input을 필요로하는 NLP 알고리즘들을 바로 적용할 수 있다는 장점이 있음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h4","properties":{},"children":[{"type":"text","value":"Vector Quantization"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://i.imgur.com/y15Qu5Z.png","width":300},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Z를 선형변환하여 logit을 만듦"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이후 Embedding matrix를 내적해 Z"},{"type":"element","tagName":"sup","properties":{},"children":[{"type":"text","value":"^"}]},{"type":"text","value":"를 만듦"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Wav2vec 2.0"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/92450554-8a22b280-f1f6-11ea-8f66-0616b29d8c94.png","width":500},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"VQ-wav2vec의 모델을 Transformer로 대체"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pre-training 이후 labeled data로 fine-tuning (Connectionist Temporal Classification (CTC) loss 사용)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이전 연구들과의 차이점으로 Filter-Bank와 같은 피쳐추출 과정이 없음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Model"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"VQ-Wav2vec와 비교하여 Transformer를 사용했다는 특징이 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Feature Encoder"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"N x [Conv1d, Dropout, GroupNorm, GELU]"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Transformer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Positional Encoding을 conv1d로 대체"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Convoulation Layer 뒷단에 layer normalization 적용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Quantization module"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Vector Quantization 부분 참고"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Training"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"BERT의 masked language modeling (MLM)과 유사하게 latent speech의 일부분을 masking하며, 모델은 quantized latent audio representation을 맞추는 방식으로 트레이닝이 진행됨. 이렇게 학습한 후 labeled 된 데이터로 fine-tuning 진행."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Masking"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Maksing 방법"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"1. 전체 오디오 구간 중 6.5%를 랜덤하게 선택  \n  \n2. 선택된 구간부터 10 time-step만큼 masking (masking은 중복될 수 있음)   "}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"전체 오디오 구간 중 약 49% 정도가 masking (평균 14.7 timestep (299ms))"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Objective"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/93301173-0474b780-f833-11ea-8206-5ab40cf418a5.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"L"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"m"}]},{"type":"text","value":" : Contrastive Loss, L"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"d"}]},{"type":"text","value":" : Diversity Loss, L"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"f"}]},{"type":"text","value":" : L2 penalty, {alpha, beta} : hyperparameter"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Contrastive Loss"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/92514957-c5040500-f24d-11ea-95c3-1183fa1145b2.png","alt":"contrastive-loss"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Diversity Loss"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/92514997-df3de300-f24d-11ea-835c-f1367aef5ebe.png","alt":"diversity-loss"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Fine-tuning"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이렇게 Pre-train 된 모델을 ASR 태스크로 Fine-tuning (+ projection layer)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"29 character token"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"CTC Loss"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"SpecAugment"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Datasets"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Unlabeled data 1 : LibriVox-60k (전처리하여 53.2k 사용) ["},{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1912.07875"},"children":[{"type":"text","value":"Reference"}]},{"type":"text","value":"]"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Unlabeled data 2 : LibriSpeech 960h"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"train-10min, train-1h, train-10h, train-100h, train-960h 설정 (LibriSpeech)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Result"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/92516234-de0db580-f24f-11ea-88f3-485ee579bfda.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"WER on Librispeech when using all labeled data of 960 hours"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/92516409-262cd800-f250-11ea-8e77-42fdc8761d2c.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conclusion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"적은 비용으로도 좋은 성능의 음성인식기를 만들 수 있는 연구 방향을 제시함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Seq2seq 구조 혹은 word-piece 단위로의 변경을 통해 성능 향상 기대"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","fields":{"readingTime":{"text":"5 min read"}},"frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","userDate":"12 September 2020","date":"2020-09-12T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/038c6/wav2vec2.png","srcSet":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/e6cc4/wav2vec2.png 750w,\n/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/038c6/wav2vec2.png 842w","sizes":"100vw"},"sources":[{"srcSet":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/79bef/wav2vec2.webp 750w,\n/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/daf06/wav2vec2.webp 842w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5106888361045131}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"19ded62e-3e91-5733-9329-a1c7bdcf859b","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"35eadfc7-646b-5194-a711-ce20e840ba58","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"fd3185b8-63e4-5e3d-aeb3-e67ed1343af9","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"aad087b1-4b0f-5956-ab2f-d7ab33fdb8c4","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/wav2vec2/","prev":{"excerpt":"Conformer: Convolution-augmented Transformer for Speech Recognition Anmol Gulati et al. Google Inc. INTERSPEECH, 2020 Reference Conformer…","frontmatter":{"title":"Conformer Paper Review","tags":["speech","paper"],"date":"2020-08-30T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAACcElEQVQ4y3VUa3PSQBTN///mZz/5xRl1Ojo6Yx0Uq44tlRawPAsJISS0PJJAstm8H8e7QWqomsyyYS8595xz70WK4xi+7yNOEkT0nOU50jSF53mwTBPr9Rq73Q5BEEBcRVEc7Y8vKacAd11sWtdYnX/HztAREnCSxHCZh41pI4oiiMSPwUTSO2MOVR7RPiuJSCIQEUPndgS31wMnRpzYFMTU9TjWtkNgEan4w/AAKE9kvKud4tnJc7x4ewLHcfaAOd2KqaK7GMBLODzXw8I00JxclGto9BGFEQoUR4AzVUV7PEe9o+C8q4AxBinLMnicwdgomN71EcQe8rwoPWVBCH1lwmEueRgeyRUWTKcqvp6d4uOHl2hefi4tKD1MIx/F8gap1gBCC2kOJL4Jd3UDa9FGsFMQEsMqoCAync7QPHuD+qsn+PT66V6yCPg+h7M0YGoKsiigSlOSmCHiOhxLhs8WRwwfJM80dK5+4PxLDd12E5xzSMKXJM7Qv5qiUe+C2UHpVcx9sHsT94oBttogCI8lb7dbzOdz9IcjdAdDMCpsToWUxEdADN2NDvtORhZTgF5g1D5GvQat9h5m+xoBtQ4q7IQ8sZTBAL2LBtbEtmwbEc/SEKHdIkbfgNRGktIhJcqSHKblIia51cauNvVS09BtXEK7He8BBUMxKRl9ybP0wXAecqjLGdTVHLZnUVHCvyZFLFFUi+Tn2J9L1bGpDpPNLPRmP9FRWpgYt2TL/0ev+iwdMh0OBW1RLY/GjtESQKKpxfg9lpxQrwp1gr2YpN8eHmcU3S7LMnRdx3g8fvDuX+xs2y5/q5GPE9pd+k/4BZe/KyhkgKhoAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/5ca9b355b1dc4393c360f527478a3ba2/503d5/conformer.png","srcSet":"/static/5ca9b355b1dc4393c360f527478a3ba2/bfaac/conformer.png 750w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/abf2b/conformer.png 1080w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/6c19a/conformer.png 1366w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/503d5/conformer.png 1890w","sizes":"100vw"},"sources":[{"srcSet":"/static/5ca9b355b1dc4393c360f527478a3ba2/1e5e2/conformer.webp 750w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/77f81/conformer.webp 1080w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/b9a60/conformer.webp 1366w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/2837d/conformer.webp 1890w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6888888888888889}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/conformer/"}},"next":{"excerpt":"Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs…","frontmatter":{"title":"Electra Paper Review","tags":["nlp","paper"],"date":"2020-09-23T07:03:47.149Z","draft":null,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABZElEQVQoz02S2arCQBBE8/9flHdBIu5bcItG1OCCcU80asp7GgZuYOjpnqrqLV5Zlvp8Pvp+v3q9Xnq/33o+n3o8HsrzXGmaajgcms87/u120/V61eVyMQunKAqh5env2+12SpJEURRpuVxqsVjofr+bMAREVquV4jg2n88VQZyEcPg8lGezmYkSRBhitVqV7/vqdrs6nU7abDbabrcaDAYKgkCVSkVhGBp+Op1qMplYpR5Z1uu1gRFywgjM53Or+Hw+G5FqaJfqsRQDDmFE0bKWIdAKlQA8Ho/WLm0xO+5g9vu9YZgl882yTIfDwYqAYy0jwCMkANwRQcAlAsM7lSHoFsLBB+8Se5TKXBgs8xqPx3bv9/tmaYd3FvY/3uv1NBqN1G63zcJlJB4OIAAE6/W6ms2mCWHdqdVq6nQ69t5qtcwHT4w7cXZggo1Gw0QhAiYrFjDLcL8VM3SHGJZlslR8RvEDFpSXtUMvB8QAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png","srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/c68af/electra.png 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/87f65/electra.png 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/d464a/electra.png 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/9fb02/electra.webp 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/cd76f/electra.webp 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/b7397/electra.webp 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/507b8/electra.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4479166666666667}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"post","slug":"/electra/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}