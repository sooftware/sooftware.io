{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/luna/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Luna: Linear Unified Nested Attention</h1>\n<ul>\n<li>USC + CMU + Facebook AI</li>\n<li>2021.06</li>\n<li><a href=\"https://github.com/XuezheMax/fairseq-apollo\">code</a></li>\n</ul>\n<h2>Abstract</h2>\n<ul>\n<li>\n<p>트랜스포머의 Multi Headed Self Attention은 시퀀스가 길어질수록 메모리, 시간복잡도가 quadratic함</p>\n</li>\n<li>\n<p>이를 linear하게 바꾸기 위한 시도로 Luna (Linear Unified Nested Attention) 을 제안</p>\n</li>\n<li>\n<p>핵심은 인풋을 고정 길이로 변환한다는 점과 attention을 2개로 분리한다는 점.</p>\n</li>\n<li>\n<p>Positional Embedding을 별도의 고정 길이 query로 뺌.</p>\n</li>\n<li>\n<p>속도는 Performer랑 비슷한데 시퀀스가 길어질수록 좀 더 효과적이고 메모리도 적게 쓴다고 함.</p>\n</li>\n<li>\n<p>정확도 성능은 경쟁력이 높은 편.</p>\n</li>\n</ul>\n<h2>Attention</h2>\n<h3>Traditional attention mechanism:</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/127510622-5af08d8d-771c-4bb9-8e9e-53bb3f4cf85e.png\" height=\"70\">\n<ul>\n<li>X: Query sequence, C: Context sequence</li>\n<li>ω: activation function (usually softmax)</li>\n<li>Q = XW<sub>Q</sub>, K = CW<sub>K</sub>, V = CW<sub>V</sub></li>\n<li>Self attention에서는 X == C</li>\n<li>공간 &#x26; 시간복잡도: O(nm) (n: X의 시퀀스 길이, m: C의 시퀀스 길이)</li>\n</ul>\n<h2>Linear Unified Nested Attention (LUNA)</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/127514004-0ca02332-8ef5-4563-b9b4-bd9bf6bb72b9.png\" height=\"400\">\n<ul>\n<li>Goal: Attention mechanism’s complexity <strong>quadratic => linear</strong></li>\n</ul>\n<h3>Luna (Pack and Unpack Attention)</h3>\n<ul>\n<li>이 어텐션의 핵심은 어텐션을 2개로 쪼개는 것.</li>\n<li>O(ln) (l은 P의 길이)</li>\n<li>Pack Attention:\n<ul>\n<li>Query를 learnable paramter인 P로 대체 (고정 길이, 길이에 대한 실험 있음)</li>\n<li>상대적으로 더 짧은 Query를 놓음으로써 complexity를 줄임</li>\n<li>P-contextual: 첫 레이어의 P는 learnable parameter이며 다음 레이어로 전달</li>\n<li>P-non-contextual: 각 레이어마다 P를 학습하고 다음 레이어로 넘기지 않음</li>\n<li>contextual &#x26; non-contextual에 대한 실험은 뒤에 있음</li>\n</ul>\n</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603299-44234ff8-1735-4dc2-95b0-bc8f66bfbbde.png\" height=\"60\">\n<ul>\n<li>Unpack Attention:\n<ul>\n<li>Pack Attention의 결과인 <code class=\"language-text\">packed context</code>를 Key와 Value로 사용. Query는 기존 query.</li>\n</ul>\n</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603352-dfb741df-ec40-4b49-8bdf-b158e55b771e.png\" height=\"70\">\n<ul>\n<li>Luna Attention:</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603409-554fa551-e89d-46ee-aec4-9a471a5cdc8f.png\" height=\"70\">\n<ul>\n<li>Luna Layer</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603246-192380b0-fde6-4941-b060-1bf639d3b8e7.png\" height=\"100\">\n<h2>Discussion</h2>\n<h3>Relation to Linformer</h3>\n<ul>\n<li>Linformer와 비슷한 포지션. 그럼 뭐가 더 나은가?\n<ul>\n<li>Linformer는 인풋 시퀀스가 모두 고정 길이를 가져야하는데, Luna는 various length가 가능 (projection matrix 때문에 linformer는 길이가 고정이여야함)</li>\n<li>결정적으로 Linformer보다 성능이 좋음</li>\n</ul>\n</li>\n</ul>\n<h2>Experiment</h2>\n<h3>Long-Context Sequence Modeling</h3>\n<h4><strong>Score</strong></h4>\n<img src=\"https://user-images.githubusercontent.com/42150335/127603947-40c6b9f0-63d7-475c-8b6d-cefdfbe5ab59.png\" height=\"500\">\n<h4><strong>Training Speed &#x26; Memory</strong></h4>\n<img src=\"https://user-images.githubusercontent.com/42150335/127604072-79facf8c-7f84-4e9d-bd1e-38b3322458d0.png\" height=\"500\">\n<ul>\n<li>인풋 길이가 길어지면 Luna가 Linformer보다 더 빠름</li>\n<li>메모리 사용량에서 Luna가 Linformer를 포함한 다른 모델들보다 경쟁력이 있음</li>\n</ul>\n<h3>NLU Task (Masked Language Modeling for Large-Scale Pretraining)</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/127604344-7074edf7-ede2-4140-a6e8-320ed711d5f3.png\" height=\"300\">\n<ul>\n<li>BERT 방식으로 pre-training 후 파인튜닝 했을 때 성능 비교</li>\n<li>RoBERTa와도 비견될만큼 좋은 성능을 보임</li>\n</ul>\n<h3>Machine Translation</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/127604608-822e1fd6-00a1-472b-801e-24cda16efa5f.png\" height=\"250\">\n<h3>Abbrebiation Study (contextual &#x3C;-> non-contextual)</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/127604743-ae8289cc-d420-41b0-9ff1-5604ef770b4c.png\" height=\"150\">\n<ul>\n<li>P를 각 레이어의 파라미터로 둘지, 위층으로 넘김으로써 context 정보를 넘겨줄지에 대한 실험</li>\n<li>결론: 다음 층으로 넘겨주는게 좋더라.</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Luna: Linear Unified Nested Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"USC + CMU + Facebook AI"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"2021.06"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/XuezheMax/fairseq-apollo"},"children":[{"type":"text","value":"code"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"트랜스포머의 Multi Headed Self Attention은 시퀀스가 길어질수록 메모리, 시간복잡도가 quadratic함"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이를 linear하게 바꾸기 위한 시도로 Luna (Linear Unified Nested Attention) 을 제안"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"핵심은 인풋을 고정 길이로 변환한다는 점과 attention을 2개로 분리한다는 점."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Positional Embedding을 별도의 고정 길이 query로 뺌."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"속도는 Performer랑 비슷한데 시퀀스가 길어질수록 좀 더 효과적이고 메모리도 적게 쓴다고 함."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"정확도 성능은 경쟁력이 높은 편."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Traditional attention mechanism:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127510622-5af08d8d-771c-4bb9-8e9e-53bb3f4cf85e.png","height":70},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"X: Query sequence, C: Context sequence"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"ω: activation function (usually softmax)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Q = XW"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"Q"}]},{"type":"text","value":", K = CW"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"K"}]},{"type":"text","value":", V = CW"},{"type":"element","tagName":"sub","properties":{},"children":[{"type":"text","value":"V"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Self attention에서는 X == C"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"공간 & 시간복잡도: O(nm) (n: X의 시퀀스 길이, m: C의 시퀀스 길이)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Linear Unified Nested Attention (LUNA)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127514004-0ca02332-8ef5-4563-b9b4-bd9bf6bb72b9.png","height":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Goal: Attention mechanism’s complexity "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"quadratic => linear"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Luna (Pack and Unpack Attention)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 어텐션의 핵심은 어텐션을 2개로 쪼개는 것."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"O(ln) (l은 P의 길이)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pack Attention:\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Query를 learnable paramter인 P로 대체 (고정 길이, 길이에 대한 실험 있음)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"상대적으로 더 짧은 Query를 놓음으로써 complexity를 줄임"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"P-contextual: 첫 레이어의 P는 learnable parameter이며 다음 레이어로 전달"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"P-non-contextual: 각 레이어마다 P를 학습하고 다음 레이어로 넘기지 않음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"contextual & non-contextual에 대한 실험은 뒤에 있음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603299-44234ff8-1735-4dc2-95b0-bc8f66bfbbde.png","height":60},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Unpack Attention:\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pack Attention의 결과인 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"packed context"}]},{"type":"text","value":"를 Key와 Value로 사용. Query는 기존 query."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603352-dfb741df-ec40-4b49-8bdf-b158e55b771e.png","height":70},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Luna Attention:"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603409-554fa551-e89d-46ee-aec4-9a471a5cdc8f.png","height":70},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Luna Layer"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603246-192380b0-fde6-4941-b060-1bf639d3b8e7.png","height":100},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Discussion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Relation to Linformer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Linformer와 비슷한 포지션. 그럼 뭐가 더 나은가?\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Linformer는 인풋 시퀀스가 모두 고정 길이를 가져야하는데, Luna는 various length가 가능 (projection matrix 때문에 linformer는 길이가 고정이여야함)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결정적으로 Linformer보다 성능이 좋음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Long-Context Sequence Modeling"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h4","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Score"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127603947-40c6b9f0-63d7-475c-8b6d-cefdfbe5ab59.png","height":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h4","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Training Speed & Memory"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127604072-79facf8c-7f84-4e9d-bd1e-38b3322458d0.png","height":500},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인풋 길이가 길어지면 Luna가 Linformer보다 더 빠름"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"메모리 사용량에서 Luna가 Linformer를 포함한 다른 모델들보다 경쟁력이 있음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"NLU Task (Masked Language Modeling for Large-Scale Pretraining)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127604344-7074edf7-ede2-4140-a6e8-320ed711d5f3.png","height":300},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT 방식으로 pre-training 후 파인튜닝 했을 때 성능 비교"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"RoBERTa와도 비견될만큼 좋은 성능을 보임"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Machine Translation"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127604608-822e1fd6-00a1-472b-801e-24cda16efa5f.png","height":250},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Abbrebiation Study (contextual <-> non-contextual)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/127604743-ae8289cc-d420-41b0-9ff1-5604ef770b4c.png","height":150},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"P를 각 레이어의 파라미터로 둘지, 위층으로 넘김으로써 context 정보를 넘겨줄지에 대한 실험"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결론: 다음 층으로 넘겨주는게 좋더라."}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"Luna: Linear Unified Nested Attention USC + CMU + Facebook AI 2021.06 code Abstract 트랜스포머의 Multi Headed Self Attention…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"Luna: Linear Unified Nested Attention","userDate":"3 July 2021","date":"2021-07-03T23:46:37.121Z","tags":["attention"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/540ac0a1a4457065fef9e262f9962c83/af6c5/luna.png","srcSet":"/static/540ac0a1a4457065fef9e262f9962c83/a5b02/luna.png 750w,\n/static/540ac0a1a4457065fef9e262f9962c83/ace3b/luna.png 1080w,\n/static/540ac0a1a4457065fef9e262f9962c83/58836/luna.png 1366w,\n/static/540ac0a1a4457065fef9e262f9962c83/af6c5/luna.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/540ac0a1a4457065fef9e262f9962c83/6f4ad/luna.webp 750w,\n/static/540ac0a1a4457065fef9e262f9962c83/9ec3b/luna.webp 1080w,\n/static/540ac0a1a4457065fef9e262f9962c83/51e4d/luna.webp 1366w,\n/static/540ac0a1a4457065fef9e262f9962c83/3c39a/luna.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5192708333333333}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":3,"edges":[{"node":{"id":"1a5eee5a-015b-5220-9f84-93c5315e2167","excerpt":"Efficient Attention: Attention with Linear Complexities Shen Zhuoran et al. Abstract Dot-product attention은 들어오는 인풋 길이에 따라 memory…","frontmatter":{"title":"Efficient Attention Paper Review","date":"2021-07-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"1 min read"},"slug":"/efficient-attention/"}}},{"node":{"id":"3c6a2ac2-68ad-5a52-80dc-019dad7501c7","excerpt":"Luna: Linear Unified Nested Attention USC + CMU + Facebook AI 2021.06 code Abstract 트랜스포머의 Multi Headed Self Attention…","frontmatter":{"title":"Luna: Linear Unified Nested Attention","date":"2021-07-03T23:46:37.121Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/luna/"}}},{"node":{"id":"1422fe0c-6c3b-5e3e-a95c-fe5a63ec8f2f","excerpt":"Attention-Based Models for Speech Recognition Paper Review title http://papers.nips.cc/paper/5847-attention-based-models-for-speech…","frontmatter":{"title":"Attention-Based Models for Speech Recognition Paper Review","date":"2020-01-20T10:00:00.000Z"},"fields":{"readingTime":{"text":"11 min read"},"slug":"/loc-attention/"}}}]}},"pageContext":{"slug":"/luna/","prev":{"excerpt":"GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…","frontmatter":{"title":"P-Tuning Paper Review","tags":["nlp","paper"],"date":"2021-05-13T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABE0lEQVQY002Q6W6DMBCEef9nqvqzl9SKpIfU5gLSEHDAxTFgwpGv60SVutJIs+PZ1Y6DcZyYpgnnOmxd0/cnxv/9MFA3DdZUVNs5rkwYZMbrTRFhkhBXG4625nyeCFzbUOmSfL9jn6ak2UGWtRR5zkFwkqW+vNm1NbU1ZPuU5TrGWss4nCiUItlsGMeRwKiE7PMB9XXPbvXKzUtOtPygCB8p3t7oswx7tJhKo+OZ+O5IFnNuZwXR4h29eULNnlFhKAsHgs45JiFtU5PEMZsoRmvN0Pf4t0bgq/d9d/XqsmS5WvO93WLMj2gjjSTpuo7An+nNpZhSHzndCdcXrW1bBvmra+SzaMOl91AqFyj+5hvxev4LH4d753+QROEAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/5251bff33539be461c690325c488ea29/c0e65/p_tuning.png","srcSet":"/static/5251bff33539be461c690325c488ea29/f0b2b/p_tuning.png 750w,\n/static/5251bff33539be461c690325c488ea29/3831f/p_tuning.png 1080w,\n/static/5251bff33539be461c690325c488ea29/ec348/p_tuning.png 1366w,\n/static/5251bff33539be461c690325c488ea29/c0e65/p_tuning.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/5251bff33539be461c690325c488ea29/a4cf2/p_tuning.webp 750w,\n/static/5251bff33539be461c690325c488ea29/4e6df/p_tuning.webp 1080w,\n/static/5251bff33539be461c690325c488ea29/04ab9/p_tuning.webp 1366w,\n/static/5251bff33539be461c690325c488ea29/19b4f/p_tuning.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.23489583333333336}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/p_tuning/"}},"next":{"excerpt":"2021 AI 온라인 경진대회 1위 이번에 열린 2021 인공지능 온라인 경진대회 대화 감성 분류 태스크에 회사 대표로 참가 Public / Private / Final 리더보드에서 모두…","frontmatter":{"title":"2021 AI 온라인 경진대회 1위","tags":["competition","tunib"],"date":"2021-07-12T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABpElEQVQoz3WT227bMAyG/f5v1AG97cV2tWEoArRb1yZrEx9kWY7Osv+RjJ06FxNA0KKtT+RPumptxkfdQGuNnDNZQSkF8zxfbWKbpotfYoX2X74f8LA7woaA0Qe4mFBNU0FKCZGMQWwMFcBivIo8f4L5cmCWdz5EOcMXVRyIZYZSmsAZzsfP7BaYVh1en3do65PsGcoZRYIItESE4C/AziaoXpMNN0Beq/d0+DyQJCle4wxLlOW+HfHt6QhlrFxUnQaPtm0xjuOi4cW2+m3XNh5zwuNe4Y60bIxDJrmqVZ8byEa/1SIJvtWRs1v1JQK4F1IyvcdgKcuuR6BDo/U3YF5vL7/we/cTXVPLnjPR9F0ufFHEYAwiacir+jsEfBxP6JSSTrGO83/KvGq7xM+k92r3P97x9alGxbPjnKMuRYHxCLFfSy1LllzJvBkfHq9IZSdKwhPj0J7RUD9Ew0AA7/xNZ7fPdjTouxbO2uvYOEpgkltmmYJ5ypeSWVw9WtRNB0O+k/FJ0iT2jNy//sHh5Rm67wXGg6zMWaCB/hJDGhqaEv7+H2OXW0Ff8iIeAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/21d62c8654d10e5d942b1712a230ff6e/93f02/1st_ranked.png","srcSet":"/static/21d62c8654d10e5d942b1712a230ff6e/02437/1st_ranked.png 750w,\n/static/21d62c8654d10e5d942b1712a230ff6e/e8771/1st_ranked.png 1080w,\n/static/21d62c8654d10e5d942b1712a230ff6e/44ee1/1st_ranked.png 1366w,\n/static/21d62c8654d10e5d942b1712a230ff6e/93f02/1st_ranked.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/21d62c8654d10e5d942b1712a230ff6e/06597/1st_ranked.webp 750w,\n/static/21d62c8654d10e5d942b1712a230ff6e/94e4c/1st_ranked.webp 1080w,\n/static/21d62c8654d10e5d942b1712a230ff6e/4094c/1st_ranked.webp 1366w,\n/static/21d62c8654d10e5d942b1712a230ff6e/da0c3/1st_ranked.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5645833333333333}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/2021_ai_online_competition/"}},"primaryTag":"attention"}},
    "staticQueryHashes": ["3170763342","3229353822"]}