{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/conformer/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Conformer: Convolution-augmented Transformer for Speech Recognition</h1>\n<p>Anmol Gulati et al.<br>\nGoogle Inc.<br>\nINTERSPEECH, 2020</p>\n<hr>\n<h2>Reference</h2>\n<ul>\n<li><a href=\"https://arxiv.org/pdf/2005.08100.pdf\">Conformer</a></li>\n<li><a href=\"https://arxiv.org/abs/1706.03762\">Attention Is All You Need</a></li>\n<li><a href=\"https://hichoe95.tistory.com/48\">Convolution</a></li>\n</ul>\n<hr>\n<h2>Summary</h2>\n<ul>\n<li>Transformer 기반 모델이 음성인식 분야에서 좋은 성능을 보이고 있음</li>\n<li>Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는 부족하다는 단점이 있음</li>\n<li>반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는 적당한 dilation과 깊은 구조를 가져야 함</li>\n<li>이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한 transformer + CNN 결합구조인 Conformer 구조 제안</li>\n<li>Conformer Encoder + Transducer 구조</li>\n</ul>\n<hr>\n<h2>Conformer Encoder</h2>\n<p>기존 트랜스포머 블록과 다르게 2개의 Feed Forward Network (FFN)에 쌓인 Sandwich 방식으로 구성</p>\n<h3>Conformer encoder model architecture</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/105320076-16af9980-5c09-11eb-86ec-b5146ac65812.png\" height=\"500\">   \n<p>기존 트랜스포머 인코더 블록은 <code class=\"language-text\">Multi Head Self Attention (MHSA) → LayerNorm → Feed Forward Network (FFN) → LayerNorm</code> 구조에서 <code class=\"language-text\">FFN Module → MHSA Module → Conv Module → FFN Module → LayerNorm</code> 구조로 변경</p>\n<h3>Multi-Headed Self-Attention Module</h3>\n<img src=\"https://images.deepai.org/converted-papers/2005.08100/x3.png\">  \n<ul>\n<li>Relative positional encoding</li>\n</ul>\n<blockquote>\n<p>절대적인 position 정보를 더하는 방식이 아닌, 상대적인 position 정보를 주는 방식\n절대적인 position 정보가 a=1, b=2와 같이 값을 지정하고 그 값의 차이를 계산하는 방식이라면, 상대적인 position 정보는 a=1, b=2이든 a=5, b=6이든 상관없이 두 수(위치)의 차이가 1이라는 것만 알려주면 되는 방식</p>\n</blockquote>\n<blockquote>\n<p>이런 방식은 가변적인 시퀀스 길이 인풋에 대해 인코더를 robust하게 만들어 줌</p>\n</blockquote>\n<ul>\n<li>Pre-norm</li>\n</ul>\n<blockquote>\n<p>기존 트랜스포머는 Post-norm인데 반해, pre-norm 적용<br>\n이전 연구들에서 pre-norm은 깊은 모델 학습이 원활하게 되도록 도와주는 효과가 있다고 알려짐</p>\n</blockquote>\n<h3>Convolution Module</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/105454437-30aeb200-5cc5-11eb-8624-1ea49b71c8cd.png\">\n<ul>\n<li>Pointwise Conv</li>\n</ul>\n<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb8hxuL%2Fbtqw5f6QxMM%2Fk4gn4DUTEqPkqbJXusPAKk%2Fimg.png\">  \n<blockquote>\n<p>kernel size가 1x1로 고정된 convolution\ndimension을 맞출 때 자주 쓰임</p>\n</blockquote>\n<ul>\n<li>GLU Activation</li>\n</ul>\n<img src=\"https://miro.medium.com/max/1400/1*EwUvi3ATcVoa9Lm-2FwNUA.png\" height=\"50\">  \n<img src=\"https://miro.medium.com/max/1400/1*4UZTVLQZSDV7gCsw2cn16Q.png\" height=\"200\">\n<ul>\n<li>Depthwise Conv</li>\n</ul>\n<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbw6Am5%2Fbtqw4n45UWN%2FqNYnywQjSGkzkOtl5Pkzc1%2Fimg.png\">  \n<blockquote>\n<p>그룹이 채널수와 같은 Group-Convolution. 각 channel마다의 spatial feature를 추출하기 위해 고안된 방법</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token operator\">>></span><span class=\"token operator\">></span> nn<span class=\"token punctuation\">.</span>Conv<span class=\"token punctuation\">(</span>in_channels<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> out_channels<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> group<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>Swish activation</li>\n</ul>\n<img src=\"https://blog.kakaocdn.net/dn/QbxpI/btqEHxducIg/hrmYfDLHDT4N1oqCtt74CK/img.png\">\n<h3>Feed Forward Module</h3>\n<img src=\"https://user-images.githubusercontent.com/1694368/103190710-1b847480-490d-11eb-8ea5-280749a32a24.png\">\n<blockquote>\n<p>Pre-norm 적용<br>\nSwish activation : regularizing에 도움</p>\n</blockquote>\n<hr>\n<h2>Conformer Block</h2>\n<p><a href=\"https://arxiv.org/pdf/1906.02762.pdf\">Macaron-Net</a>에 영감을 받아서 2개의 FFN에 쌓인 Sandwich 구조로 구성.<br>\nFFN 모듈에 half-step residual connection 적용</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/105326425-13b8a700-5c11-11eb-804c-bd8efef6060b.png\" height=\"200\">  \n<blockquote>\n<p>뒤의 Ablation study에서 Macaron-net FFN과 half-step residual connection이 성능 향상에 많은 기여를 했다고 함</p>\n</blockquote>\n<hr>\n<h2>Experiment</h2>\n<h3>Data</h3>\n<ul>\n<li>LibriSpeech</li>\n<li>80 channel filterbank, 25ms window, 10ms stride</li>\n<li>SpecAugment (F=27), ten time masks (maximum ratio 0.05)</li>\n</ul>\n<h3>Conformer Transducer</h3>\n<ul>\n<li>Three models: 10M, 30M, and 118M params</li>\n<li>Decoder: single LSTM-layer (Transducer)</li>\n<li>Dropout ratio: 0.1</li>\n<li>Adam optimizer, β1=0.9, β2=0.98 and έ=10^-9</li>\n<li>Learning rate scheduler: transformer lr scheduler, 10k warm-up steps</li>\n<li>3-layer LSTM language model (LM) with 4096 hidden dimension (shallow fusion)</li>\n</ul>\n<h3>Results on LibriSpeech</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/105327556-5cbd2b00-5c12-11eb-8714-2c0ce2c7a1b0.png\">  \n<hr>\n<img src=\"https://user-images.githubusercontent.com/42150335/105327620-752d4580-5c12-11eb-9091-433ce8700141.png\">\n<ul>\n<li>Conformer Block vs Transformer Block (without external LM)</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/105327876-c9d0c080-5c12-11eb-8b02-948f87c5f47d.png\">\n<hr>\n<img src=\"https://user-images.githubusercontent.com/42150335/105328157-1916f100-5c13-11eb-9473-69ac0c658e15.png\">  \n<hr>\n<img src=\"https://user-images.githubusercontent.com/42150335/105328196-2338ef80-5c13-11eb-9e8a-50ff45bad7b5.png\">\n<hr>\n<img src=\"https://user-images.githubusercontent.com/42150335/105328376-54b1bb00-5c13-11eb-9059-38bc7361ba6d.png\">\n<hr>\n<img src=\"https://user-images.githubusercontent.com/42150335/105328408-5aa79c00-5c13-11eb-94b2-8ee455c8daca.png\">\n<h2>Conclusion</h2>\n<p>Transformer + CNN 구조인 Conformer를 제안했고, 이를 잘 결합하기 위한 다양한 실험을 해서 결과를 냄.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Conformer: Convolution-augmented Transformer for Speech Recognition"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Anmol Gulati et al."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nGoogle Inc."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nINTERSPEECH, 2020"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Reference"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/2005.08100.pdf"},"children":[{"type":"text","value":"Conformer"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1706.03762"},"children":[{"type":"text","value":"Attention Is All You Need"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://hichoe95.tistory.com/48"},"children":[{"type":"text","value":"Convolution"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Summary"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Transformer 기반 모델이 음성인식 분야에서 좋은 성능을 보이고 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는 부족하다는 단점이 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는 적당한 dilation과 깊은 구조를 가져야 함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한 transformer + CNN 결합구조인 Conformer 구조 제안"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Conformer Encoder + Transducer 구조"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conformer Encoder"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기존 트랜스포머 블록과 다르게 2개의 Feed Forward Network (FFN)에 쌓인 Sandwich 방식으로 구성"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Conformer encoder model architecture"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105320076-16af9980-5c09-11eb-86ec-b5146ac65812.png","height":500},"children":[]},{"type":"text","value":"   \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기존 트랜스포머 인코더 블록은 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"Multi Head Self Attention (MHSA) → LayerNorm → Feed Forward Network (FFN) → LayerNorm"}]},{"type":"text","value":" 구조에서 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"FFN Module → MHSA Module → Conv Module → FFN Module → LayerNorm"}]},{"type":"text","value":" 구조로 변경"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Multi-Headed Self-Attention Module"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://images.deepai.org/converted-papers/2005.08100/x3.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Relative positional encoding"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"절대적인 position 정보를 더하는 방식이 아닌, 상대적인 position 정보를 주는 방식\n절대적인 position 정보가 a=1, b=2와 같이 값을 지정하고 그 값의 차이를 계산하는 방식이라면, 상대적인 position 정보는 a=1, b=2이든 a=5, b=6이든 상관없이 두 수(위치)의 차이가 1이라는 것만 알려주면 되는 방식"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이런 방식은 가변적인 시퀀스 길이 인풋에 대해 인코더를 robust하게 만들어 줌"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pre-norm"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"기존 트랜스포머는 Post-norm인데 반해, pre-norm 적용"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이전 연구들에서 pre-norm은 깊은 모델 학습이 원활하게 되도록 도와주는 효과가 있다고 알려짐"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Convolution Module"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105454437-30aeb200-5cc5-11eb-8624-1ea49b71c8cd.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pointwise Conv"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb8hxuL%2Fbtqw5f6QxMM%2Fk4gn4DUTEqPkqbJXusPAKk%2Fimg.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"kernel size가 1x1로 고정된 convolution\ndimension을 맞출 때 자주 쓰임"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"GLU Activation"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://miro.medium.com/max/1400/1*EwUvi3ATcVoa9Lm-2FwNUA.png","height":50},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"img","properties":{"src":"https://miro.medium.com/max/1400/1*4UZTVLQZSDV7gCsw2cn16Q.png","height":200},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Depthwise Conv"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbw6Am5%2Fbtqw4n45UWN%2FqNYnywQjSGkzkOtl5Pkzc1%2Fimg.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그룹이 채널수와 같은 Group-Convolution. 각 channel마다의 spatial feature를 추출하기 위해 고안된 방법"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"python"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-python"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-python"]},"children":[{"type":"element","tagName":"span","properties":{"className":["token","operator"]},"children":[{"type":"text","value":">>"}]},{"type":"element","tagName":"span","properties":{"className":["token","operator"]},"children":[{"type":"text","value":">"}]},{"type":"text","value":" nn"},{"type":"element","tagName":"span","properties":{"className":["token","punctuation"]},"children":[{"type":"text","value":"."}]},{"type":"text","value":"Conv"},{"type":"element","tagName":"span","properties":{"className":["token","punctuation"]},"children":[{"type":"text","value":"("}]},{"type":"text","value":"in_channels"},{"type":"element","tagName":"span","properties":{"className":["token","operator"]},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"span","properties":{"className":["token","number"]},"children":[{"type":"text","value":"10"}]},{"type":"element","tagName":"span","properties":{"className":["token","punctuation"]},"children":[{"type":"text","value":","}]},{"type":"text","value":" out_channels"},{"type":"element","tagName":"span","properties":{"className":["token","operator"]},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"span","properties":{"className":["token","number"]},"children":[{"type":"text","value":"10"}]},{"type":"element","tagName":"span","properties":{"className":["token","punctuation"]},"children":[{"type":"text","value":","}]},{"type":"text","value":" group"},{"type":"element","tagName":"span","properties":{"className":["token","operator"]},"children":[{"type":"text","value":"="}]},{"type":"element","tagName":"span","properties":{"className":["token","number"]},"children":[{"type":"text","value":"10"}]},{"type":"element","tagName":"span","properties":{"className":["token","punctuation"]},"children":[{"type":"text","value":")"}]}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Swish activation"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://blog.kakaocdn.net/dn/QbxpI/btqEHxducIg/hrmYfDLHDT4N1oqCtt74CK/img.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Feed Forward Module"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/1694368/103190710-1b847480-490d-11eb-8ea5-280749a32a24.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pre-norm 적용"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nSwish activation : regularizing에 도움"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conformer Block"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1906.02762.pdf"},"children":[{"type":"text","value":"Macaron-Net"}]},{"type":"text","value":"에 영감을 받아서 2개의 FFN에 쌓인 Sandwich 구조로 구성."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nFFN 모듈에 half-step residual connection 적용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105326425-13b8a700-5c11-11eb-804c-bd8efef6060b.png","height":200},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"뒤의 Ablation study에서 Macaron-net FFN과 half-step residual connection이 성능 향상에 많은 기여를 했다고 함"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Data"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"LibriSpeech"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"80 channel filterbank, 25ms window, 10ms stride"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"SpecAugment (F=27), ten time masks (maximum ratio 0.05)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Conformer Transducer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Three models: 10M, 30M, and 118M params"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Decoder: single LSTM-layer (Transducer)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Dropout ratio: 0.1"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Adam optimizer, β1=0.9, β2=0.98 and έ=10^-9"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Learning rate scheduler: transformer lr scheduler, 10k warm-up steps"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"3-layer LSTM language model (LM) with 4096 hidden dimension (shallow fusion)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Results on LibriSpeech"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105327556-5cbd2b00-5c12-11eb-8714-2c0ce2c7a1b0.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105327620-752d4580-5c12-11eb-9091-433ce8700141.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Conformer Block vs Transformer Block (without external LM)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105327876-c9d0c080-5c12-11eb-8b02-948f87c5f47d.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105328157-1916f100-5c13-11eb-9473-69ac0c658e15.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105328196-2338ef80-5c13-11eb-9e8a-50ff45bad7b5.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105328376-54b1bb00-5c13-11eb-9059-38bc7361ba6d.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/105328408-5aa79c00-5c13-11eb-94b2-8ee455c8daca.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conclusion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Transformer + CNN 구조인 Conformer를 제안했고, 이를 잘 결합하기 위한 다양한 실험을 해서 결과를 냄."}]}],"data":{"quirksMode":false}},"excerpt":"Conformer: Convolution-augmented Transformer for Speech Recognition Anmol Gulati et al. Google Inc. INTERSPEECH, 2020 Reference Conformer…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"Conformer Paper Review","userDate":"30 August 2020","date":"2020-08-30T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/5ca9b355b1dc4393c360f527478a3ba2/503d5/conformer.png","srcSet":"/static/5ca9b355b1dc4393c360f527478a3ba2/bfaac/conformer.png 750w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/abf2b/conformer.png 1080w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/6c19a/conformer.png 1366w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/503d5/conformer.png 1890w","sizes":"100vw"},"sources":[{"srcSet":"/static/5ca9b355b1dc4393c360f527478a3ba2/1e5e2/conformer.webp 750w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/77f81/conformer.webp 1080w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/b9a60/conformer.webp 1366w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/2837d/conformer.webp 1890w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6888888888888889}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"19ded62e-3e91-5733-9329-a1c7bdcf859b","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"35eadfc7-646b-5194-a711-ce20e840ba58","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"fd3185b8-63e4-5e3d-aeb3-e67ed1343af9","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"aad087b1-4b0f-5956-ab2f-d7ab33fdb8c4","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/conformer/","prev":{"excerpt":"ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers image 논문링크 2020-04-2…","frontmatter":{"title":"ClovaCall Paper Review","tags":["speech","paper"],"date":"2020-03-13T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA6klEQVQY011P2Y6DMAzk/38PWgLhSiEpRxKgWl5QYdYO7T6sJWsce8aeRMdxQKkGRVlAPR4oCgkhBOq6RlmWkDLH7X5DlgnkUqKqK+R5Tu8MqmkQxzH8PIPjPE9EXCilSChpaRnIvDwjvIQizNI0pQMVmqaGoB5r+GiSJPjZNnwj6roOk50w0xXnHJz38JQbkaZphHUWy7JQ34X5TDW/rbVBw9j3PfphwL7viIwxJJzAyAPzfKJtW4zjSD0d+t/UHxxIrLUO/Itngm59va4vcxzv9991dsLulnXFSjlSzW787C8kt+yM83/8Alz0dI6i3dakAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/d7bd082afa8ae681e2420238dc3cd6d0/c2b97/clovacall.png","srcSet":"/static/d7bd082afa8ae681e2420238dc3cd6d0/51d72/clovacall.png 750w,\n/static/d7bd082afa8ae681e2420238dc3cd6d0/b9600/clovacall.png 1080w,\n/static/d7bd082afa8ae681e2420238dc3cd6d0/c2b97/clovacall.png 1322w","sizes":"100vw"},"sources":[{"srcSet":"/static/d7bd082afa8ae681e2420238dc3cd6d0/1f497/clovacall.webp 750w,\n/static/d7bd082afa8ae681e2420238dc3cd6d0/8f986/clovacall.webp 1080w,\n/static/d7bd082afa8ae681e2420238dc3cd6d0/ffe85/clovacall.webp 1322w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.23600605143721634}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"12 min read"},"layout":"","slug":"/clovacall/"}},"next":{"excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","tags":["speech","paper"],"date":"2020-09-12T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACV0lEQVQoz22S6U8TURTF+ycqQUOQlkJAgygBEYVQFELZbGLiJ/8AEv1mNNEYazQhwabsSy1LF+xC95YuM21n6/x8M/2oL7mT985958x9514H/1mm2RFfK0zMTgfTUKiUizSbMqrS5vo6TbGQIZ/PinOry+kYgmfisMilUolEIkG73Rak7oWyBLmakDUhnGxRlTTklkalKtv5WM5AFf+0Il7odEVFOBQhkkwmicfjlCsVFEVFb8Qop3dJRwO0iwckLn9RzexxI7BMdAu1ckIsvIVcOLIjFQmg3Zyiay0c/z4X2uFlOO5D3XNiHDjhZJBWsJ/CjzsUf96ltt1nY+axC/PIJfYu9EM3Sj3ZFdQ0TVSm2B5Ygq34Jkp4ifqJF/ViHSnkpfl7ldL+EqltD3JoRZy9aJcbKOdrtMIrqJc+VDmPw/Lt/PyM01CIYrFIrVZD0w272shVmLrcQGlLXFyd2difVIxcKQ+GRiRxga5rwqos6ULazjskSSISiRCNRcnmsjQakp0IJxpsh3Jip/Nlp0C13hAkGf9+xnoT/oMK14UbarLK52CGjtFtpqPZbNpNSaVSdlOslSrr+L7Baz98PYX5j/DhEN5+b+J5XxaYyYLANgPwLggLn2AvYTXAEB4K0zpi1nRdt8Oau8DOIeu+N3hXfUzOvGBpeZVHU3MMjYwzMTlNv3sMl3uE++NTOIfHGBwaZdazLPhCUNNUew7z+TzVahXDMNjdCfJqY40FzzxzszMsLr5k4vE4zoF7TD+Zwj04QG9vDw/HHjA6MkzP7Vs8f/bULugvq76654K/U9cAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/038c6/wav2vec2.png","srcSet":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/e6cc4/wav2vec2.png 750w,\n/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/038c6/wav2vec2.png 842w","sizes":"100vw"},"sources":[{"srcSet":"/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/79bef/wav2vec2.webp 750w,\n/static/ef0fb6095c0dcc78d73cfdf24bc9ac11/daf06/wav2vec2.webp 842w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5106888361045131}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"5 min read"},"layout":"","slug":"/wav2vec2/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}