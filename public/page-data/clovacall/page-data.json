{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/clovacall/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers</h1>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/80241423-8a367180-869e-11ea-8438-6b651ab65fb6.png\" alt=\"image\"></p>\n<p><a href=\"https://arxiv.org/abs/2004.09367\">논문링크</a></p>\n<p>2020-04-20에 클로바에서 공개한 따끈따끈한 논문입니다. 한국어 음성 데이터셋 공개와 더불어 베이스라인 코드와 AI Hub, 공개한 데이터셋에 대한 학습 결과까지 포함하고 있습니다. 현재 제가 진행하는 프로젝트와 완전히 동일한 주제이면서도 같은 데이터셋 (AI Hub) 을 이용한 학습까지 했다고하니 굉장히 기대하면서 읽었습니다. 게다가 논문을 낸 기관이 네이버 Clova여서 더 기대가 됐네요.</p>\n<h2>Abstract</h2>\n<p>ASR은 여러 어플리케이션에서 필수적입니다만, License-free한 데이터는 찾기 힘들고, 유명한 Switchboard와 같은 데이터는 조금 구식입니다. 또한 가장 큰 문제점은 이러한 오픈 데이터는 대부분 영어로 이루어져 있다는 점입니다. 그래서 본 논문에서는 대용량의 한국어 음성 데이터셋을 공개한다고 밝힙니다. 11,000명의 화자에게서 얻은 60,000 쌍의 음성 데이터셋입니다. (1,000시간의 AI Hub 데이터셋이 2,000명의 화자로 이루어진 점과 비교하여 다양한 화자들로 구성된 데이터셋임을 알 수 있습니다.) 또한 해당 데이터셋을 검증하기 위한 베이스라인 코드를 같이 공개했습니다. 해당 데이터셋 및 코드는 <a href=\"https://github.com/ClovaAI/ClovaCall\">이곳</a>에 공개되어 있습니다.</p>\n<h2>1. Introduction</h2>\n<p>음성인식 서비스는 다양한 분야에 적용가능한 핵심 기술입니다. 하지만 이러한 음성인식 서비스 개발을 위한 오픈 데이터들은 (Wall Street Journal (WSJ), TIMIT, Switchboard, CallHome, Librispeech) 공개된 지 오래된 구식의 데이터들이며, 모두 영어로 이루어져 있습니다. 물론 <a href=\"http://www.aihub.or.kr/aidata/105\">AI Hub</a>와 <a href=\"https://github.com/goodatlas/zeroth\">zeroth</a>와 같이 오픈된 한국어 음성 데이터셋도 있지만, 이 데이터셋은 일상대화와 같은 주제들을 다룬 데이터셋입니다. 이러한 데이터셋들을 사용하게 되면 특정 도메인을 타겟으로 한 태스크에서는 성능이 좋지 않습니다. 그래서 본 논문에서는 음식점을 도메인으로한 데이터셋을 공개했다고 밝힙니다. 대부분의 문장은 10초 이내의 짧은 발화로 구성되어 있으며, End-point detection이나 alignment가 이슈가 되지 않는다고 합니다.</p>\n<p>또한 본 논문에서는 이러한 데이터셋이 어느 정도의 성능을 낼 수 있는지를 보이기 위해 Deep Speech 2 (DS2)와 Listen, Attend and Spell (LAS)의 2개의 모델로 실험을 진행했다고 밝힙니다. 이때 <strong>Pretraining-finetuning</strong>, <strong>from-scratch training</strong>, <strong>scratch training with data augmentation</strong> 의 3가지 방법을 통해 비교했다고 합니다. 이에 더하여, 비교를 위해 이미 공개되어 있는 2개의 한국어 음성 데이터셋에 대해서도 같이 실험을 진행했습니다. 2개의 데이터셋은 QA Dataset (task-specific)과 AI Hub Dataset (dialog)를 의미합니다.</p>\n<h2>2. Related Work</h2>\n<p>본 장에서는 기존의 여러 데이터셋들에 대해 소개하고 있습니다. WSJ, TIMIT, Switchboard, CallHome, LibriSpeech 등의 데이터셋을 소개하고 있으며, 해당 데이터셋들은 여러 ASR 모델들의 성능을 비교하는 벤치마크로 사용되고 있습니다. 하지만 다시 강조하듯이, 이러한 데이터셋들은 일상 대화라는 주제를 가진 데이터셋이며, 특정 도메인을 주제로한 데이터셋들은 거의 오픈되지 않습니다. 또한 이렇게 오픈된 일상 대화 데이터셋들은 특정 도메인을 타겟으로 한 모델의 Pretraining 데이터셋이 될 수 있지만, 일상 대화라는 도메인과 특정 도메인을 타겟으로하는 데이터셋은 서로 꽤나 상이하여 Pretraining 하더라도 좋은 결과를 내지 못하고 있다고 합니다.</p>\n<h2>3. Korean Clova Call Speech Corpus</h2>\n<h3>3.1 Naver Clova AI for Contact Center</h3>\n<p>ClovaCall 데이터셋 구축은 <em>NAVER Clova AI for Contact Center (AICC)</em> 프로젝트의 메인 주제였습니다. ClovaCall 데이터셋은 자연어 이해, 음성인식, 음성합성 등에 활용될 수 있으며, ‘음식점 예약’이라는 시나리오를 주제를 목적으로한 대용량 데이터셋임을 다시 한번 강조합니다.</p>\n<h3>3.2 Data Cconstruction from Humans</h3>\n<p>데이터 구축 과정에 대해 설명합니다. 데이터 구축은 다음 과정을 거쳤다고 합니다.</p>\n<ol>\n<li>\n<p>making a Sentence Pool (어떤 문장들로 데이터셋을 구성할지)</p>\n</li>\n<li>\n<p>call-based recording (전화상으로 들어온 소리를 녹음)</p>\n</li>\n<li>\n<p>refining the recorded speech data (데이터 정제)</p>\n</li>\n</ol>\n<p>자세한 내용은 논문을 참고해주시면 되겠습니다. (데이터 구축 과정이 제 관심분야는 아닌지라 ㅎㅎ;;)</p>\n<h3>3.3 Statistical Analysis</h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/80275445-3a968b00-871c-11ea-86f4-296e88ba1269.png\" alt=\"image\"></p>\n<p>본 논문에서는 ClovaCall 데이터셋과 AI Hub 데이터셋을 분석한 결과를 보이고 있습니다. <strong>단어, 문자, 음소, 발화 길이</strong>를 분석하여 막대그래프로 표현했습니다.</p>\n<h2>4. Speech Recognition Result</h2>\n<p>이제 드디어 제가 관심있는 파트가 등장했습니다.</p>\n<h3>4.1 Experimental Setup</h3>\n<h4>Dataset &#x26; Training Schemes</h4>\n<p>앞서 언급했듯이, <strong>Pretraining-finetuning</strong>, <strong>from-scratch training</strong>, <strong>scratch training with data augmentation</strong>와 같은 3가지 방식으로의 학습방식 결과를 비교했습니다.</p>\n<p>데이터셋은 총 3가지를 사용했습니다.</p>\n<ol>\n<li>\n<p><strong>AI Hub</strong> for Pretraining</p>\n</li>\n<li>\n<p><strong>QA Call</strong> for Finetuning</p>\n</li>\n<li>\n<p><strong>Clova Call</strong> for Finetuning</p>\n</li>\n</ol>\n<p>AI Hub 데이터셋은 Pretraining을 위해 사용되었고, QA Call 데이터셋은 Clova Call 데이터셋과의 성능 비교를 위해 사용되었습니다. 또한 Noise-Augmentation &#x26; Spec-Augmentation 2가지 기법을 각자 사용하여 결과를 비교했습니다.</p>\n<h4>Data preprocessing</h4>\n<p>먼저 AI Hub 데이터셋과 CLova Call 데이터셋의 샘플링 레이트가 서로 다릅니다. AI HUb는 16k의 샘플링 레이트를 가지는데 반해, Clova Call 데이터는 통화로부터 수집하다보니, 8k의 샘플링 레이트를 가집니다. 그래서 Clova Call 데이터를 Upsampling을 통해 8k => 16k의 샘플링 레이트를 가지도록 수정했습니다. 그리고 모든 모델은 <strong>log-spectrogram</strong>을 인풋으로 가지며, 20ms의 <em>window_size</em>와 10ms의 <em>stride</em>를 가집니다. (librosa 라이브러리를 사용했습니다.) 또한 모든 스펙트로그램은 <em>instance-wise standardazation</em> 방식으로 정규화를 진행했습니다.</p>\n<p>개인적으로 왜 Mel-Spectrogram이 아닌, 그냥 Spectrogram을 사용했는지에 대해 궁금증이 남아서, 공동 제 1저자 분 중 한분께 메일로 문의를 드렸습니다. 혹시 답변 해주신다면 답변 내용도 남기겠습니다.</p>\n<h4>ASR Models</h4>\n<p>ASR 모델로는 DeepSpeech2, Listen, Attend and Spell 2개의 아키텍처로 비교했습니다.</p>\n<p>DeepSpeech2 아키텍처는 <em>2D-Convolutional layer with 32 channel</em>를 포함하고 있으며, 5개의 Bidirectional LSTM layer로 구성되어 있습니다. (각 방향당 800개의 unit을 사용했습니다.) 또한 트레이닝은 CTC loss를 이용하여 학습했다고 합니다.</p>\n<p>LAS 아키텍쳐는 DeepSpeech2와 같은 <em>Convolution layer</em>를 포함시키고, 각 방향당 512개의 unit을 가진 3층의 <em>Bidirectional LSTM layer</em>로 인코더를 구성하며, 512개의 unit을 가진 2층의 <em>Unidirectional LSTM layer</em>로 디코더를 구성했다고 합니다. 또한 어텐션 매커니즘으로는 「Attention based Models for Speech Recognition」- <a href=\"https://github.com/sooftware/Paper-Review/blob/master/Review/Attention-Based%20Models%20for%20Speech%20Recognition.md\">Review Link</a>에서 제안한 Location Aware 어텐션 매커니즘을 사용했습니다.</p>\n<p>모델 평가 지표로는 Character Error Rate (CER)을 사용했으며 다음과 같은 Metric을 따릅니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/80277350-42a8f780-8729-11ea-9609-665ea1944f7b.png\" alt=\"image\"></p>\n<h3>4.2 Comparison Results on Datasets</h3>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/80277359-60765c80-8729-11ea-928f-1f9941c7f36a.png\" alt=\"image\"></p>\n<p>이제 결과를 비교하는 시간입니다. 결과만 놓고보자면, AI Hub 데이터셋으로 Pretraining 한 후, ClovaCall 데이터셋으로 Finetuning한 LAS 모델이 가장 좋은 성능 (Error Rate 7%) 을 냈습니다.</p>\n<p>또한 주목할만한 점은, AI Hub 데이터셋만으로 학습시킨 모델의 경우, Specific-domain의 데이터로 테스트 했을 때, 성능이 매우 저조했다는 점입니다. 가장 좋은 성능을 낸 LAS 모델로 비교를 하자면 Error Rate 69.2%를 기록했네요.</p>\n<p>또한 본 실험에서는 Data Augmentation이 의미있는 결과를 내지 못했습니다. 본 논문에서는 이를 이미 노이즈가 낀 상황에서 데이터를 만들었다는 점과, 파라미터 수가 작은 LAS 모델에서 Spec Augmentation을 적용했다는 점이 의미있는 효과를 내지 못한 점으로 꼽았습니다.</p>\n<h2>5. Concluding Remarks</h2>\n<p>결국 이 논문의 핵심을 특정 도메인을 대상으로 한 ASR 시스템을 구축하기 위해서는 해당 도메인의 데이터셋이 필요한데, 저희가 그런 ‘음식점 예약’ 도메인에서의 큰 데이터셋을 공개했고, 이 데이터셋을 사용했더니 결과가 좋았습니다 ! 를 강조한 논문이였습니다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/80241423-8a367180-869e-11ea-8438-6b651ab65fb6.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/2004.09367"},"children":[{"type":"text","value":"논문링크"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"2020-04-20에 클로바에서 공개한 따끈따끈한 논문입니다. 한국어 음성 데이터셋 공개와 더불어 베이스라인 코드와 AI Hub, 공개한 데이터셋에 대한 학습 결과까지 포함하고 있습니다. 현재 제가 진행하는 프로젝트와 완전히 동일한 주제이면서도 같은 데이터셋 (AI Hub) 을 이용한 학습까지 했다고하니 굉장히 기대하면서 읽었습니다. 게다가 논문을 낸 기관이 네이버 Clova여서 더 기대가 됐네요."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"ASR은 여러 어플리케이션에서 필수적입니다만, License-free한 데이터는 찾기 힘들고, 유명한 Switchboard와 같은 데이터는 조금 구식입니다. 또한 가장 큰 문제점은 이러한 오픈 데이터는 대부분 영어로 이루어져 있다는 점입니다. 그래서 본 논문에서는 대용량의 한국어 음성 데이터셋을 공개한다고 밝힙니다. 11,000명의 화자에게서 얻은 60,000 쌍의 음성 데이터셋입니다. (1,000시간의 AI Hub 데이터셋이 2,000명의 화자로 이루어진 점과 비교하여 다양한 화자들로 구성된 데이터셋임을 알 수 있습니다.) 또한 해당 데이터셋을 검증하기 위한 베이스라인 코드를 같이 공개했습니다. 해당 데이터셋 및 코드는 "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/ClovaAI/ClovaCall"},"children":[{"type":"text","value":"이곳"}]},{"type":"text","value":"에 공개되어 있습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"1. Introduction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"음성인식 서비스는 다양한 분야에 적용가능한 핵심 기술입니다. 하지만 이러한 음성인식 서비스 개발을 위한 오픈 데이터들은 (Wall Street Journal (WSJ), TIMIT, Switchboard, CallHome, Librispeech) 공개된 지 오래된 구식의 데이터들이며, 모두 영어로 이루어져 있습니다. 물론 "},{"type":"element","tagName":"a","properties":{"href":"http://www.aihub.or.kr/aidata/105"},"children":[{"type":"text","value":"AI Hub"}]},{"type":"text","value":"와 "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/goodatlas/zeroth"},"children":[{"type":"text","value":"zeroth"}]},{"type":"text","value":"와 같이 오픈된 한국어 음성 데이터셋도 있지만, 이 데이터셋은 일상대화와 같은 주제들을 다룬 데이터셋입니다. 이러한 데이터셋들을 사용하게 되면 특정 도메인을 타겟으로 한 태스크에서는 성능이 좋지 않습니다. 그래서 본 논문에서는 음식점을 도메인으로한 데이터셋을 공개했다고 밝힙니다. 대부분의 문장은 10초 이내의 짧은 발화로 구성되어 있으며, End-point detection이나 alignment가 이슈가 되지 않는다고 합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한 본 논문에서는 이러한 데이터셋이 어느 정도의 성능을 낼 수 있는지를 보이기 위해 Deep Speech 2 (DS2)와 Listen, Attend and Spell (LAS)의 2개의 모델로 실험을 진행했다고 밝힙니다. 이때 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Pretraining-finetuning"}]},{"type":"text","value":", "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"from-scratch training"}]},{"type":"text","value":", "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"scratch training with data augmentation"}]},{"type":"text","value":" 의 3가지 방법을 통해 비교했다고 합니다. 이에 더하여, 비교를 위해 이미 공개되어 있는 2개의 한국어 음성 데이터셋에 대해서도 같이 실험을 진행했습니다. 2개의 데이터셋은 QA Dataset (task-specific)과 AI Hub Dataset (dialog)를 의미합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"2. Related Work"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 장에서는 기존의 여러 데이터셋들에 대해 소개하고 있습니다. WSJ, TIMIT, Switchboard, CallHome, LibriSpeech 등의 데이터셋을 소개하고 있으며, 해당 데이터셋들은 여러 ASR 모델들의 성능을 비교하는 벤치마크로 사용되고 있습니다. 하지만 다시 강조하듯이, 이러한 데이터셋들은 일상 대화라는 주제를 가진 데이터셋이며, 특정 도메인을 주제로한 데이터셋들은 거의 오픈되지 않습니다. 또한 이렇게 오픈된 일상 대화 데이터셋들은 특정 도메인을 타겟으로 한 모델의 Pretraining 데이터셋이 될 수 있지만, 일상 대화라는 도메인과 특정 도메인을 타겟으로하는 데이터셋은 서로 꽤나 상이하여 Pretraining 하더라도 좋은 결과를 내지 못하고 있다고 합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"3. Korean Clova Call Speech Corpus"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"3.1 Naver Clova AI for Contact Center"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"ClovaCall 데이터셋 구축은 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"NAVER Clova AI for Contact Center (AICC)"}]},{"type":"text","value":" 프로젝트의 메인 주제였습니다. ClovaCall 데이터셋은 자연어 이해, 음성인식, 음성합성 등에 활용될 수 있으며, ‘음식점 예약’이라는 시나리오를 주제를 목적으로한 대용량 데이터셋임을 다시 한번 강조합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"3.2 Data Cconstruction from Humans"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"데이터 구축 과정에 대해 설명합니다. 데이터 구축은 다음 과정을 거쳤다고 합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"making a Sentence Pool (어떤 문장들로 데이터셋을 구성할지)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"call-based recording (전화상으로 들어온 소리를 녹음)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"refining the recorded speech data (데이터 정제)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"자세한 내용은 논문을 참고해주시면 되겠습니다. (데이터 구축 과정이 제 관심분야는 아닌지라 ㅎㅎ;;)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"3.3 Statistical Analysis"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/80275445-3a968b00-871c-11ea-86f4-296e88ba1269.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 ClovaCall 데이터셋과 AI Hub 데이터셋을 분석한 결과를 보이고 있습니다. "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"단어, 문자, 음소, 발화 길이"}]},{"type":"text","value":"를 분석하여 막대그래프로 표현했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"4. Speech Recognition Result"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이제 드디어 제가 관심있는 파트가 등장했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"4.1 Experimental Setup"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h4","properties":{},"children":[{"type":"text","value":"Dataset & Training Schemes"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"앞서 언급했듯이, "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Pretraining-finetuning"}]},{"type":"text","value":", "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"from-scratch training"}]},{"type":"text","value":", "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"scratch training with data augmentation"}]},{"type":"text","value":"와 같은 3가지 방식으로의 학습방식 결과를 비교했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"데이터셋은 총 3가지를 사용했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"AI Hub"}]},{"type":"text","value":" for Pretraining"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"QA Call"}]},{"type":"text","value":" for Finetuning"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Clova Call"}]},{"type":"text","value":" for Finetuning"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"AI Hub 데이터셋은 Pretraining을 위해 사용되었고, QA Call 데이터셋은 Clova Call 데이터셋과의 성능 비교를 위해 사용되었습니다. 또한 Noise-Augmentation & Spec-Augmentation 2가지 기법을 각자 사용하여 결과를 비교했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h4","properties":{},"children":[{"type":"text","value":"Data preprocessing"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"먼저 AI Hub 데이터셋과 CLova Call 데이터셋의 샘플링 레이트가 서로 다릅니다. AI HUb는 16k의 샘플링 레이트를 가지는데 반해, Clova Call 데이터는 통화로부터 수집하다보니, 8k의 샘플링 레이트를 가집니다. 그래서 Clova Call 데이터를 Upsampling을 통해 8k => 16k의 샘플링 레이트를 가지도록 수정했습니다. 그리고 모든 모델은 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"log-spectrogram"}]},{"type":"text","value":"을 인풋으로 가지며, 20ms의 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"window_size"}]},{"type":"text","value":"와 10ms의 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"stride"}]},{"type":"text","value":"를 가집니다. (librosa 라이브러리를 사용했습니다.) 또한 모든 스펙트로그램은 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"instance-wise standardazation"}]},{"type":"text","value":" 방식으로 정규화를 진행했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"개인적으로 왜 Mel-Spectrogram이 아닌, 그냥 Spectrogram을 사용했는지에 대해 궁금증이 남아서, 공동 제 1저자 분 중 한분께 메일로 문의를 드렸습니다. 혹시 답변 해주신다면 답변 내용도 남기겠습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h4","properties":{},"children":[{"type":"text","value":"ASR Models"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"ASR 모델로는 DeepSpeech2, Listen, Attend and Spell 2개의 아키텍처로 비교했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"DeepSpeech2 아키텍처는 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"2D-Convolutional layer with 32 channel"}]},{"type":"text","value":"를 포함하고 있으며, 5개의 Bidirectional LSTM layer로 구성되어 있습니다. (각 방향당 800개의 unit을 사용했습니다.) 또한 트레이닝은 CTC loss를 이용하여 학습했다고 합니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"LAS 아키텍쳐는 DeepSpeech2와 같은 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Convolution layer"}]},{"type":"text","value":"를 포함시키고, 각 방향당 512개의 unit을 가진 3층의 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Bidirectional LSTM layer"}]},{"type":"text","value":"로 인코더를 구성하며, 512개의 unit을 가진 2층의 "},{"type":"element","tagName":"em","properties":{},"children":[{"type":"text","value":"Unidirectional LSTM layer"}]},{"type":"text","value":"로 디코더를 구성했다고 합니다. 또한 어텐션 매커니즘으로는 「Attention based Models for Speech Recognition」- "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/sooftware/Paper-Review/blob/master/Review/Attention-Based%20Models%20for%20Speech%20Recognition.md"},"children":[{"type":"text","value":"Review Link"}]},{"type":"text","value":"에서 제안한 Location Aware 어텐션 매커니즘을 사용했습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델 평가 지표로는 Character Error Rate (CER)을 사용했으며 다음과 같은 Metric을 따릅니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/80277350-42a8f780-8729-11ea-9609-665ea1944f7b.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"4.2 Comparison Results on Datasets"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/80277359-60765c80-8729-11ea-928f-1f9941c7f36a.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이제 결과를 비교하는 시간입니다. 결과만 놓고보자면, AI Hub 데이터셋으로 Pretraining 한 후, ClovaCall 데이터셋으로 Finetuning한 LAS 모델이 가장 좋은 성능 (Error Rate 7%) 을 냈습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한 주목할만한 점은, AI Hub 데이터셋만으로 학습시킨 모델의 경우, Specific-domain의 데이터로 테스트 했을 때, 성능이 매우 저조했다는 점입니다. 가장 좋은 성능을 낸 LAS 모델로 비교를 하자면 Error Rate 69.2%를 기록했네요."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한 본 실험에서는 Data Augmentation이 의미있는 결과를 내지 못했습니다. 본 논문에서는 이를 이미 노이즈가 낀 상황에서 데이터를 만들었다는 점과, 파라미터 수가 작은 LAS 모델에서 Spec Augmentation을 적용했다는 점이 의미있는 효과를 내지 못한 점으로 꼽았습니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"5. Concluding Remarks"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"결국 이 논문의 핵심을 특정 도메인을 대상으로 한 ASR 시스템을 구축하기 위해서는 해당 도메인의 데이터셋이 필요한데, 저희가 그런 ‘음식점 예약’ 도메인에서의 큰 데이터셋을 공개했고, 이 데이터셋을 사용했더니 결과가 좋았습니다 ! 를 강조한 논문이였습니다."}]}],"data":{"quirksMode":false}},"excerpt":"ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers image 논문링크 2020-04-2…","fields":{"readingTime":{"text":"12 min read"}},"frontmatter":{"title":"ClovaCall Paper Review","userDate":"13 March 2020","date":"2020-03-13T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/d7bd082afa8ae681e2420238dc3cd6d0/c2b97/clovacall.png","srcSet":"/static/d7bd082afa8ae681e2420238dc3cd6d0/51d72/clovacall.png 750w,\n/static/d7bd082afa8ae681e2420238dc3cd6d0/b9600/clovacall.png 1080w,\n/static/d7bd082afa8ae681e2420238dc3cd6d0/c2b97/clovacall.png 1322w","sizes":"100vw"},"sources":[{"srcSet":"/static/d7bd082afa8ae681e2420238dc3cd6d0/1f497/clovacall.webp 750w,\n/static/d7bd082afa8ae681e2420238dc3cd6d0/8f986/clovacall.webp 1080w,\n/static/d7bd082afa8ae681e2420238dc3cd6d0/ffe85/clovacall.webp 1322w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.23600605143721634}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f8857a8d-9121-5e23-91bc-3fbe4f090418","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"93dec710-458c-531f-acbc-28d14f762768","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"a4f99081-c696-5b06-85b1-c2268aed1215","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"e1128eb2-c5b3-55e1-83ef-3b6478bb7d7b","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"52b143f2-f85b-5c62-911f-b1ebdf23fa89","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/clovacall/","prev":{"excerpt":"Beam Search (빔서치) 본 포스팅은 “빔서치”에 대한 본질적인 개념보다는 Encoder-Decoder 모델 (Seq2seq…","frontmatter":{"title":"Beam Search (빔서치)","tags":["nlp"],"date":"2020-02-14T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAACKklEQVQ4y3VUiXbaQAz0/39eG0LLEQeICYTG+NzbVkfybjAN3fd4ltfa2dGMREZY4zhSeqbffJmmoer3igbvp7wp+ZbAZ2KY3YHNQAbryB3fyWpNCoAfyyWFYaBBKQqnk+SErqNwPt+RyeYMCQdGAAx498ZQk+fkrCWDg9V2Kyl9WdJp8UwDYlVe6XO9uQecl+j7nnqUpgHiAdxsX+QSh/1y+YsMLrFc/mZDFuWbqqL29fU74ERuoBCClMV7TmmqN1sacZAZdwDlTAEBW4nBsMsfAeLHYCkWtmB5Blt+ToIFeThoWL0VErfXKxWrlZxNoFkyQgBnpgSUp4uCRux7gOjDQeIvlxm8aclGg9LKCAwG3OS4NNbuchF92JQO+rDLzKrb7Sb2+Da2LXlIo2BQGc1iQixbZnELb3rWDyCmOJJ1TuI2vwGKQVgaGl6el2SRb6sal+6E8RcgM+u4t2LJqRxhCFZcJrvMLTTiQI9qPqGtNDziNppyK3muYTRFgPHe7PfyzuBXABpIEnpFLurWXv5Q8fQknXEzJdotbRM/iOAoVVoCexqafaCZHaRQYHX88ZM6fPfoSQWzHrbNGBtbQ0+DgzIpL7loaABYrteThnVNJfrTRQ3V/vD/0QsAqjGbbBAzrGAE9yED8nSwLHwpGycEsG9i+d8m5d9/GJ6UEk3LJfMsnxcLUjDPI1asbTQlzfhDhvO55nFTKDsZ1oINhlJYJUDe17hkjvEXFk/eeR90j0UAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/2ff5ceb2a289c296c3f360461508704b/90853/beamsearch.png","srcSet":"/static/2ff5ceb2a289c296c3f360461508704b/d0f30/beamsearch.png 750w,\n/static/2ff5ceb2a289c296c3f360461508704b/90853/beamsearch.png 773w","sizes":"100vw"},"sources":[{"srcSet":"/static/2ff5ceb2a289c296c3f360461508704b/df98f/beamsearch.webp 750w,\n/static/2ff5ceb2a289c296c3f360461508704b/5b2f4/beamsearch.webp 773w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8214747736093144}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"9 min read"},"layout":"","slug":"/beamsearch/"}},"next":{"excerpt":"Conformer: Convolution-augmented Transformer for Speech Recognition Anmol Gulati et al. Google Inc. INTERSPEECH, 2020 Reference Conformer…","frontmatter":{"title":"Conformer Paper Review","tags":["speech","paper"],"date":"2020-08-30T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAACcElEQVQ4y3VUa3PSQBTN///mZz/5xRl1Ojo6Yx0Uq44tlRawPAsJISS0PJJAstm8H8e7QWqomsyyYS8595xz70WK4xi+7yNOEkT0nOU50jSF53mwTBPr9Rq73Q5BEEBcRVEc7Y8vKacAd11sWtdYnX/HztAREnCSxHCZh41pI4oiiMSPwUTSO2MOVR7RPiuJSCIQEUPndgS31wMnRpzYFMTU9TjWtkNgEan4w/AAKE9kvKud4tnJc7x4ewLHcfaAOd2KqaK7GMBLODzXw8I00JxclGto9BGFEQoUR4AzVUV7PEe9o+C8q4AxBinLMnicwdgomN71EcQe8rwoPWVBCH1lwmEueRgeyRUWTKcqvp6d4uOHl2hefi4tKD1MIx/F8gap1gBCC2kOJL4Jd3UDa9FGsFMQEsMqoCAync7QPHuD+qsn+PT66V6yCPg+h7M0YGoKsiigSlOSmCHiOhxLhs8WRwwfJM80dK5+4PxLDd12E5xzSMKXJM7Qv5qiUe+C2UHpVcx9sHsT94oBttogCI8lb7dbzOdz9IcjdAdDMCpsToWUxEdADN2NDvtORhZTgF5g1D5GvQat9h5m+xoBtQ4q7IQ8sZTBAL2LBtbEtmwbEc/SEKHdIkbfgNRGktIhJcqSHKblIia51cauNvVS09BtXEK7He8BBUMxKRl9ybP0wXAecqjLGdTVHLZnUVHCvyZFLFFUi+Tn2J9L1bGpDpPNLPRmP9FRWpgYt2TL/0ev+iwdMh0OBW1RLY/GjtESQKKpxfg9lpxQrwp1gr2YpN8eHmcU3S7LMnRdx3g8fvDuX+xs2y5/q5GPE9pd+k/4BZe/KyhkgKhoAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/5ca9b355b1dc4393c360f527478a3ba2/503d5/conformer.png","srcSet":"/static/5ca9b355b1dc4393c360f527478a3ba2/bfaac/conformer.png 750w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/abf2b/conformer.png 1080w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/6c19a/conformer.png 1366w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/503d5/conformer.png 1890w","sizes":"100vw"},"sources":[{"srcSet":"/static/5ca9b355b1dc4393c360f527478a3ba2/1e5e2/conformer.webp 750w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/77f81/conformer.webp 1080w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/b9a60/conformer.webp 1366w,\n/static/5ca9b355b1dc4393c360f527478a3ba2/2837d/conformer.webp 1890w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6888888888888889}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/conformer/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}