{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/tokenizer/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h2>Tokenization</h2>\n<p>문장에서 의미있는 단위로 나누는 작업을 <code class=\"language-text\">토큰화</code>라고 한다.</p>\n<ul>\n<li><strong>문자 단위 토큰화</strong>\n<ul>\n<li>문자 단위로 토큰화를 하는 것이다.</li>\n<li>한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 15,000개를 넘기 어렵다.</li>\n<li><code class=\"language-text\">장점</code> : 모든 문자를 포함시켜서 <code class=\"language-text\">UNK</code>토큰이 잘 발생하지 않는다.</li>\n<li><code class=\"language-text\">단점</code> : 의미 있는 단위가 되기 어렵고, 상대적으로 시퀀스가 길어진다.</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">어제 카페 갔었어 > 어, 제, 카, 페, 갔, 었, 어\n어제 카페 갔었는데요 > 어, 제, 카, 페, 갔, 었, 는, 데, 요</code></pre></div>\n<br>\n<ul>\n<li><strong>단어 단위 토큰화</strong>\n<ul>\n<li>단어 단위로 토큰화를 하는 것이다.</li>\n<li><code class=\"language-text\">장점</code> : 공백으로 쉽게 분리할 수 있다.</li>\n<li><code class=\"language-text\">단점</code> : 모든 단어들을 다 포함시키기에는 단어 사전의 크기가 상당히 크다. 이는 메모리 문제를 야기!</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">어제 카페 갔었어 > 어제, 카페, 갔었어\n어제 카페 갔었는데요 > 어제, 카페, 갔었는데요</code></pre></div>\n<br>\n<ul>\n<li><strong>서브워드 단위 토큰화</strong>\n<ul>\n<li>문자 단위와 단어 단위 토큰화의 중간에 있는 형태이다.</li>\n<li>“자주 등장한 단어는 그대로 두고, 자주 등장하지 않은 단어는 의미있는 서브 워드 토큰들로 분절한다” 라는 원칙에 기반을 둔 알고리즘</li>\n<li>단어 사전의 크기를 지나치게 늘리지 않으면서도 <code class=\"language-text\">UNK</code> 문제를 해결할 수 있다.</li>\n<li>희귀 단어, 신조어와 같은 문제를 완화시킬 수 있다.</li>\n</ul>\n</li>\n</ul>\n<h2>서브워드 기반 토크나이저</h2>\n<h3>BPE(Byte-Pair Encoding)</h3>\n<ul>\n<li>BPE(Byte pair encoding) 알고리즘은 1994년에 제안된 데이터 압축 알고리즘</li>\n<li>연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식을 수행</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">aaabdaaabac    # 가장 자주 등장하고 있는 바이트의 쌍(byte pair)은 'aa' , Z=aa\nZabdZabac      # Y=ab\nZYdZYac        # X=ZY\nXdXac          # 더 이상 병합할 바이트의 쌍이 없으므로 최종 결과로 하여 종료</code></pre></div>\n<br>\n<h3><a href=\"https://arxiv.org/abs/1508.07909\">자연어 처리에서의 BPE</a></h3>\n<ul>\n<li>\n<p>글자(charcter) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 Bottom up 방식의 접근을 사용</p>\n</li>\n<li>\n<p>BPE는 일반적으로 훈련 데이터를 단어 단위로 분절하는 Pre-tokenize 과정을 거쳐야한다.</p>\n</li>\n<li>\n<p>Pre-tokenize는 공백 단위나 규칙 기반으로 수행될 수 있다.</p>\n</li>\n<li>\n<p>Example)<br>\n<code class=\"language-text\">('hug', 10), ('pug', 5), ('pun', 12), ('bun', 4), ('hugs', 5)</code></p>\n<p>Pre-tokenize를 거쳐서 나온 단어들이라 하고 여기서 정수 값은 각 단어가 얼마나 등장했는지를 나타내는 값이다.<br>\n이때 기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’] 이다.<br>\n기본 사전을 기반으로 단어들을 쪼개면 다음과 같다.</p>\n<p><code class=\"language-text\">('h' 'u' 'g', 10), ('p' 'u' 'g', 5), ('p' 'u' 'n', 12), ('b' 'u' 'n', 4), ('h' 'u' 'g' 's', 5)</code><br>\n“hu”는 총 15번, “ug”는 총 20번이 나와 가장 많이 등장한 쌍은 “ug”가 되고 “u”와 “g”를 합친 “ug”를 사전에 새로 추가한다.<br>\n그럼 이때 기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’, ‘ug’] 이다.</p>\n<p><code class=\"language-text\">('h' 'ug', 10), ('p' 'ug', 5), ('p' 'u' 'n', 12), ('b' 'u' 'n', 4), ('h' 'ug' 's', 5)</code><br>\n또 가장 많이 나온 쌍은 16번 등장한 “un”이므로, “un”을 사전에 추가한다. 그 다음은 15번 등장한 “hug”이므로 “hug”도 사전에 추가한다.</p>\n<p><code class=\"language-text\">('hug', 10), ('p' 'ug', 5), ('p' 'un', 12), ('b' 'un', 4), ('hug' 's', 5)</code><br>\n기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’, ‘ug’, ‘un’, ‘hug’]가 됩니다.\n이렇게 처음에는 글자 단위였던 것이 의미있는 서브워드 토큰들로 분절할 수 있다.</p>\n</li>\n</ul>\n<br>\n<ul>\n<li><strong><a href=\"https://arxiv.org/pdf/1909.03341.pdf\">Byte-level BPE(BBPE)</a></strong></li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/54731898/133186594-e4f0a5d8-65a2-4ba5-b6a2-09be7bdc6757.png\" alt=\"image\"></p>\n<ul>\n<li>\n<p><a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">GPT-2 논문</a>에서 바이트를 사전의 기본 단위로 사용하는 트릭을 사용</p>\n</li>\n<li>\n<p>GPT-2 모델은 256개의 기본 바이트 토큰과 <code class=\"language-text\">&lt;end-of-text></code> 토큰 그리고 50,000 개의 서브 워드를 더해 총 50,257 개의 단어 집합(vocabulary)을 가진다.</p>\n</li>\n<li>\n<p>256 바이트셋으로 모든 텍스트를 표현할 수 있다.</p>\n</li>\n<li>\n<p><code class=\"language-text\">UNK</code> 문제없이 모든 텍스트를 분절할 수 있다.</p>\n</li>\n<li>\n<p>multilingual일 때, 언어들 사이에서 vocabulary 공유를 가장 많이 한다.</p>\n</li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/54731898/133136459-f1b4fdbf-d9d4-4976-842e-c00cbf657624.png\" alt=\"image\"></p>\n<br>\n<h3>WordPiece</h3>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1810.04805\">BERT</a>에서 활용된 서브 워드 토크나이저 알고리즘</li>\n<li>BPE와 마찬가지로 사전을 코퍼스 내 등장한 캐릭터들로 초기화 한 후, 사용자가 지정한 횟수 만큼 서브 워드를 병합하는 방식으로 훈련</li>\n<li>하지만 WordPiece는 BPE와 같이 가장 많이 등장한 쌍을 병합하는 것이 아니라, 병합되었을 때 코퍼스의 Likelihood를 가장 높이는 쌍을 병합하게 된다.</li>\n<li>즉, WordPiece에서는 코퍼스 내에서 “ug”가 등장할 확률을 “u”와 “g”가 각각 등장할 확률을 곱한 값으로 나눈 값이 다른 쌍보다 클 경우 해당 쌍을 병합하게 된다.</li>\n</ul>\n<p><img src=\"https://user-images.githubusercontent.com/54731898/133140262-f0afdedc-e54e-4564-a88d-134163c0a219.png\" alt=\"image\"></p>\n<p><code class=\"language-text\">또 이 학생의 집에서 병든 소를 도축했던 35살 남성도 탄저병에 걸린 것으로 확인됐습니다.</code></p>\n<blockquote>\n<p><code class=\"language-text\">['또', '이', '학생', '##의', '집', '##에', '##서', '병든', '소', '##를', '도축', '##했', '##던', '35', '##살', '남성', '##도', '탄', '##저', '##병', '##에', '걸린', '것', '##으로', '확인', '##됐', '##습', '##니다', '.']</code></p>\n</blockquote>\n<br>\n<h3>Unigram</h3>\n<ul>\n<li>서브 워드에서 시작해 점차 사전을 줄여나가는 top-down 방식으로 진행</li>\n<li>매 스텝마다 Unigram은 주어진 코퍼스와 현재 사전에 대한 Loss를 측정한다.</li>\n<li>또한 각각의 서브 워드에 대해 해당 서브 워드가 코퍼스에서 제거되었을 때, Loss가 얼마나 증가하는지를 측정하여 Loss를 가장 조금 증가시키는 p 개 토큰을 제거한다. (p는 보통 전체 사전 크기의 10-20% 값으로 설정)</li>\n<li>해당 과정을 사용자가 원하는 사전 크기를 지니게 될 때 까지 반복하게 되고, 기본 캐릭터들은 반드시 사전에서 제거되지 않고 유지되어야한다.</li>\n<li>매번 같은 토큰 리스트를 반환하는 BPE, WordPiece와 달리 Unigram은 다양한 토큰 리스트가 생길 수 있다.</li>\n</ul>\n<br>\n<h3>SentencePiece</h3>\n<ul>\n<li>\n<p>지금까지 살펴본 모든 방법들은 공백을 기준으로 단어를 분절할 수 없기 때문에 Pre-tokenize 과정을 필요로 했다.</p>\n</li>\n<li>\n<p>하지만 sentencepiece는 공백을 기준으로 단어를 분절할 수 있기 때문에 Pre-tokenize 과정이 필요없다.</p>\n</li>\n<li>\n<p>또한 디코딩 과정에서 모든 토큰들을 붙여준 후,  메타스페이스(”▁”)만 공백으로 바꿔주면 되기 때문에 원상복구가 가능하다는 특징이 있다.</p>\n</li>\n<li>\n<p>BPE 혹은 Unigram을 적용하여 사전을 구축하게 된다.</p>\n</li>\n</ul>\n<p><code class=\"language-text\">또 이 학생의 집에서 병든 소를 도축했던 35살 남성도 탄저병에 걸린 것으로 확인됐습니다.</code></p>\n<blockquote>\n<p><code class=\"language-text\">['▁또', '▁이', '▁학생', '의', '▁집에서', '▁병', '든', '▁소', '를', '▁', '도', '축', '했던', '▁35', '살', '▁남성', '도', '▁탄', '저', '병', '에', '▁걸린', '▁것으로', '▁확인', '됐', '습니다', '.']</code></p>\n</blockquote>\n<br>\n<h2>Reference</h2>\n<ul>\n<li><a href=\"https://wikidocs.net/22592\">https://wikidocs.net/22592</a></li>\n<li><a href=\"https://huggingface.co/transformers/master/tokenizer_summary.html\">https://huggingface.co/transformers/master/tokenizer_summary.html</a></li>\n<li><a href=\"https://karter.io/huggingface\">https://karter.io/huggingface</a></li>\n<li><a href=\"https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/\">https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/</a></li>\n<li><a href=\"https://arxiv.org/pdf/1508.07909.pdf\">https://arxiv.org/pdf/1508.07909.pdf</a></li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Tokenization"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"문장에서 의미있는 단위로 나누는 작업을 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"토큰화"}]},{"type":"text","value":"라고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"문자 단위 토큰화"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"문자 단위로 토큰화를 하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 15,000개를 넘기 어렵다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"장점"}]},{"type":"text","value":" : 모든 문자를 포함시켜서 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"UNK"}]},{"type":"text","value":"토큰이 잘 발생하지 않는다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"단점"}]},{"type":"text","value":" : 의미 있는 단위가 되기 어렵고, 상대적으로 시퀀스가 길어진다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"어제 카페 갔었어 > 어, 제, 카, 페, 갔, 었, 어\n어제 카페 갔었는데요 > 어, 제, 카, 페, 갔, 었, 는, 데, 요"}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"단어 단위 토큰화"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"단어 단위로 토큰화를 하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"장점"}]},{"type":"text","value":" : 공백으로 쉽게 분리할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"단점"}]},{"type":"text","value":" : 모든 단어들을 다 포함시키기에는 단어 사전의 크기가 상당히 크다. 이는 메모리 문제를 야기!"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"어제 카페 갔었어 > 어제, 카페, 갔었어\n어제 카페 갔었는데요 > 어제, 카페, 갔었는데요"}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"서브워드 단위 토큰화"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"문자 단위와 단어 단위 토큰화의 중간에 있는 형태이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"“자주 등장한 단어는 그대로 두고, 자주 등장하지 않은 단어는 의미있는 서브 워드 토큰들로 분절한다” 라는 원칙에 기반을 둔 알고리즘"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"단어 사전의 크기를 지나치게 늘리지 않으면서도 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"UNK"}]},{"type":"text","value":" 문제를 해결할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"희귀 단어, 신조어와 같은 문제를 완화시킬 수 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"서브워드 기반 토크나이저"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"BPE(Byte-Pair Encoding)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BPE(Byte pair encoding) 알고리즘은 1994년에 제안된 데이터 압축 알고리즘"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식을 수행"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"aaabdaaabac    # 가장 자주 등장하고 있는 바이트의 쌍(byte pair)은 'aa' , Z=aa\nZabdZabac      # Y=ab\nZYdZYac        # X=ZY\nXdXac          # 더 이상 병합할 바이트의 쌍이 없으므로 최종 결과로 하여 종료"}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1508.07909"},"children":[{"type":"text","value":"자연어 처리에서의 BPE"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"글자(charcter) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 Bottom up 방식의 접근을 사용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"BPE는 일반적으로 훈련 데이터를 단어 단위로 분절하는 Pre-tokenize 과정을 거쳐야한다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pre-tokenize는 공백 단위나 규칙 기반으로 수행될 수 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Example)"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"('hug', 10), ('pug', 5), ('pun', 12), ('bun', 4), ('hugs', 5)"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Pre-tokenize를 거쳐서 나온 단어들이라 하고 여기서 정수 값은 각 단어가 얼마나 등장했는지를 나타내는 값이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이때 기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’] 이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n기본 사전을 기반으로 단어들을 쪼개면 다음과 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"('h' 'u' 'g', 10), ('p' 'u' 'g', 5), ('p' 'u' 'n', 12), ('b' 'u' 'n', 4), ('h' 'u' 'g' 's', 5)"}]},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n“hu”는 총 15번, “ug”는 총 20번이 나와 가장 많이 등장한 쌍은 “ug”가 되고 “u”와 “g”를 합친 “ug”를 사전에 새로 추가한다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n그럼 이때 기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’, ‘ug’] 이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"('h' 'ug', 10), ('p' 'ug', 5), ('p' 'u' 'n', 12), ('b' 'u' 'n', 4), ('h' 'ug' 's', 5)"}]},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n또 가장 많이 나온 쌍은 16번 등장한 “un”이므로, “un”을 사전에 추가한다. 그 다음은 15번 등장한 “hug”이므로 “hug”도 사전에 추가한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"('hug', 10), ('p' 'ug', 5), ('p' 'un', 12), ('b' 'un', 4), ('hug' 's', 5)"}]},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’, ‘ug’, ‘un’, ‘hug’]가 됩니다.\n이렇게 처음에는 글자 단위였던 것이 의미있는 서브워드 토큰들로 분절할 수 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1909.03341.pdf"},"children":[{"type":"text","value":"Byte-level BPE(BBPE)"}]}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/54731898/133186594-e4f0a5d8-65a2-4ba5-b6a2-09be7bdc6757.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"},"children":[{"type":"text","value":"GPT-2 논문"}]},{"type":"text","value":"에서 바이트를 사전의 기본 단위로 사용하는 트릭을 사용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"GPT-2 모델은 256개의 기본 바이트 토큰과 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"<end-of-text>"}]},{"type":"text","value":" 토큰 그리고 50,000 개의 서브 워드를 더해 총 50,257 개의 단어 집합(vocabulary)을 가진다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"256 바이트셋으로 모든 텍스트를 표현할 수 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"UNK"}]},{"type":"text","value":" 문제없이 모든 텍스트를 분절할 수 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"multilingual일 때, 언어들 사이에서 vocabulary 공유를 가장 많이 한다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/54731898/133136459-f1b4fdbf-d9d4-4976-842e-c00cbf657624.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"WordPiece"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1810.04805"},"children":[{"type":"text","value":"BERT"}]},{"type":"text","value":"에서 활용된 서브 워드 토크나이저 알고리즘"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BPE와 마찬가지로 사전을 코퍼스 내 등장한 캐릭터들로 초기화 한 후, 사용자가 지정한 횟수 만큼 서브 워드를 병합하는 방식으로 훈련"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"하지만 WordPiece는 BPE와 같이 가장 많이 등장한 쌍을 병합하는 것이 아니라, 병합되었을 때 코퍼스의 Likelihood를 가장 높이는 쌍을 병합하게 된다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"즉, WordPiece에서는 코퍼스 내에서 “ug”가 등장할 확률을 “u”와 “g”가 각각 등장할 확률을 곱한 값으로 나눈 값이 다른 쌍보다 클 경우 해당 쌍을 병합하게 된다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/54731898/133140262-f0afdedc-e54e-4564-a88d-134163c0a219.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"또 이 학생의 집에서 병든 소를 도축했던 35살 남성도 탄저병에 걸린 것으로 확인됐습니다."}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"['또', '이', '학생', '##의', '집', '##에', '##서', '병든', '소', '##를', '도축', '##했', '##던', '35', '##살', '남성', '##도', '탄', '##저', '##병', '##에', '걸린', '것', '##으로', '확인', '##됐', '##습', '##니다', '.']"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Unigram"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"서브 워드에서 시작해 점차 사전을 줄여나가는 top-down 방식으로 진행"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"매 스텝마다 Unigram은 주어진 코퍼스와 현재 사전에 대한 Loss를 측정한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"또한 각각의 서브 워드에 대해 해당 서브 워드가 코퍼스에서 제거되었을 때, Loss가 얼마나 증가하는지를 측정하여 Loss를 가장 조금 증가시키는 p 개 토큰을 제거한다. (p는 보통 전체 사전 크기의 10-20% 값으로 설정)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"해당 과정을 사용자가 원하는 사전 크기를 지니게 될 때 까지 반복하게 되고, 기본 캐릭터들은 반드시 사전에서 제거되지 않고 유지되어야한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"매번 같은 토큰 리스트를 반환하는 BPE, WordPiece와 달리 Unigram은 다양한 토큰 리스트가 생길 수 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"SentencePiece"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"지금까지 살펴본 모든 방법들은 공백을 기준으로 단어를 분절할 수 없기 때문에 Pre-tokenize 과정을 필요로 했다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만 sentencepiece는 공백을 기준으로 단어를 분절할 수 있기 때문에 Pre-tokenize 과정이 필요없다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한 디코딩 과정에서 모든 토큰들을 붙여준 후,  메타스페이스(”▁”)만 공백으로 바꿔주면 되기 때문에 원상복구가 가능하다는 특징이 있다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"BPE 혹은 Unigram을 적용하여 사전을 구축하게 된다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"또 이 학생의 집에서 병든 소를 도축했던 35살 남성도 탄저병에 걸린 것으로 확인됐습니다."}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"blockquote","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"['▁또', '▁이', '▁학생', '의', '▁집에서', '▁병', '든', '▁소', '를', '▁', '도', '축', '했던', '▁35', '살', '▁남성', '도', '▁탄', '저', '병', '에', '▁걸린', '▁것으로', '▁확인', '됐', '습니다', '.']"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Reference"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://wikidocs.net/22592"},"children":[{"type":"text","value":"https://wikidocs.net/22592"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://huggingface.co/transformers/master/tokenizer_summary.html"},"children":[{"type":"text","value":"https://huggingface.co/transformers/master/tokenizer_summary.html"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://karter.io/huggingface"},"children":[{"type":"text","value":"https://karter.io/huggingface"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/"},"children":[{"type":"text","value":"https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1508.07909.pdf"},"children":[{"type":"text","value":"https://arxiv.org/pdf/1508.07909.pdf"}]}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"Tokenization 문장에서 의미있는 단위로 나누는 작업을 라고 한다. 문자 단위 토큰화 문자 단위로 토큰화를 하는 것이다. 한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 1…","fields":{"readingTime":{"text":"10 min read"}},"frontmatter":{"title":"Tokenizer","userDate":"13 September 2021","date":"2021-09-13T23:46:37.121Z","tags":["nlp"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#c8b8b8","images":{"fallback":{"src":"/static/8e92507f0141c508e4d5a30d3ca6fe53/4d1d2/writing.jpg","srcSet":"/static/8e92507f0141c508e4d5a30d3ca6fe53/6cce3/writing.jpg 750w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/fb319/writing.jpg 1080w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/6a5de/writing.jpg 1366w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/4d1d2/writing.jpg 1400w","sizes":"100vw"},"sources":[{"srcSet":"/static/8e92507f0141c508e4d5a30d3ca6fe53/d2a19/writing.webp 750w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/72f43/writing.webp 1080w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/8992a/writing.webp 1366w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/d652b/writing.webp 1400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6678571428571428}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":10,"edges":[{"node":{"id":"f8857a8d-9121-5e23-91bc-3fbe4f090418","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"3b2d6ea5-69c9-5459-815d-b59e3dbb34f8","excerpt":"Tokenization 문장에서 의미있는 단위로 나누는 작업을 라고 한다. 문자 단위 토큰화 문자 단위로 토큰화를 하는 것이다. 한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 1…","frontmatter":{"title":"Tokenizer","date":"2021-09-13T23:46:37.121Z"},"fields":{"readingTime":{"text":"10 min read"},"slug":"/tokenizer/"}}},{"node":{"id":"476f2558-a27e-5d36-bc5e-d8f8a5ca05e0","excerpt":"정규 표현식 정규표현식(regular expression)은 일종의 문자를 표현하는 공식으로, 특정 규칙이 있는 문자열 집합을 추출할 때 자주 사용되는 기법입니다. 주로 Prograaming Language나 Text Editor…","frontmatter":{"title":"정규표현식 (regex)","date":"2021-09-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"30 min read"},"slug":"/regex/"}}},{"node":{"id":"eaa0dc8a-25ba-5f4e-9a44-0022cc38776d","excerpt":"최근 NLP 토크나이저를 만드는데 가장 많이 사용되는  라이브러와 실제 사용이 가장 많이 되는  라이브러리로의 변환에 대한 코드를 담고 있습니다. 해당 내용은  버젼에서 수행되었습니다. Train 아래 코드는 wordpiece, char-bpe…","frontmatter":{"title":"Hugging Face Tokenizers","date":"2021-08-11T15:11:55.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/tokenizers/"}}},{"node":{"id":"261863c4-58d3-5883-b410-c0a7cc3d8319","excerpt":"GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…","frontmatter":{"title":"P-Tuning Paper Review","date":"2021-05-13T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/p_tuning/"}}}]}},"pageContext":{"slug":"/tokenizer/","prev":{"excerpt":"정규 표현식 정규표현식(regular expression)은 일종의 문자를 표현하는 공식으로, 특정 규칙이 있는 문자열 집합을 추출할 때 자주 사용되는 기법입니다. 주로 Prograaming Language나 Text Editor…","frontmatter":{"title":"정규표현식 (regex)","tags":["nlp"],"date":"2021-09-08T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABpElEQVQoz32RW3PaMBCF+f9/p31pZzp9apNALjBxGy4OYDBIGGNbMZeAJX9ZiyaZdDrdmTPSrnaPpHNatXM4pbFLhV0safK6rmWtcQK/fwV8hNSctdgsxyVr3NMTLZrY7+XUycbx3zgeYDSEUFBk7/XS4PKcWng84fgxodOZ8tBXDAeKyTihfTXm86ee5JpwtBIeTfd6TO/bHcH3LtcXI25vIzrtCZcX4Rt3q5YnP+8O6DAmm6/Ybgy7oiRRBWF/QTbTmFWG0SlJnJKXR9LigNIGvcxIBOt16UVoJGi57RbbbsPVJfXPH9RBAJsU7OldgpX+I4kop5Xkgngu386lr/ItDRkNoTelLKnEmAZ2s8GdTtij4PBMVRiO1zecJlMqvcI24suM3e29Mc46b94rqdewflOg/ssFydfJ2YjZ9Lyq5bleVTi5zK1T7DzGZdmZ0D9Vwpg982lCNFIyUxBFKb97EQtliOOcKNQo0TXN9+goYZtvccMh1WCA7fdxxnwk1NIcBLHHQNzudWd8/fKL4H7ucXczoX8/43GkGT4sSBPzzz+9ADf8rbHygOR3AAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/ebc4f36cae54a4a8d9eea6079daca5bc/acc2d/regex.png","srcSet":"/static/ebc4f36cae54a4a8d9eea6079daca5bc/acc2d/regex.png 699w","sizes":"100vw"},"sources":[{"srcSet":"/static/ebc4f36cae54a4a8d9eea6079daca5bc/8a3ab/regex.webp 699w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.45064377682403434}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"30 min read"},"layout":"","slug":"/regex/"}},"next":{"excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","tags":["speech","nlp","paper"],"date":"2021-09-19T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABDUlEQVQoz5VSQU7DMBDMf3kD7+iRH/AEXoA4gDgUEcVKqiCahiYtdZx414NbN4lFHRosrWytZ8czu47gLWPMsA/nc1zkjUFoRb8TPtAYhumkjQMM6z77J3HkVXsghlQtdNuA1m9QqxdQU9mcBjMHHw8q7AFsN6XJ2aXuFMwEIkLxVaHc1qAz8aTCk41eabOHqkuwOqCsdvjcbKGJodkpP8aRfB6h7Rnd30Le3YAfFlCtJWi1fcRazl6hPpbWgrpu2TXaNb3dJCiXj+Dv9Yjs9qjfn7ATz7ANHd1MDsXdIzT1kJKr38YvLIoCcRxDSjkAtdZIhECaZcOkZ1geCYVILgjTNEWe5/MIp6z81/IPMathqzY7Z+QAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/afa5c/gslm.png","srcSet":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/0dee1/gslm.png 750w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/8beaa/gslm.png 1080w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/d079a/gslm.png 1366w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/afa5c/gslm.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/a66aa/gslm.webp 750w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/65dd5/gslm.webp 1080w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/4fad6/gslm.webp 1366w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/c512e/gslm.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5625}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/Textledd NLP: Generating expressive speech from raw audio/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}