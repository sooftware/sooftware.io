{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition</h1>\n<p>Yu Zhang et al., 2020<br>\nGoogle Research, Brain Team</p>\n<hr>\n<h2>Reference</h2>\n<ul>\n<li><a href=\"https://github.com/kakaobrain/nlp-paper-reading/blob/master/notes/wav2vec%202.0.md\">Wav2vec 2.0 Summary</a></li>\n<li><a href=\"https://github.com/speech-paper-reading/speech-paper-reading/blob/main/notes/conformer.md\">Conformer Summary</a></li>\n</ul>\n<hr>\n<h2>Summary</h2>\n<ul>\n<li>현재 Papers with Code 기준 ASR 부문 State-Of-The-Art</li>\n<li>Conformer + Wav2vec 2.0 + Noisy Student Training</li>\n<li>1.3%/2.6%/1.4%/2.6% on the dev/dev-other/test/test-other sets</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/111322357-2f3dac80-86ac-11eb-8a05-24be848077de.png\" height=\"300\">\n<hr>\n<h3>Wav2vec 2.0 Pre-training</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/92450554-8a22b280-f1f6-11ea-8f66-0616b29d8c94.png\">\n<ul>\n<li>53,000이라는 대량의 Unlabeled speech data로 학습</li>\n<li>Pre-training 과정\n<ul>\n<li>Waveform에서 CNN을 이용해서 피쳐를 뽑음</li>\n<li>이를 Vector Quantization을 통해 one-hot-vector로 만들고 Embedding matrix를 내적하여 token화 함</li>\n<li>일정 비율로 Masking하고 다음 Token이 뭔지 알아맞추게하는 Masked Language Modeling (MLM) 학습 방식 적용</li>\n</ul>\n</li>\n<li>Vector Quantization</li>\n</ul>\n<img src=\"https://camo.githubusercontent.com/4e4253817961b5bead8072739c39bd3f3daaced98e8735018c50e8a55d78fb9c/68747470733a2f2f692e696d6775722e636f6d2f7931355175355a2e706e67\" height=\"300\">\n  - Z를 선형변환하여 logit을 만듦\n  - 여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦\n  - 이후 Embedding matrix를 내적해 Z^를 만듦\n<h3>Conformer</h3>\n<ul>\n<li>Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는 부족하다는 단점이 있음</li>\n<li>반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는 적당한 dilation과 깊은 구조를 가져야 함</li>\n<li>이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한 transformer + CNN 결합구조인 Conformer 구조 제안</li>\n</ul>\n<h2>Method</h2>\n<p>본 논문에서 실험한 모델 구조 및 트레이닝 방법</p>\n<h3>Model Architecture</h3>\n<p>Conformer Encoder + LSTM decoder로 이루어진 Transducer 구조</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/111322627-77f56580-86ac-11eb-957c-d51db823e4e4.png\" height=\"600\">\n<ul>\n<li>기존 Conformer 구조에서 Wav2vec 2.0 Pre-training을 도입하기 위해 Masking하는 과정과 Linear Layer (Quantization 대체) 추가</li>\n<li>사이즈별로 L, XL, XXL로 구분</li>\n<li>XXL+는 Conformer XXL에 Conformer block을 stack (XXL보다 50M 파라미터 추가)</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/111324910-8ba1cb80-86ae-11eb-997f-12634e7b6164.png\">\n<h3>Wav2vec 2.0 Pre-training</h3>\n<ul>\n<li>60k Libri-Light 데이터셋 사용</li>\n<li>기존 논문과 달리, 인풋으로 log-mel spectrogram 사용</li>\n<li>Masking 된 인풋과 예측한 context vector 간의 contrastive loss로 학습</li>\n</ul>\n<h3>Noisy Student Training with SpecAugment</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/111328868-fa345880-86b1-11eb-925c-a76cdbfd7c8c.png\" height=\"200\">\n<h2>Experiiments</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/111329076-2e0f7e00-86b2-11eb-8c87-17d2eca8948b.png\" height=\"400\">\n<ul>\n<li>결과적으로 Pre-training + NST가 좋은 성적을 냄</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Yu Zhang et al., 2020"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nGoogle Research, Brain Team"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Reference"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/kakaobrain/nlp-paper-reading/blob/master/notes/wav2vec%202.0.md"},"children":[{"type":"text","value":"Wav2vec 2.0 Summary"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/speech-paper-reading/speech-paper-reading/blob/main/notes/conformer.md"},"children":[{"type":"text","value":"Conformer Summary"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Summary"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"현재 Papers with Code 기준 ASR 부문 State-Of-The-Art"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Conformer + Wav2vec 2.0 + Noisy Student Training"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"1.3%/2.6%/1.4%/2.6% on the dev/dev-other/test/test-other sets"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111322357-2f3dac80-86ac-11eb-8a05-24be848077de.png","height":300},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Wav2vec 2.0 Pre-training"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/92450554-8a22b280-f1f6-11ea-8f66-0616b29d8c94.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"53,000이라는 대량의 Unlabeled speech data로 학습"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pre-training 과정\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Waveform에서 CNN을 이용해서 피쳐를 뽑음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이를 Vector Quantization을 통해 one-hot-vector로 만들고 Embedding matrix를 내적하여 token화 함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"일정 비율로 Masking하고 다음 Token이 뭔지 알아맞추게하는 Masked Language Modeling (MLM) 학습 방식 적용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Vector Quantization"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://camo.githubusercontent.com/4e4253817961b5bead8072739c39bd3f3daaced98e8735018c50e8a55d78fb9c/68747470733a2f2f692e696d6775722e636f6d2f7931355175355a2e706e67","height":300},"children":[]},{"type":"text","value":"\n  - Z를 선형변환하여 logit을 만듦\n  - 여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦\n  - 이후 Embedding matrix를 내적해 Z^를 만듦\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Conformer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는 부족하다는 단점이 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는 적당한 dilation과 깊은 구조를 가져야 함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한 transformer + CNN 결합구조인 Conformer 구조 제안"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Method"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 실험한 모델 구조 및 트레이닝 방법"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Model Architecture"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Conformer Encoder + LSTM decoder로 이루어진 Transducer 구조"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111322627-77f56580-86ac-11eb-957c-d51db823e4e4.png","height":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존 Conformer 구조에서 Wav2vec 2.0 Pre-training을 도입하기 위해 Masking하는 과정과 Linear Layer (Quantization 대체) 추가"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"사이즈별로 L, XL, XXL로 구분"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"XXL+는 Conformer XXL에 Conformer block을 stack (XXL보다 50M 파라미터 추가)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111324910-8ba1cb80-86ae-11eb-997f-12634e7b6164.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Wav2vec 2.0 Pre-training"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"60k Libri-Light 데이터셋 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존 논문과 달리, 인풋으로 log-mel spectrogram 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Masking 된 인풋과 예측한 context vector 간의 contrastive loss로 학습"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Noisy Student Training with SpecAugment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111328868-fa345880-86b1-11eb-925c-a76cdbfd7c8c.png","height":200},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiiments"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111329076-2e0f7e00-86b2-11eb-8c87-17d2eca8948b.png","height":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결과적으로 Pre-training + NST가 좋은 성적을 냄"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","fields":{"readingTime":{"text":"3 min read"}},"frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","userDate":"17 March 2021","date":"2021-03-17T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/7ca66667228d877c1b0d96e85b974435/ee3dd/pushing.png","srcSet":"/static/7ca66667228d877c1b0d96e85b974435/6a16f/pushing.png 750w,\n/static/7ca66667228d877c1b0d96e85b974435/ee3dd/pushing.png 928w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ca66667228d877c1b0d96e85b974435/e0c95/pushing.webp 750w,\n/static/7ca66667228d877c1b0d96e85b974435/86029/pushing.webp 928w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6142241379310345}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":20,"edges":[{"node":{"id":"f8857a8d-9121-5e23-91bc-3fbe4f090418","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"93dec710-458c-531f-acbc-28d14f762768","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"127c2cf6-c23b-5410-a480-70de1d98ce87","excerpt":"PORORO Text-To-Speech (TTS) 얼마전에 저희 팀에서 공개한 PORORO: Platform Of neuRal mOdels for natuRal language prOcessing 라이브러리에 제가 공들여만든 TTS…","frontmatter":{"title":"PORORO Text-To-Speech (TTS)","date":"2021-02-16T10:00:00.000Z"},"fields":{"readingTime":{"text":"1 min read"},"slug":"/pororo-tts/"}}},{"node":{"id":"a4f99081-c696-5b06-85b1-c2268aed1215","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"e1128eb2-c5b3-55e1-83ef-3b6478bb7d7b","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}}]}},"pageContext":{"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/","prev":{"excerpt":"카카오브레인 퇴사, 그리고 창업 (feat…","frontmatter":{"title":"카카오브레인 퇴사, 그리고 창업 (feat. 졸업)","tags":["about"],"date":"2021-02-27T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAADTUlEQVQozxWT60+bBRTG349+948wMZiZ6D7M4NyQYmTyQaPJgppoJotZolOWbZLMGW67AB0bbNO5rAwXGCA66CilWC5toaV32rf08pZLO9pulNJCSylgx8/XDyc5eU7ynN9zkiOEbivor1NQ8k4VnbWVrA6cIz3WQdJyF/GmAv+t9/HJJSrfw133KY5LNQRaj+Hu/pb1ISWBG+XYrryNQ9ZcbUcRrNePI3WWcr+2Au29ZnZtOjzzEmuZBM+eXMCirERV+y7irUrM56vRnfmSpc6PmTFa2ZgYwTrah1t1Gl+HgljXNwjO1rfY7H+F1GAJG93Ncv+AhXiSZG6XhGRj8GE7oTk1e6sGdnzD5O09ZLy9rHgt+Jx+Bp1FzMZ5Ck/bifX+gpBSvwnjAi9HBHJjFeQTXlKxEOsRkWJShIwftsMUYyIHuRhZq5li1AmbIttxL1IoiOhexG5wMNd+AsHypJ6i6TBoBPYmX2cvNsVuXM/LNQfe2WHifiNseYnNdJGZd0FaIjg3wopnQjYNsh+ZZvrRT8x0ncV19bAcWanAc7eCVN+r5MNqDjb8xJemyK7acUz9hX5IhUHzB6viGAnfFOllM/GAkcyyRaaX2Jb+YfxKGa2Nn6Br+QBhSfkZvstluH89STEbkbcGcFs0LHomyUasBDzTSHYtHtMQTwd+Z2neADkJNnyyYZCUq4/+c4cYbi7DqjyOMHrma4wXvye74qCQ3OLfZJhc2ERGpsnFndzXG1GbZ0mGZ5HcM7BwkXu6JtIvwrD/HEmnwtT0ObaOL9DUy4S22mMsD/cSzR3wTI63F1tgNuKkzzMFUQtR0cCPPVoMlv9vFqZga2L88VW21pfZzO8Q9YyzMdxI4w+nKDlSiTBT/waRydtk92ExHmU7pOe1mIsLATmadRDsf5JK+CisLVB47iGXCIFvgh3T3wSWYqwmI6TmVFhaTqC5dATB9Vs1ZimIOziHPRDE9biV83cacGkaSWgaEG9cJm15wJY4QDYwwk7UwLZfBxMPSUYlXuRlEO0dnNdLWZA/Sgh0lDPQo6Rbb8D36CzampN4f/4Irzy0t5Uxerqa6boqHMqj2NoUslaO/loVp2q+Q932FcGhBhw3P8TRUopLWcZ/bPbwCx8SJ6MAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/326851fe970bf69bd6fc9b3f450fe424/776e5/new_start.png","srcSet":"/static/326851fe970bf69bd6fc9b3f450fe424/7f1c2/new_start.png 750w,\n/static/326851fe970bf69bd6fc9b3f450fe424/776e5/new_start.png 960w","sizes":"100vw"},"sources":[{"srcSet":"/static/326851fe970bf69bd6fc9b3f450fe424/754fd/new_start.webp 750w,\n/static/326851fe970bf69bd6fc9b3f450fe424/b8767/new_start.webp 960w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5916666666666667}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/new_start/"}},"next":{"excerpt":"GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…","frontmatter":{"title":"P-Tuning Paper Review","tags":["nlp","paper"],"date":"2021-05-13T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABE0lEQVQY002Q6W6DMBCEef9nqvqzl9SKpIfU5gLSEHDAxTFgwpGv60SVutJIs+PZ1Y6DcZyYpgnnOmxd0/cnxv/9MFA3DdZUVNs5rkwYZMbrTRFhkhBXG4625nyeCFzbUOmSfL9jn6ak2UGWtRR5zkFwkqW+vNm1NbU1ZPuU5TrGWss4nCiUItlsGMeRwKiE7PMB9XXPbvXKzUtOtPygCB8p3t7oswx7tJhKo+OZ+O5IFnNuZwXR4h29eULNnlFhKAsHgs45JiFtU5PEMZsoRmvN0Pf4t0bgq/d9d/XqsmS5WvO93WLMj2gjjSTpuo7An+nNpZhSHzndCdcXrW1bBvmra+SzaMOl91AqFyj+5hvxev4LH4d753+QROEAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/5251bff33539be461c690325c488ea29/c0e65/p_tuning.png","srcSet":"/static/5251bff33539be461c690325c488ea29/f0b2b/p_tuning.png 750w,\n/static/5251bff33539be461c690325c488ea29/3831f/p_tuning.png 1080w,\n/static/5251bff33539be461c690325c488ea29/ec348/p_tuning.png 1366w,\n/static/5251bff33539be461c690325c488ea29/c0e65/p_tuning.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/5251bff33539be461c690325c488ea29/a4cf2/p_tuning.webp 750w,\n/static/5251bff33539be461c690325c488ea29/4e6df/p_tuning.webp 1080w,\n/static/5251bff33539be461c690325c488ea29/04ab9/p_tuning.webp 1366w,\n/static/5251bff33539be461c690325c488ea29/19b4f/p_tuning.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.23489583333333336}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/p_tuning/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}