{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition</h1>\n<p>Yu Zhang et al., 2020<br>\nGoogle Research, Brain Team</p>\n<hr>\n<h2>Reference</h2>\n<ul>\n<li><a href=\"https://github.com/kakaobrain/nlp-paper-reading/blob/master/notes/wav2vec%202.0.md\">Wav2vec 2.0 Summary</a></li>\n<li><a href=\"https://github.com/speech-paper-reading/speech-paper-reading/blob/main/notes/conformer.md\">Conformer Summary</a></li>\n</ul>\n<hr>\n<h2>Summary</h2>\n<ul>\n<li>현재 Papers with Code 기준 ASR 부문 State-Of-The-Art</li>\n<li>Conformer + Wav2vec 2.0 + Noisy Student Training</li>\n<li>1.3%/2.6%/1.4%/2.6% on the dev/dev-other/test/test-other sets</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/111322357-2f3dac80-86ac-11eb-8a05-24be848077de.png\" height=\"300\">\n<hr>\n<h3>Wav2vec 2.0 Pre-training</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/92450554-8a22b280-f1f6-11ea-8f66-0616b29d8c94.png\">\n<ul>\n<li>53,000이라는 대량의 Unlabeled speech data로 학습</li>\n<li>Pre-training 과정\n<ul>\n<li>Waveform에서 CNN을 이용해서 피쳐를 뽑음</li>\n<li>이를 Vector Quantization을 통해 one-hot-vector로 만들고 Embedding matrix를 내적하여 token화 함</li>\n<li>일정 비율로 Masking하고 다음 Token이 뭔지 알아맞추게하는 Masked Language Modeling (MLM) 학습 방식 적용</li>\n</ul>\n</li>\n<li>Vector Quantization</li>\n</ul>\n<img src=\"https://camo.githubusercontent.com/4e4253817961b5bead8072739c39bd3f3daaced98e8735018c50e8a55d78fb9c/68747470733a2f2f692e696d6775722e636f6d2f7931355175355a2e706e67\" height=\"300\">\n  - Z를 선형변환하여 logit을 만듦\n  - 여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦\n  - 이후 Embedding matrix를 내적해 Z^를 만듦\n<h3>Conformer</h3>\n<ul>\n<li>Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는 부족하다는 단점이 있음</li>\n<li>반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는 적당한 dilation과 깊은 구조를 가져야 함</li>\n<li>이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한 transformer + CNN 결합구조인 Conformer 구조 제안</li>\n</ul>\n<h2>Method</h2>\n<p>본 논문에서 실험한 모델 구조 및 트레이닝 방법</p>\n<h3>Model Architecture</h3>\n<p>Conformer Encoder + LSTM decoder로 이루어진 Transducer 구조</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/111322627-77f56580-86ac-11eb-957c-d51db823e4e4.png\" height=\"600\">\n<ul>\n<li>기존 Conformer 구조에서 Wav2vec 2.0 Pre-training을 도입하기 위해 Masking하는 과정과 Linear Layer (Quantization 대체) 추가</li>\n<li>사이즈별로 L, XL, XXL로 구분</li>\n<li>XXL+는 Conformer XXL에 Conformer block을 stack (XXL보다 50M 파라미터 추가)</li>\n</ul>\n<img src=\"https://user-images.githubusercontent.com/42150335/111324910-8ba1cb80-86ae-11eb-997f-12634e7b6164.png\">\n<h3>Wav2vec 2.0 Pre-training</h3>\n<ul>\n<li>60k Libri-Light 데이터셋 사용</li>\n<li>기존 논문과 달리, 인풋으로 log-mel spectrogram 사용</li>\n<li>Masking 된 인풋과 예측한 context vector 간의 contrastive loss로 학습</li>\n</ul>\n<h3>Noisy Student Training with SpecAugment</h3>\n<img src=\"https://user-images.githubusercontent.com/42150335/111328868-fa345880-86b1-11eb-925c-a76cdbfd7c8c.png\" height=\"200\">\n<h2>Experiiments</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/111329076-2e0f7e00-86b2-11eb-8c87-17d2eca8948b.png\" height=\"400\">\n<ul>\n<li>결과적으로 Pre-training + NST가 좋은 성적을 냄</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Yu Zhang et al., 2020"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nGoogle Research, Brain Team"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Reference"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/kakaobrain/nlp-paper-reading/blob/master/notes/wav2vec%202.0.md"},"children":[{"type":"text","value":"Wav2vec 2.0 Summary"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/speech-paper-reading/speech-paper-reading/blob/main/notes/conformer.md"},"children":[{"type":"text","value":"Conformer Summary"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Summary"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"현재 Papers with Code 기준 ASR 부문 State-Of-The-Art"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Conformer + Wav2vec 2.0 + Noisy Student Training"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"1.3%/2.6%/1.4%/2.6% on the dev/dev-other/test/test-other sets"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111322357-2f3dac80-86ac-11eb-8a05-24be848077de.png","height":300},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"hr","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Wav2vec 2.0 Pre-training"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/92450554-8a22b280-f1f6-11ea-8f66-0616b29d8c94.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"53,000이라는 대량의 Unlabeled speech data로 학습"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pre-training 과정\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Waveform에서 CNN을 이용해서 피쳐를 뽑음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이를 Vector Quantization을 통해 one-hot-vector로 만들고 Embedding matrix를 내적하여 token화 함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"일정 비율로 Masking하고 다음 Token이 뭔지 알아맞추게하는 Masked Language Modeling (MLM) 학습 방식 적용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Vector Quantization"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://camo.githubusercontent.com/4e4253817961b5bead8072739c39bd3f3daaced98e8735018c50e8a55d78fb9c/68747470733a2f2f692e696d6775722e636f6d2f7931355175355a2e706e67","height":300},"children":[]},{"type":"text","value":"\n  - Z를 선형변환하여 logit을 만듦\n  - 여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦\n  - 이후 Embedding matrix를 내적해 Z^를 만듦\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Conformer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는 부족하다는 단점이 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는 적당한 dilation과 깊은 구조를 가져야 함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한 transformer + CNN 결합구조인 Conformer 구조 제안"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Method"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 실험한 모델 구조 및 트레이닝 방법"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Model Architecture"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Conformer Encoder + LSTM decoder로 이루어진 Transducer 구조"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111322627-77f56580-86ac-11eb-957c-d51db823e4e4.png","height":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존 Conformer 구조에서 Wav2vec 2.0 Pre-training을 도입하기 위해 Masking하는 과정과 Linear Layer (Quantization 대체) 추가"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"사이즈별로 L, XL, XXL로 구분"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"XXL+는 Conformer XXL에 Conformer block을 stack (XXL보다 50M 파라미터 추가)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111324910-8ba1cb80-86ae-11eb-997f-12634e7b6164.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Wav2vec 2.0 Pre-training"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"60k Libri-Light 데이터셋 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존 논문과 달리, 인풋으로 log-mel spectrogram 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Masking 된 인풋과 예측한 context vector 간의 contrastive loss로 학습"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Noisy Student Training with SpecAugment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111328868-fa345880-86b1-11eb-925c-a76cdbfd7c8c.png","height":200},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiiments"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/111329076-2e0f7e00-86b2-11eb-8c87-17d2eca8948b.png","height":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결과적으로 Pre-training + NST가 좋은 성적을 냄"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","fields":{"readingTime":{"text":"3 min read"}},"frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","userDate":"17 March 2021","date":"2021-03-17T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/7ca66667228d877c1b0d96e85b974435/ee3dd/pushing.png","srcSet":"/static/7ca66667228d877c1b0d96e85b974435/6a16f/pushing.png 750w,\n/static/7ca66667228d877c1b0d96e85b974435/ee3dd/pushing.png 928w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ca66667228d877c1b0d96e85b974435/e0c95/pushing.webp 750w,\n/static/7ca66667228d877c1b0d96e85b974435/86029/pushing.webp 928w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6142241379310345}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"19ded62e-3e91-5733-9329-a1c7bdcf859b","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"35eadfc7-646b-5194-a711-ce20e840ba58","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"fd3185b8-63e4-5e3d-aeb3-e67ed1343af9","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"aad087b1-4b0f-5956-ab2f-d7ab33fdb8c4","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/","prev":{"excerpt":"Longformer: The Long-Document Transformer Paper Code Iz Beltagy et al. Introduction 트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음 이유는 시퀀스 길이에 O(n^…","frontmatter":{"title":"Longformer Paper Review","tags":["nlp","paper"],"date":"2021-02-06T23:46:37.121Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC/0lEQVQ4y32UWVPTYBSG+ade+BPUkWEc9crxwgVR8QId6UBLKVuhNG1hWNo0gUK3JF0paZqmDd0tqHhTeD3fhygKmJkz30ky88xzlmTo/Pwc7BoMBjjp9dFtttBtXUSb8kajgWazeRFXcwr2rtvt4pLBzqGzszN+k8llcef5fYx5PRj1OPFy3gmHdxHBYAi+QADLq374KWenTwhgNRDk55TTBduucwZjDQ1+ATVVw/DSBJ4pK3AXUvCaecyXVBTaTRx/+w6bTHp0HnV7aPWP0Tn5iu7Xb4jGYtB1/Q/w0jCtKnjr92IsEcI9/wfMaHuYSsr4nIwirpegGyb0Wg3FsoFShfKqBd2qYX1rC4ZhXDdkwJfCAgTrEK92fHgcdWI2l4CHbN2FJFJmGR3qsWEfwaLe1tsd2J0OwpIEo1y+bqipKt4LXsyW0vCocYzuCXi4/gkuMnVn49w0Xa3AtOrc1CSwedTARjhChjcCNWyKUaxVivgYj2CpqOGFtIzhjc+YzSbgVHbhyu4jQeVbBDNs+/9AlQzFnV3e+JV8GjPFJIeOxYN4FJnmlgs0pOnMHhTLRKvT42WHJfnmkhlwPRxGg6bIGu8rKPiUEDGfTfGePghOwJ2JYzq1g8mUhKSho2Y3sLa1/fdQrgIjvwwrdZuvx+qBCmcujuXDLF7H/DQo11+Dyh7VIcu7KN9muBER0eh94YYW9Yethi+XhkPdwZyWoEH58SA0AZe6h5nMPqa1GDzrQZhG5WbgNvWDLSwD1VptPkmr0YRQymAyLV+Y7q5ieHMSc2Q5SzHmX0T1VmBUQvv4BIfVKgcatTqZNjl0JZfi5bOejsYEjGw74M4nME6rZt7WQ25IQGZYJyDrJVuNGi0ygwf1HCYVGUsFFW/2AxgJT+GdsAiLvpxrQEVR4PULOCD9fcq1QhHJTAZKLg8ln0dSy/BnDmkT42IIjpiIpyEn7o4/Qc2s/gFe/nr6/T4ioghJlhGlz0n6J9gzmd6xfDsq8pCooq1oBKc/Tn//vn4CpaVjGnZwBOMAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/ef32af94bd92c05cbfebe0cb00c3966e/597e6/longformer.png","srcSet":"/static/ef32af94bd92c05cbfebe0cb00c3966e/597e6/longformer.png 512w","sizes":"100vw"},"sources":[{"srcSet":"/static/ef32af94bd92c05cbfebe0cb00c3966e/3d2a6/longformer.webp 512w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8046875000000001}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"post","slug":"/longformer/"}},"next":{"excerpt":"GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…","frontmatter":{"title":"P-Tuning Paper Review","tags":["nlp","paper"],"date":"2021-05-13T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABE0lEQVQY002Q6W6DMBCEef9nqvqzl9SKpIfU5gLSEHDAxTFgwpGv60SVutJIs+PZ1Y6DcZyYpgnnOmxd0/cnxv/9MFA3DdZUVNs5rkwYZMbrTRFhkhBXG4625nyeCFzbUOmSfL9jn6ak2UGWtRR5zkFwkqW+vNm1NbU1ZPuU5TrGWss4nCiUItlsGMeRwKiE7PMB9XXPbvXKzUtOtPygCB8p3t7oswx7tJhKo+OZ+O5IFnNuZwXR4h29eULNnlFhKAsHgs45JiFtU5PEMZsoRmvN0Pf4t0bgq/d9d/XqsmS5WvO93WLMj2gjjSTpuo7An+nNpZhSHzndCdcXrW1bBvmra+SzaMOl91AqFyj+5hvxev4LH4d753+QROEAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/5251bff33539be461c690325c488ea29/c0e65/p_tuning.png","srcSet":"/static/5251bff33539be461c690325c488ea29/f0b2b/p_tuning.png 750w,\n/static/5251bff33539be461c690325c488ea29/3831f/p_tuning.png 1080w,\n/static/5251bff33539be461c690325c488ea29/ec348/p_tuning.png 1366w,\n/static/5251bff33539be461c690325c488ea29/c0e65/p_tuning.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/5251bff33539be461c690325c488ea29/a4cf2/p_tuning.webp 750w,\n/static/5251bff33539be461c690325c488ea29/4e6df/p_tuning.webp 1080w,\n/static/5251bff33539be461c690325c488ea29/04ab9/p_tuning.webp 1366w,\n/static/5251bff33539be461c690325c488ea29/19b4f/p_tuning.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.23489583333333336}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/p_tuning/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}