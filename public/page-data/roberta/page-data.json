{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/roberta/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>RoBERTa</h1>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1907.11692\">paper</a> / <a href=\"https://github.com/pytorch/fairseq/tree/master/examples/roberta\">code</a></li>\n</ul>\n<h2>Abstract</h2>\n<ul>\n<li>BERT를 제대로 학습시키는 법을 제안</li>\n<li>BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음</li>\n<li>BERT를 더 좋은 성능을 내게 하기 위한 replication study.</li>\n</ul>\n<h2>Background (BERT)</h2>\n<img src=\"https://baekyeongmin.github.io/images/RoBERTa/bert.png\" width=\"700\">\n<ul>\n<li>학습 1단계) 많은 양의 unlabeled corpus를 이용한 pre-train</li>\n<li>학습 2단계) 특정 도메인의 태스크에 집중하여 학습하는 fine-tuning</li>\n<li>“Attention Is All You Need”에서 제안된 transformer의 encoder를 사용</li>\n</ul>\n<h2>Main Idea</h2>\n<h3>Dynamic Masking</h3>\n<ul>\n<li>기존의 BERT는 학습 전에 데이터에 무작위로 mask를 씌움.</li>\n<li>매 학습 단계에서 똑같은 mask를 보게 됨. (static masking)</li>\n<li>이를 같은 문장을 10번 복사한 뒤 서로 다른 마스크를 씌움으로써 해결하려고 했지만, 이는 크기가 큰 데이터에 대해서 비효율적임.</li>\n<li>RoBERTa는 매 에폭마다 mask를 새로 씌우는 dynamic masking을 사용</li>\n<li>결과: static masking보다 좋은 성능을 보여줌</li>\n</ul>\n<h3>Input Format / Next Sentence Prediction</h3>\n<ul>\n<li>기존 BERT에서는 Next Sentence Prediction(NSP)이라는 과정이 있었음\n<ul>\n<li>\n<ol>\n<li>두 개의 문장을 이어 붙인다</li>\n</ol>\n</li>\n<li>\n<ol start=\"2\">\n<li>두 문장이 문맥상으로 연결된 문장인지를 분류하는 binary classification을 수행</li>\n</ol>\n</li>\n</ul>\n</li>\n<li>RoBERTa는 NSP에 의문을 제기하고, Masked Language Modeling (MLM)만으로 pre-training을 수행</li>\n<li>NSP를 없앰으로써 두 문장을 이어 붙인 형태의 인풋 형태를 사용할 필요가 없어졌고, RoBERTa는 최대 토큰 수를 넘어가지 않는 선에서 문장을 최대한 이어 붙여서 input을 만들 수 있었음</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[CLS]가나다라마바사아자카타파하[SEP]오늘 날씨가 좋은걸?[SEP]튜닙은 정말 대단한 회사야!.....[SEP]오늘은 빨리 퇴근하고 싶다.</code></pre></div>\n<ul>\n<li>BERT는 짧은 인풋들을 이어붙이는 경우도 있었지만, RoBERTa는 모든 인풋 토큰 수를 최대길이에 가깝게 사용할 수 있었음 (학습 효율면에서 좋음)</li>\n<li>결과: NSP를 없앤 BERT가 기존의 BERT보다 더 나은 성능을 보임.</li>\n</ul>\n<h3>Batch Size</h3>\n<ul>\n<li>(batch size X step 수)가 일정하게 유지되는 선에서 배치사이즈에 따른 성능 실험\n<ul>\n<li>batch size가 256에 step이 1M이라면 batch size가 2K에 step이 125K가 되도록. (둘의 곱이 같도록 유지)</li>\n</ul>\n</li>\n<li>배치가 클수록 성능이 좋아지는 경향을 보임.</li>\n</ul>\n<h3>Data</h3>\n<ul>\n<li>데이터가 많을수록 BERT의 성능이 좋아지는 경향을 이전 연구들에서 관찰됐었음.</li>\n<li>기존 BERT는 16GB로 학습했는데, RoBERT는 160GB의 데이터로 학습하였음.</li>\n<li>당연하게도 160GB로 학습한 RoBERTa가 더 좋은 성능을 기록함</li>\n<li>학습 시간을 길게 하면 할수록 더 좋은 성능을 보였다고함.</li>\n</ul>\n<h2>Conclusion</h2>\n<ul>\n<li>160GB의 데이터</li>\n<li>Dynamic Masking</li>\n<li>NSP 밴</li>\n<li>최대한 문장을 구겨넣은 인풋 ([SEP]으로 분리)</li>\n<li>큰 배치사이즈</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"RoBERTa"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1907.11692"},"children":[{"type":"text","value":"paper"}]},{"type":"text","value":" / "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/pytorch/fairseq/tree/master/examples/roberta"},"children":[{"type":"text","value":"code"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Abstract"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT를 제대로 학습시키는 법을 제안"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT를 더 좋은 성능을 내게 하기 위한 replication study."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Background (BERT)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://baekyeongmin.github.io/images/RoBERTa/bert.png","width":700},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"학습 1단계) 많은 양의 unlabeled corpus를 이용한 pre-train"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"학습 2단계) 특정 도메인의 태스크에 집중하여 학습하는 fine-tuning"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"“Attention Is All You Need”에서 제안된 transformer의 encoder를 사용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Main Idea"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Dynamic Masking"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존의 BERT는 학습 전에 데이터에 무작위로 mask를 씌움."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"매 학습 단계에서 똑같은 mask를 보게 됨. (static masking)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이를 같은 문장을 10번 복사한 뒤 서로 다른 마스크를 씌움으로써 해결하려고 했지만, 이는 크기가 큰 데이터에 대해서 비효율적임."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"RoBERTa는 매 에폭마다 mask를 새로 씌우는 dynamic masking을 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결과: static masking보다 좋은 성능을 보여줌"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Input Format / Next Sentence Prediction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존 BERT에서는 Next Sentence Prediction(NSP)이라는 과정이 있었음\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"두 개의 문장을 이어 붙인다"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"ol","properties":{"start":2},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"두 문장이 문맥상으로 연결된 문장인지를 분류하는 binary classification을 수행"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"RoBERTa는 NSP에 의문을 제기하고, Masked Language Modeling (MLM)만으로 pre-training을 수행"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"NSP를 없앰으로써 두 문장을 이어 붙인 형태의 인풋 형태를 사용할 필요가 없어졌고, RoBERTa는 최대 토큰 수를 넘어가지 않는 선에서 문장을 최대한 이어 붙여서 input을 만들 수 있었음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"div","properties":{"className":["gatsby-highlight"],"dataLanguage":"text"},"children":[{"type":"element","tagName":"pre","properties":{"className":["language-text"]},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"[CLS]가나다라마바사아자카타파하[SEP]오늘 날씨가 좋은걸?[SEP]튜닙은 정말 대단한 회사야!.....[SEP]오늘은 빨리 퇴근하고 싶다."}]}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT는 짧은 인풋들을 이어붙이는 경우도 있었지만, RoBERTa는 모든 인풋 토큰 수를 최대길이에 가깝게 사용할 수 있었음 (학습 효율면에서 좋음)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"결과: NSP를 없앤 BERT가 기존의 BERT보다 더 나은 성능을 보임."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Batch Size"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"(batch size X step 수)가 일정하게 유지되는 선에서 배치사이즈에 따른 성능 실험\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"batch size가 256에 step이 1M이라면 batch size가 2K에 step이 125K가 되도록. (둘의 곱이 같도록 유지)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"배치가 클수록 성능이 좋아지는 경향을 보임."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Data"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"데이터가 많을수록 BERT의 성능이 좋아지는 경향을 이전 연구들에서 관찰됐었음."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"기존 BERT는 16GB로 학습했는데, RoBERT는 160GB의 데이터로 학습하였음."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"당연하게도 160GB로 학습한 RoBERTa가 더 좋은 성능을 기록함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"학습 시간을 길게 하면 할수록 더 좋은 성능을 보였다고함."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conclusion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"160GB의 데이터"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Dynamic Masking"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"NSP 밴"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"최대한 문장을 구겨넣은 인풋 ([SEP]으로 분리)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"큰 배치사이즈"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"RoBERTa paper / code Abstract BERT를 제대로 학습시키는 법을 제안 BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음 BERT…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"RoBERTa Paper Review","userDate":"11 October 2020","date":"2020-10-11T10:00:00.000Z","tags":["nlp","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png","srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/5dae1/roberta.png 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/35cd7/roberta.png 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png 1134w","sizes":"100vw"},"sources":[{"srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/76436/roberta.webp 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/ce7b4/roberta.webp 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/03f03/roberta.webp 1134w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8835978835978836}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":10,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"8dee7e19-fa95-58ad-8a80-1e44402376ac","excerpt":"Tokenization 문장에서 의미있는 단위로 나누는 작업을 라고 한다. 문자 단위 토큰화 문자 단위로 토큰화를 하는 것이다. 한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 1…","frontmatter":{"title":"Tokenizer","date":"2021-09-13T23:46:37.121Z"},"fields":{"readingTime":{"text":"10 min read"},"slug":"/tokenizer/"}}},{"node":{"id":"21d42035-7177-5790-b983-c664ade00b2c","excerpt":"정규 표현식 정규표현식(regular expression)은 일종의 문자를 표현하는 공식으로, 특정 규칙이 있는 문자열 집합을 추출할 때 자주 사용되는 기법입니다. 주로 Prograaming Language나 Text Editor…","frontmatter":{"title":"정규표현식 (regex)","date":"2021-09-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"30 min read"},"slug":"/regex/"}}},{"node":{"id":"1203ae76-632c-525e-b502-707b3e57a736","excerpt":"최근 NLP 토크나이저를 만드는데 가장 많이 사용되는  라이브러와 실제 사용이 가장 많이 되는  라이브러리로의 변환에 대한 코드를 담고 있습니다. 해당 내용은  버젼에서 수행되었습니다. Train 아래 코드는 wordpiece, char-bpe…","frontmatter":{"title":"Hugging Face Tokenizers","date":"2021-08-11T15:11:55.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/tokenizers/"}}},{"node":{"id":"4a4289a0-53fc-562e-925a-097b91752eec","excerpt":"GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…","frontmatter":{"title":"P-Tuning Paper Review","date":"2021-05-13T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/p_tuning/"}}}]}},"pageContext":{"slug":"/roberta/","prev":{"excerpt":"Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs…","frontmatter":{"title":"Electra Paper Review","tags":["nlp","paper"],"date":"2020-09-23T07:03:47.149Z","draft":null,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABZElEQVQoz02S2arCQBBE8/9flHdBIu5bcItG1OCCcU80asp7GgZuYOjpnqrqLV5Zlvp8Pvp+v3q9Xnq/33o+n3o8HsrzXGmaajgcms87/u120/V61eVyMQunKAqh5env2+12SpJEURRpuVxqsVjofr+bMAREVquV4jg2n88VQZyEcPg8lGezmYkSRBhitVqV7/vqdrs6nU7abDbabrcaDAYKgkCVSkVhGBp+Op1qMplYpR5Z1uu1gRFywgjM53Or+Hw+G5FqaJfqsRQDDmFE0bKWIdAKlQA8Ho/WLm0xO+5g9vu9YZgl882yTIfDwYqAYy0jwCMkANwRQcAlAsM7lSHoFsLBB+8Se5TKXBgs8xqPx3bv9/tmaYd3FvY/3uv1NBqN1G63zcJlJB4OIAAE6/W6ms2mCWHdqdVq6nQ69t5qtcwHT4w7cXZggo1Gw0QhAiYrFjDLcL8VM3SHGJZlslR8RvEDFpSXtUMvB8QAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png","srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/c68af/electra.png 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/87f65/electra.png 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/d464a/electra.png 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/59ccb/electra.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ccdb9951c9d362c8a3548e8c2a87231/9fb02/electra.webp 750w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/cd76f/electra.webp 1080w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/b7397/electra.webp 1366w,\n/static/7ccdb9951c9d362c8a3548e8c2a87231/507b8/electra.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.4479166666666667}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"post","slug":"/electra/"}},"next":{"excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","tags":["speech","tts","paper"],"date":"2020-10-14T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABtElEQVQozyWQW2/aQBCF+f/PrdT+h6oVtWhVpWlKgJIQLiJQbi4Xe20cr41312BjGHy2K3gYfXN0pE+jqbgBQ3fRx8u8h+FmBHFU2Nk2gkYTQauNuNsDf3q+5qO/hda6jCIFexFjMedYrXZI0wPO59O1q+SnHLNggcFyiL/+FC5niCdTpEYm208oBkMoQ9n8g3zjIEli7NMLRkOFdovBdS4I/DekSt2EWZ5B5gpB8gZ5VKXKFDLOcXIZCuah8HxDs5usswxCSLBtBC/ghiE2DkMYcxTn4iYUe4HHeQs/Xx/wMG3gX7gGf+ki/PQZcdUC1RsIv1QRmQzHxUHusJ31Ea3GiJYjhFuGzEmh8xK4Cg+y7DqDsm13yud1t2SJX+6Gr5C171A/7nB6bCI1FFYN5Hi4CB/x3TvI3x+RmjkrD+G9Z6TydmFn3UN91kRz0cavSR1MmsebS45WTYuqpffWN50Y5l8trY2wNH1W/6CT+/damtFFhH1H6My9ClHZFwdaOisaT8YUyZgKOpHinNzZnDzbJhUEtJ5OaT2ZUC4k6cuZVMhoPuqTCFxK5Y7iFaciPZIR0n+H2vvkK4i2aQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/deca33714347f50cf9f1b33b2db865ef/7189c/multilingual-tts.png","srcSet":"/static/deca33714347f50cf9f1b33b2db865ef/cefb5/multilingual-tts.png 750w,\n/static/deca33714347f50cf9f1b33b2db865ef/c1615/multilingual-tts.png 1080w,\n/static/deca33714347f50cf9f1b33b2db865ef/7189c/multilingual-tts.png 1280w","sizes":"100vw"},"sources":[{"srcSet":"/static/deca33714347f50cf9f1b33b2db865ef/da87f/multilingual-tts.webp 750w,\n/static/deca33714347f50cf9f1b33b2db865ef/bd382/multilingual-tts.webp 1080w,\n/static/deca33714347f50cf9f1b33b2db865ef/2dc0b/multilingual-tts.webp 1280w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.328125}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/one-model-many-langs/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}