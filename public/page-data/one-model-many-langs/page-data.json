{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/one-model-many-langs/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech</h1>\n<p>Tomáš Nekvinda, Ondřej Dušek<br>\nCharles University<br>\nINTERSPEECH, 2020</p>\n<h2>Reference</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2008.00768\">ArXiv</a></li>\n<li><a href=\"https://github.com/Tomiinek/Multilingual_Text_to_Speech\">Source Code</a></li>\n<li><a href=\"https://tomiinek.github.io/multilingual_speech_samples/\">Demo Webpage</a></li>\n<li><a href=\"https://arxiv.org/abs/1703.10135\">Tacotron</a>, <a href=\"https://arxiv.org/abs/1712.05884\">Tacotron2</a></li>\n<li><a href=\"https://arxiv.org/pdf/1710.08969.pdf\">DC-TTS</a></li>\n<li><a href=\"https://github.com/Kyubyong/css10\">CSS 10 Dataset</a></li>\n<li><a href=\"https://commonvoice.mozilla.org/en/datasets\">Common Voice Dataset</a></li>\n</ul>\n<h2>Summary</h2>\n<ul>\n<li>Multilingual Speech Synthesis</li>\n<li>Meta-learning</li>\n<li>Voice Cloning : Speech in multiple languages with the same voice</li>\n<li>Code switching : Speak two (or more) languages with a single utterance.</li>\n<li>Tacotron2 base architecture</li>\n</ul>\n<h2>Tacotron</h2>\n<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwnGyQ%2FbtqDblNauXg%2FJsSXkwgQY1yc3lIHtdgIP0%2Fimg.png\" width=\"700\">\n<ul>\n<li>딥러닝 기반 음성합성의 대표적인 모델</li>\n<li>Attention + Sequence-to-Sequence의 TTS 버전</li>\n<li>Griffin-Lim Vocoder 사용 (빠르지만 성능은 좋지 못함)</li>\n</ul>\n<h2>Tacotron2</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/94840259-1cfbe900-0453-11eb-8803-cac2ea30b425.png\" width=\"470\">  \n<ul>\n<li>Mel-Prediction Network : Attention based Sequence-to-Sequence Network\n<ul>\n<li>인코더에서 Bi-directional LSTM 적용</li>\n<li>Location Sensitive Attention 적용 (음성 Alignment에 강한 어텐션)</li>\n<li>인코더, 디코더에 Convolution Layer 적용</li>\n</ul>\n</li>\n<li>Stop Token 사용</li>\n<li>Vocoder : WaveNet\n<ul>\n<li>장점 : 상당히 고품질의 음성으로 변환</li>\n<li>단점 : 엄청나게 느림</li>\n</ul>\n</li>\n</ul>\n<h2>Model Architecture</h2>\n<img src=\"https://github.com/Tomiinek/Multilingual_Text_to_Speech/raw/master/_img/generated.png\" width=\"800\">\n<ul>\n<li>Tacotron2 기반의 모델들로 실험 진행</li>\n<li>WaveRNN Vocoder 사용</li>\n</ul>\n<h3>This Paper`s Model: Generated (GEN)</h3>\n<ul>\n<li>\n<p><strong>Parameter Generation Convolutional Encoder</strong></p>\n<ul>\n<li>이 논문에서는 Fully convolutional encoder를 사용 (from DC-TTS)</li>\n<li>Cross-lingual knowledge-sharing을 가능하게 하기 위해 인코더 컨볼루션 레이어의 파라미터를 생성하여 사용</li>\n<li>입력되는 Language ID에 따라 Fully Connected 레이어를 통해 다른 다른 파라미터를 생성</li>\n</ul>\n</li>\n<li>\n<p><strong>Speaker Embedding</strong></p>\n<ul>\n<li>Multi-speaker, Cross-lingual voice cloning을 위해 Speaker Embedding을 사용</li>\n<li>인코더 아웃풋에 Concatenate하여 스펙트로그램 생성시에 반영되도록 함</li>\n</ul>\n</li>\n<li>\n<p><strong>Adversarial Speaker Classifier</strong></p>\n<ul>\n<li>이상적으로 Voice cloning을 위해서는 텍스트(언어)로부터 화자의 정보가 반영되면 안됨</li>\n<li>Speaker Classifier와 나머지 모델(인코더, 디코더)은 forward에서는 독립적이지만,  backpropagation을 진행할 때, 두 loss (L2 of predict spectrogram, cross entropy of predicted speaker ID)가 인코더 파라미터 업데이트에 영향을 미침</li>\n<li>Gradient reversal layer를 통해 인코더가 speaker에 대한 정보를 반영 못하도록 학습</li>\n</ul>\n</li>\n</ul>\n<h3>Baselines: Shared, Separate &#x26; Single</h3>\n<p>※ GEN과 다른점만 비교</p>\n<ul>\n<li><strong>Single (SGL)</strong>\n<ul>\n<li>Monolingual Vanilla Tacotron 2 (Code-switching에 사용 X)</li>\n</ul>\n</li>\n<li><strong>Shared (SHA)</strong>\n<ul>\n<li>GEN과 다르게 Tacotron 2의 인코더 사용 (Multilingual)</li>\n</ul>\n</li>\n<li><strong>Separate (SEP)</strong>\n<ul>\n<li>GEN과 같이 Multiple convolution layer를 사용</li>\n<li>Parameter generation 사용 X</li>\n<li>Adversarial speaker classifier 사용 X</li>\n</ul>\n</li>\n</ul>\n<h2>Dataset</h2>\n<p>10개의 언어로 구성된 CSS10과 Common Voice 데이터셋의 일부를 사용\nCode-switching을 학습하기 위해 multi-speaker 데이터가 필요 (언어와 화자 일치를 없애기 위해)</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/95888064-9680c900-0dbb-11eb-9967-a30b21dbfa80.png\" width=\"600\">  \n<h2>Experiment</h2>\n<p>SGL, SHA, SEP, GEN을 비교했을 때 GEN이 거의 모든 결과에서 우수한 성능을 보임</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/95888889-a816a080-0dbc-11eb-81a9-9a2d036f2def.png\" alt=\"image\"></p>\n<img src=\"https://user-images.githubusercontent.com/42150335/95888982-cbd9e680-0dbc-11eb-984f-1524ab3a9f38.png\" width=\"400\">\n<h2>Conclusion</h2>\n<ul>\n<li>본 논문에서 제안하는 모델은 Multilingual Voice cloning, Code-switching에 우수한 성능을 보임</li>\n<li>추후 연구로 어텐션 모듈을 수정하는 것을 생각중이라고 함</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Tomáš Nekvinda, Ondřej Dušek"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nCharles University"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nINTERSPEECH, 2020"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Reference"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/2008.00768"},"children":[{"type":"text","value":"ArXiv"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/Tomiinek/Multilingual_Text_to_Speech"},"children":[{"type":"text","value":"Source Code"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://tomiinek.github.io/multilingual_speech_samples/"},"children":[{"type":"text","value":"Demo Webpage"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1703.10135"},"children":[{"type":"text","value":"Tacotron"}]},{"type":"text","value":", "},{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1712.05884"},"children":[{"type":"text","value":"Tacotron2"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1710.08969.pdf"},"children":[{"type":"text","value":"DC-TTS"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/Kyubyong/css10"},"children":[{"type":"text","value":"CSS 10 Dataset"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://commonvoice.mozilla.org/en/datasets"},"children":[{"type":"text","value":"Common Voice Dataset"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Summary"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Multilingual Speech Synthesis"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Meta-learning"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Voice Cloning : Speech in multiple languages with the same voice"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Code switching : Speak two (or more) languages with a single utterance."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Tacotron2 base architecture"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Tacotron"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwnGyQ%2FbtqDblNauXg%2FJsSXkwgQY1yc3lIHtdgIP0%2Fimg.png","width":700},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"딥러닝 기반 음성합성의 대표적인 모델"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Attention + Sequence-to-Sequence의 TTS 버전"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Griffin-Lim Vocoder 사용 (빠르지만 성능은 좋지 못함)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Tacotron2"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/94840259-1cfbe900-0453-11eb-8803-cac2ea30b425.png","width":470},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Mel-Prediction Network : Attention based Sequence-to-Sequence Network\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인코더에서 Bi-directional LSTM 적용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Location Sensitive Attention 적용 (음성 Alignment에 강한 어텐션)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인코더, 디코더에 Convolution Layer 적용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Stop Token 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Vocoder : WaveNet\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"장점 : 상당히 고품질의 음성으로 변환"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"단점 : 엄청나게 느림"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Model Architecture"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://github.com/Tomiinek/Multilingual_Text_to_Speech/raw/master/_img/generated.png","width":800},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Tacotron2 기반의 모델들로 실험 진행"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"WaveRNN Vocoder 사용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"This Paper`s Model: Generated (GEN)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Parameter Generation Convolutional Encoder"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 논문에서는 Fully convolutional encoder를 사용 (from DC-TTS)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Cross-lingual knowledge-sharing을 가능하게 하기 위해 인코더 컨볼루션 레이어의 파라미터를 생성하여 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"입력되는 Language ID에 따라 Fully Connected 레이어를 통해 다른 다른 파라미터를 생성"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Speaker Embedding"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Multi-speaker, Cross-lingual voice cloning을 위해 Speaker Embedding을 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인코더 아웃풋에 Concatenate하여 스펙트로그램 생성시에 반영되도록 함"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Adversarial Speaker Classifier"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이상적으로 Voice cloning을 위해서는 텍스트(언어)로부터 화자의 정보가 반영되면 안됨"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Speaker Classifier와 나머지 모델(인코더, 디코더)은 forward에서는 독립적이지만,  backpropagation을 진행할 때, 두 loss (L2 of predict spectrogram, cross entropy of predicted speaker ID)가 인코더 파라미터 업데이트에 영향을 미침"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Gradient reversal layer를 통해 인코더가 speaker에 대한 정보를 반영 못하도록 학습"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Baselines: Shared, Separate & Single"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"※ GEN과 다른점만 비교"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Single (SGL)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Monolingual Vanilla Tacotron 2 (Code-switching에 사용 X)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Shared (SHA)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"GEN과 다르게 Tacotron 2의 인코더 사용 (Multilingual)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Separate (SEP)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"GEN과 같이 Multiple convolution layer를 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Parameter generation 사용 X"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Adversarial speaker classifier 사용 X"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Dataset"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"10개의 언어로 구성된 CSS10과 Common Voice 데이터셋의 일부를 사용\nCode-switching을 학습하기 위해 multi-speaker 데이터가 필요 (언어와 화자 일치를 없애기 위해)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/95888064-9680c900-0dbb-11eb-9967-a30b21dbfa80.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"SGL, SHA, SEP, GEN을 비교했을 때 GEN이 거의 모든 결과에서 우수한 성능을 보임"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/95888889-a816a080-0dbc-11eb-81a9-9a2d036f2def.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/95888982-cbd9e680-0dbc-11eb-984f-1524ab3a9f38.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conclusion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"본 논문에서 제안하는 모델은 Multilingual Voice cloning, Code-switching에 우수한 성능을 보임"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"추후 연구로 어텐션 모듈을 수정하는 것을 생각중이라고 함"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","userDate":"14 October 2020","date":"2020-10-14T10:00:00.000Z","tags":["speech","tts","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/deca33714347f50cf9f1b33b2db865ef/7189c/multilingual-tts.png","srcSet":"/static/deca33714347f50cf9f1b33b2db865ef/cefb5/multilingual-tts.png 750w,\n/static/deca33714347f50cf9f1b33b2db865ef/c1615/multilingual-tts.png 1080w,\n/static/deca33714347f50cf9f1b33b2db865ef/7189c/multilingual-tts.png 1280w","sizes":"100vw"},"sources":[{"srcSet":"/static/deca33714347f50cf9f1b33b2db865ef/da87f/multilingual-tts.webp 750w,\n/static/deca33714347f50cf9f1b33b2db865ef/bd382/multilingual-tts.webp 1080w,\n/static/deca33714347f50cf9f1b33b2db865ef/2dc0b/multilingual-tts.webp 1280w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.328125}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"19ded62e-3e91-5733-9329-a1c7bdcf859b","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"35eadfc7-646b-5194-a711-ce20e840ba58","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"fd3185b8-63e4-5e3d-aeb3-e67ed1343af9","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"aad087b1-4b0f-5956-ab2f-d7ab33fdb8c4","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/one-model-many-langs/","prev":{"excerpt":"RoBERTa paper / code Abstract BERT를 제대로 학습시키는 법을 제안 BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음 BERT…","frontmatter":{"title":"RoBERTa Paper Review","tags":["nlp","paper"],"date":"2020-10-11T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAADh0lEQVQ4y61USY/iRhTmhybXSJGSa/IDkiiHUTQdzaGTQzZpcunTqGemmW4aMOANDF4w2CwNGDfQzb7ZZufLc9EdjeaalPT0Xr3vq6+eq1wvEgQB3HYbs9kM4Tgej//JItPJGJwkwel18X+MSLA/oHObwFgvwtsesFz4WC49LJZL8mEcYOkFzC9YvKJ49YQ9c1aE+VivN4gUag5iL14i/vtrXMplRNMKPgh1xLIuoryNq4yCd6kC+QKuRR1vuTzLRXkLN7LLfJRXcJnUkdOqiOS1IpIVAXFTAKelUGjy4PQOlAbAl1oQSjdIFJKIF65xm/+AmBKDUueQNquQKgfiNMCbUcRVDXnTRUQ167iWG0jk20hq97SwSaQpsrUNUsUhYrk6CTm4ydYRFauIU5xQW0gTFnIy5gi3CnEKXRTMFiJ6uQnBXhG4JttCrq4h2h6b5xs7aA6gtQE9NBeUOxK2g0y4VAmIv2LCoUa+2DgJpksLqmqGTGmOlDGGYC3xhqvh1Z9XeHF+gVd/vMfPv13il79juBLbDM+YM/DlJYtFi9ZbPgk2T4K85dFOAatKqvjMUsYEb5JVXFwX8TbdYEJXgkPnO6Lz3bGqwuok24dc8Z4EqUKNBJPGHJwxJZvR4Y5JbEq7EqnsUQU+WUCxzxalzAWS+oRxT5wl5ejLTI8uhSo0jByama/hSN/ijv8G48KXkLUS+BqdnZpEL/MZqtkzhj3KX8CWznAvfoVG9kdkKsSh2++Ln6Os/AWl1KNb1lQoiZdQuHPo/K+oSD8hrd4hZQOyqqLIfQ9FvICWOUfu5gdk+QuY/BlU6TVS5R0kJQuT+w6i+B4y3Vyk+zBCrtSBaj+QPdIuXRQr9zCsBnS7w+Z69ZFhBasHvTZAvky/iEU8qwmj2kWe8vmSC7dDFR53W/RVBcOSgWHZQFcWsA989i63szGcTBLjehUjwjuEzVt36GRFDO0S46zHQzjpJOZu6/SW156PtiCgnc3hPpdDRxAxGwxxJHDe6+HuNg6XXkGXMCedRkfX4RL/wTBwCDndLmqxGIbV2kkwbDn7wwEH8pvtFj61M9/3Eba11WrFmsRuv2f4s63WawSEhbyQs/Q8lmeCn7af+XyO0WjE+mO44PhE/Hh4JDCZTDAYDDCdTrHZbP7tpZGPm2M4wmZrWRbq9Tr6/cEp/wmn3+/Dtm3UajU4jsO+5lnwH3sJDqGy7C/mAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png","srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/5dae1/roberta.png 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/35cd7/roberta.png 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png 1134w","sizes":"100vw"},"sources":[{"srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/76436/roberta.webp 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/ce7b4/roberta.webp 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/03f03/roberta.webp 1134w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8835978835978836}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/roberta/"}},"next":{"excerpt":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism ​ Mohammad Shoeybi et al. 2019. NVIDIA Corp. ​ Summary…","frontmatter":{"title":"Megatron LM Paper Review","tags":["nlp","parallelism","paper"],"date":"2020-12-03T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAACbUlEQVQoz32SyWoUcRDG21fw7hv4CoKCXvWgJ3FDEfTsQVEQRcnBgHgU8aASJaK4RaMXQ7ZZepnee3qZ7p7uSSbp6TRJnAE1OOTn/6IHDxYUVVR9X1EfVdL29haqquCabSzToSlrqM0mzVqNlqGjqCpVWTEaDjFELoueqqgoTRVTs5lfWqTeaOB4DuPxGMmybGRZZVZ7hR+1SAMFL0kxopDAMFEaNWpqHdOymJNlzLZHEIdoboNZ9TWhbuHoOq2aTrFeIHmuTui6XPl6jLr9gm1riv0bFXvLPsanGvcXbnC+fZaPSgep6HO0a3Kxd5oP3hR36ucYezld0+CZ/5x8LUPK3Hck5heufz2LlXxhlC7yYDji1lZFojrMtl7y2H+E7MZc6q/weC3l/dYbFpIZ7jUuUyptXKXJofwwwSBAyt23pNZnbsydI1hdZKfyKX+N6f34ziBOCdsWWtAiiDooWUa2ucH38U/M9QXuzl9kpWZi1pc5kB0kKMXAMLDIooir88dputMM7Sn2lRvsWc+xPjeYXLrGmfAks7qQM1jjSGZwqneCGSFxQrkgJPfIHYub+W26RYrkBwGh3+GpOknckxmtyLz+NuTJZkXacpjTZnjpTKM6IRNpwtuVhPlqDntV4JyHDDtiYOATOTHF2jqSYRgoikw3zomErLCTsJnn9P2AOIlxDAejYWDoLSLxLkU3E16QdXPMloUu+FGnw9LyIv1+H6ksSzRNw3EdXHFtz3OxRdQE0PM8QdAZbAwYjUYo4j0cUfN8D19sVW/UxTIKtm1j2RY7OztI/Md2d3cpioI4jjFNk6qq/tb/xf2x39lGqyQia/HFAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/a4dec0b9c36035e9191658ce9647ae73/a70e6/megatron.png","srcSet":"/static/a4dec0b9c36035e9191658ce9647ae73/37b55/megatron.png 750w,\n/static/a4dec0b9c36035e9191658ce9647ae73/a70e6/megatron.png 791w","sizes":"100vw"},"sources":[{"srcSet":"/static/a4dec0b9c36035e9191658ce9647ae73/0b2ce/megatron.webp 750w,\n/static/a4dec0b9c36035e9191658ce9647ae73/c471e/megatron.webp 791w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5170670037926676}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/Megatron-lm/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}