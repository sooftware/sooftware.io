{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/one-model-many-langs/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech</h1>\n<p>Tomáš Nekvinda, Ondřej Dušek<br>\nCharles University<br>\nINTERSPEECH, 2020</p>\n<h2>Reference</h2>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2008.00768\">ArXiv</a></li>\n<li><a href=\"https://github.com/Tomiinek/Multilingual_Text_to_Speech\">Source Code</a></li>\n<li><a href=\"https://tomiinek.github.io/multilingual_speech_samples/\">Demo Webpage</a></li>\n<li><a href=\"https://arxiv.org/abs/1703.10135\">Tacotron</a>, <a href=\"https://arxiv.org/abs/1712.05884\">Tacotron2</a></li>\n<li><a href=\"https://arxiv.org/pdf/1710.08969.pdf\">DC-TTS</a></li>\n<li><a href=\"https://github.com/Kyubyong/css10\">CSS 10 Dataset</a></li>\n<li><a href=\"https://commonvoice.mozilla.org/en/datasets\">Common Voice Dataset</a></li>\n</ul>\n<h2>Summary</h2>\n<ul>\n<li>Multilingual Speech Synthesis</li>\n<li>Meta-learning</li>\n<li>Voice Cloning : Speech in multiple languages with the same voice</li>\n<li>Code switching : Speak two (or more) languages with a single utterance.</li>\n<li>Tacotron2 base architecture</li>\n</ul>\n<h2>Tacotron</h2>\n<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwnGyQ%2FbtqDblNauXg%2FJsSXkwgQY1yc3lIHtdgIP0%2Fimg.png\" width=\"700\">\n<ul>\n<li>딥러닝 기반 음성합성의 대표적인 모델</li>\n<li>Attention + Sequence-to-Sequence의 TTS 버전</li>\n<li>Griffin-Lim Vocoder 사용 (빠르지만 성능은 좋지 못함)</li>\n</ul>\n<h2>Tacotron2</h2>\n<img src=\"https://user-images.githubusercontent.com/42150335/94840259-1cfbe900-0453-11eb-8803-cac2ea30b425.png\" width=\"470\">  \n<ul>\n<li>Mel-Prediction Network : Attention based Sequence-to-Sequence Network\n<ul>\n<li>인코더에서 Bi-directional LSTM 적용</li>\n<li>Location Sensitive Attention 적용 (음성 Alignment에 강한 어텐션)</li>\n<li>인코더, 디코더에 Convolution Layer 적용</li>\n</ul>\n</li>\n<li>Stop Token 사용</li>\n<li>Vocoder : WaveNet\n<ul>\n<li>장점 : 상당히 고품질의 음성으로 변환</li>\n<li>단점 : 엄청나게 느림</li>\n</ul>\n</li>\n</ul>\n<h2>Model Architecture</h2>\n<img src=\"https://github.com/Tomiinek/Multilingual_Text_to_Speech/raw/master/_img/generated.png\" width=\"800\">\n<ul>\n<li>Tacotron2 기반의 모델들로 실험 진행</li>\n<li>WaveRNN Vocoder 사용</li>\n</ul>\n<h3>This Paper`s Model: Generated (GEN)</h3>\n<ul>\n<li>\n<p><strong>Parameter Generation Convolutional Encoder</strong></p>\n<ul>\n<li>이 논문에서는 Fully convolutional encoder를 사용 (from DC-TTS)</li>\n<li>Cross-lingual knowledge-sharing을 가능하게 하기 위해 인코더 컨볼루션 레이어의 파라미터를 생성하여 사용</li>\n<li>입력되는 Language ID에 따라 Fully Connected 레이어를 통해 다른 다른 파라미터를 생성</li>\n</ul>\n</li>\n<li>\n<p><strong>Speaker Embedding</strong></p>\n<ul>\n<li>Multi-speaker, Cross-lingual voice cloning을 위해 Speaker Embedding을 사용</li>\n<li>인코더 아웃풋에 Concatenate하여 스펙트로그램 생성시에 반영되도록 함</li>\n</ul>\n</li>\n<li>\n<p><strong>Adversarial Speaker Classifier</strong></p>\n<ul>\n<li>이상적으로 Voice cloning을 위해서는 텍스트(언어)로부터 화자의 정보가 반영되면 안됨</li>\n<li>Speaker Classifier와 나머지 모델(인코더, 디코더)은 forward에서는 독립적이지만,  backpropagation을 진행할 때, 두 loss (L2 of predict spectrogram, cross entropy of predicted speaker ID)가 인코더 파라미터 업데이트에 영향을 미침</li>\n<li>Gradient reversal layer를 통해 인코더가 speaker에 대한 정보를 반영 못하도록 학습</li>\n</ul>\n</li>\n</ul>\n<h3>Baselines: Shared, Separate &#x26; Single</h3>\n<p>※ GEN과 다른점만 비교</p>\n<ul>\n<li><strong>Single (SGL)</strong>\n<ul>\n<li>Monolingual Vanilla Tacotron 2 (Code-switching에 사용 X)</li>\n</ul>\n</li>\n<li><strong>Shared (SHA)</strong>\n<ul>\n<li>GEN과 다르게 Tacotron 2의 인코더 사용 (Multilingual)</li>\n</ul>\n</li>\n<li><strong>Separate (SEP)</strong>\n<ul>\n<li>GEN과 같이 Multiple convolution layer를 사용</li>\n<li>Parameter generation 사용 X</li>\n<li>Adversarial speaker classifier 사용 X</li>\n</ul>\n</li>\n</ul>\n<h2>Dataset</h2>\n<p>10개의 언어로 구성된 CSS10과 Common Voice 데이터셋의 일부를 사용\nCode-switching을 학습하기 위해 multi-speaker 데이터가 필요 (언어와 화자 일치를 없애기 위해)</p>\n<img src=\"https://user-images.githubusercontent.com/42150335/95888064-9680c900-0dbb-11eb-9967-a30b21dbfa80.png\" width=\"600\">  \n<h2>Experiment</h2>\n<p>SGL, SHA, SEP, GEN을 비교했을 때 GEN이 거의 모든 결과에서 우수한 성능을 보임</p>\n<p><img src=\"https://user-images.githubusercontent.com/42150335/95888889-a816a080-0dbc-11eb-81a9-9a2d036f2def.png\" alt=\"image\"></p>\n<img src=\"https://user-images.githubusercontent.com/42150335/95888982-cbd9e680-0dbc-11eb-984f-1524ab3a9f38.png\" width=\"400\">\n<h2>Conclusion</h2>\n<ul>\n<li>본 논문에서 제안하는 모델은 Multilingual Voice cloning, Code-switching에 우수한 성능을 보임</li>\n<li>추후 연구로 어텐션 모듈을 수정하는 것을 생각중이라고 함</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Tomáš Nekvinda, Ondřej Dušek"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nCharles University"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nINTERSPEECH, 2020"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Reference"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/2008.00768"},"children":[{"type":"text","value":"ArXiv"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/Tomiinek/Multilingual_Text_to_Speech"},"children":[{"type":"text","value":"Source Code"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://tomiinek.github.io/multilingual_speech_samples/"},"children":[{"type":"text","value":"Demo Webpage"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1703.10135"},"children":[{"type":"text","value":"Tacotron"}]},{"type":"text","value":", "},{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1712.05884"},"children":[{"type":"text","value":"Tacotron2"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/pdf/1710.08969.pdf"},"children":[{"type":"text","value":"DC-TTS"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://github.com/Kyubyong/css10"},"children":[{"type":"text","value":"CSS 10 Dataset"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://commonvoice.mozilla.org/en/datasets"},"children":[{"type":"text","value":"Common Voice Dataset"}]}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Summary"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Multilingual Speech Synthesis"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Meta-learning"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Voice Cloning : Speech in multiple languages with the same voice"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Code switching : Speak two (or more) languages with a single utterance."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Tacotron2 base architecture"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Tacotron"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwnGyQ%2FbtqDblNauXg%2FJsSXkwgQY1yc3lIHtdgIP0%2Fimg.png","width":700},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"딥러닝 기반 음성합성의 대표적인 모델"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Attention + Sequence-to-Sequence의 TTS 버전"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Griffin-Lim Vocoder 사용 (빠르지만 성능은 좋지 못함)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Tacotron2"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/94840259-1cfbe900-0453-11eb-8803-cac2ea30b425.png","width":470},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Mel-Prediction Network : Attention based Sequence-to-Sequence Network\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인코더에서 Bi-directional LSTM 적용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Location Sensitive Attention 적용 (음성 Alignment에 강한 어텐션)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인코더, 디코더에 Convolution Layer 적용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Stop Token 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Vocoder : WaveNet\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"장점 : 상당히 고품질의 음성으로 변환"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"단점 : 엄청나게 느림"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Model Architecture"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://github.com/Tomiinek/Multilingual_Text_to_Speech/raw/master/_img/generated.png","width":800},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Tacotron2 기반의 모델들로 실험 진행"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"WaveRNN Vocoder 사용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"This Paper`s Model: Generated (GEN)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Parameter Generation Convolutional Encoder"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 논문에서는 Fully convolutional encoder를 사용 (from DC-TTS)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Cross-lingual knowledge-sharing을 가능하게 하기 위해 인코더 컨볼루션 레이어의 파라미터를 생성하여 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"입력되는 Language ID에 따라 Fully Connected 레이어를 통해 다른 다른 파라미터를 생성"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Speaker Embedding"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Multi-speaker, Cross-lingual voice cloning을 위해 Speaker Embedding을 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인코더 아웃풋에 Concatenate하여 스펙트로그램 생성시에 반영되도록 함"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Adversarial Speaker Classifier"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이상적으로 Voice cloning을 위해서는 텍스트(언어)로부터 화자의 정보가 반영되면 안됨"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Speaker Classifier와 나머지 모델(인코더, 디코더)은 forward에서는 독립적이지만,  backpropagation을 진행할 때, 두 loss (L2 of predict spectrogram, cross entropy of predicted speaker ID)가 인코더 파라미터 업데이트에 영향을 미침"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Gradient reversal layer를 통해 인코더가 speaker에 대한 정보를 반영 못하도록 학습"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Baselines: Shared, Separate & Single"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"※ GEN과 다른점만 비교"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Single (SGL)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Monolingual Vanilla Tacotron 2 (Code-switching에 사용 X)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Shared (SHA)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"GEN과 다르게 Tacotron 2의 인코더 사용 (Multilingual)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Separate (SEP)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"GEN과 같이 Multiple convolution layer를 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Parameter generation 사용 X"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Adversarial speaker classifier 사용 X"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Dataset"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"10개의 언어로 구성된 CSS10과 Common Voice 데이터셋의 일부를 사용\nCode-switching을 학습하기 위해 multi-speaker 데이터가 필요 (언어와 화자 일치를 없애기 위해)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/95888064-9680c900-0dbb-11eb-9967-a30b21dbfa80.png","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiment"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"SGL, SHA, SEP, GEN을 비교했을 때 GEN이 거의 모든 결과에서 우수한 성능을 보임"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/95888889-a816a080-0dbc-11eb-81a9-9a2d036f2def.png","alt":"image"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://user-images.githubusercontent.com/42150335/95888982-cbd9e680-0dbc-11eb-984f-1524ab3a9f38.png","width":400},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conclusion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"본 논문에서 제안하는 모델은 Multilingual Voice cloning, Code-switching에 우수한 성능을 보임"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"추후 연구로 어텐션 모듈을 수정하는 것을 생각중이라고 함"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","userDate":"14 October 2020","date":"2020-10-14T10:00:00.000Z","tags":["speech","tts","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/deca33714347f50cf9f1b33b2db865ef/7189c/multilingual-tts.png","srcSet":"/static/deca33714347f50cf9f1b33b2db865ef/cefb5/multilingual-tts.png 750w,\n/static/deca33714347f50cf9f1b33b2db865ef/c1615/multilingual-tts.png 1080w,\n/static/deca33714347f50cf9f1b33b2db865ef/7189c/multilingual-tts.png 1280w","sizes":"100vw"},"sources":[{"srcSet":"/static/deca33714347f50cf9f1b33b2db865ef/da87f/multilingual-tts.webp 750w,\n/static/deca33714347f50cf9f1b33b2db865ef/bd382/multilingual-tts.webp 1080w,\n/static/deca33714347f50cf9f1b33b2db865ef/2dc0b/multilingual-tts.webp 1280w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.328125}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#181818","images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/2456b/soohwan.png 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/ab12d/soohwan.png 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/2584f/soohwan.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65256/soohwan.webp 40w,\n/static/a2699b4a2f164aa71106535e018ceafc/c6b8d/soohwan.webp 80w,\n/static/a2699b4a2f164aa71106535e018ceafc/03d15/soohwan.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.25}}]}}]}},"relatedPosts":{"totalCount":20,"edges":[{"node":{"id":"f8857a8d-9121-5e23-91bc-3fbe4f090418","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"93dec710-458c-531f-acbc-28d14f762768","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"127c2cf6-c23b-5410-a480-70de1d98ce87","excerpt":"PORORO Text-To-Speech (TTS) 얼마전에 저희 팀에서 공개한 PORORO: Platform Of neuRal mOdels for natuRal language prOcessing 라이브러리에 제가 공들여만든 TTS…","frontmatter":{"title":"PORORO Text-To-Speech (TTS)","date":"2021-02-16T10:00:00.000Z"},"fields":{"readingTime":{"text":"1 min read"},"slug":"/pororo-tts/"}}},{"node":{"id":"a4f99081-c696-5b06-85b1-c2268aed1215","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"e1128eb2-c5b3-55e1-83ef-3b6478bb7d7b","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}}]}},"pageContext":{"slug":"/one-model-many-langs/","prev":{"excerpt":"RoBERTa paper / code Abstract BERT를 제대로 학습시키는 법을 제안 BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음 BERT…","frontmatter":{"title":"RoBERTa Paper Review","tags":["nlp","paper"],"date":"2020-10-11T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAADh0lEQVQ4y61USY/iRhTmhybXSJGSa/IDkiiHUTQdzaGTQzZpcunTqGemmW4aMOANDF4w2CwNGDfQzb7ZZufLc9EdjeaalPT0Xr3vq6+eq1wvEgQB3HYbs9kM4Tgej//JItPJGJwkwel18X+MSLA/oHObwFgvwtsesFz4WC49LJZL8mEcYOkFzC9YvKJ49YQ9c1aE+VivN4gUag5iL14i/vtrXMplRNMKPgh1xLIuoryNq4yCd6kC+QKuRR1vuTzLRXkLN7LLfJRXcJnUkdOqiOS1IpIVAXFTAKelUGjy4PQOlAbAl1oQSjdIFJKIF65xm/+AmBKDUueQNquQKgfiNMCbUcRVDXnTRUQ167iWG0jk20hq97SwSaQpsrUNUsUhYrk6CTm4ydYRFauIU5xQW0gTFnIy5gi3CnEKXRTMFiJ6uQnBXhG4JttCrq4h2h6b5xs7aA6gtQE9NBeUOxK2g0y4VAmIv2LCoUa+2DgJpksLqmqGTGmOlDGGYC3xhqvh1Z9XeHF+gVd/vMfPv13il79juBLbDM+YM/DlJYtFi9ZbPgk2T4K85dFOAatKqvjMUsYEb5JVXFwX8TbdYEJXgkPnO6Lz3bGqwuok24dc8Z4EqUKNBJPGHJwxJZvR4Y5JbEq7EqnsUQU+WUCxzxalzAWS+oRxT5wl5ejLTI8uhSo0jByama/hSN/ijv8G48KXkLUS+BqdnZpEL/MZqtkzhj3KX8CWznAvfoVG9kdkKsSh2++Ln6Os/AWl1KNb1lQoiZdQuHPo/K+oSD8hrd4hZQOyqqLIfQ9FvICWOUfu5gdk+QuY/BlU6TVS5R0kJQuT+w6i+B4y3Vyk+zBCrtSBaj+QPdIuXRQr9zCsBnS7w+Z69ZFhBasHvTZAvky/iEU8qwmj2kWe8vmSC7dDFR53W/RVBcOSgWHZQFcWsA989i63szGcTBLjehUjwjuEzVt36GRFDO0S46zHQzjpJOZu6/SW156PtiCgnc3hPpdDRxAxGwxxJHDe6+HuNg6XXkGXMCedRkfX4RL/wTBwCDndLmqxGIbV2kkwbDn7wwEH8pvtFj61M9/3Eba11WrFmsRuv2f4s63WawSEhbyQs/Q8lmeCn7af+XyO0WjE+mO44PhE/Hh4JDCZTDAYDDCdTrHZbP7tpZGPm2M4wmZrWRbq9Tr6/cEp/wmn3+/Dtm3UajU4jsO+5lnwH3sJDqGy7C/mAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png","srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/5dae1/roberta.png 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/35cd7/roberta.png 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/1be58/roberta.png 1134w","sizes":"100vw"},"sources":[{"srcSet":"/static/26b5f2f1a6d4d031c6cf36eac285256a/76436/roberta.webp 750w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/ce7b4/roberta.webp 1080w,\n/static/26b5f2f1a6d4d031c6cf36eac285256a/03f03/roberta.webp 1134w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8835978835978836}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"4 min read"},"layout":"","slug":"/roberta/"}},"next":{"excerpt":"Mac iTerm2 + ZSH 세팅 개발환경에서 가장 중요한 소프트웨어 중 하나는 쉘입니다. 어떤 OS에서 작업하냐에 따라서 어떤 쉘을 쓰는지 등이 달라질텐데요, Mac OS에서 가장 많이 사용되는 iTerm2와 ZSH…","frontmatter":{"title":"Mac iTerm2 + ZSH 세팅","tags":["mac"],"date":"2020-12-02T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAABrklEQVQoz62Q2W7TYBCF/SJsEktIvMSx4yS1azt2vGVnqdQi8TigiuxJyQ1914+xgQihVEKIi6MZzX/OmfOPcn+4Y787cPx2z/pw5Ha5ZrtdsN5uWKx37BdLVqsNm92eu69H9sLf7Q98kfmnz7dSVyzlvUTZK8PLgG7XxY1TzI6H2myhG3XqmsYr1cKsNVDVJg29xeuGQa1EXedlTePJsxc8fvqcRz9R9spNFON7AeF4RljMiPKC0ThkkA/wkwnzICSJM7LhhL7UOC0IowQ/jGm2HPSmjWG2T1X5GIa4nQuC6Ru80rAomE4DkmJAmM+56UfEfsD83Xuurj8wFl4xnjKazHAvQ0yrQ8vuYju9aoFyFSc/Eo4m+OmIfppLwpgoiej6CVnPI3B94kFGkg2rZGk+qqrtXNAWWO0eTterzJVrMUqTnGz2lmQ8Zzidkw8HeL6P3QsJZLMrArtKcVEJHbl5Ka6Xt9XMCnXVRJU7K7b8XTcsdCHoEtkQmC1L4rdpWmIidzFMR8hWJfgdmuj+hNL4RdDME/E0081Tf058bq6cIz6Eh4z/2fBv8N8NvwO4x3tV6vu/mQAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/0804d911b246a6b0f242a087c53d6920/ffc42/iterm_zsh.png","srcSet":"/static/0804d911b246a6b0f242a087c53d6920/ffc42/iterm_zsh.png 673w","sizes":"100vw"},"sources":[{"srcSet":"/static/0804d911b246a6b0f242a087c53d6920/20eb0/iterm_zsh.webp 673w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5557206537890045}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,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"},"images":{"fallback":{"src":"/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png","srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/65307/soohwan.png 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/bf8e0/soohwan.png 1080w","sizes":"100vw"},"sources":[{"srcSet":"/static/a2699b4a2f164aa71106535e018ceafc/f6200/soohwan.webp 750w,\n/static/a2699b4a2f164aa71106535e018ceafc/529f6/soohwan.webp 1080w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.2462962962962962}}}}]},"fields":{"readingTime":{"text":"7 min read"},"layout":"","slug":"/iterm_zsh/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}