{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/longformer/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Longformer: The Long-Document Transformer</h1>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2004.05150\">Paper</a> <a href=\"https://github.com/allenai/longformer\">Code</a></li>\n<li>Iz Beltagy et al.</li>\n</ul>\n<h2>Introduction</h2>\n<ul>\n<li>트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음</li>\n<li>이유는 시퀀스 길이에 O(n^2)하게 늘어나는 높은 복잡도 때문</li>\n<li>본 논문은 Attention 이루어지는 복잡도를 낮추는 방법을 제안 (O(n))</li>\n<li>BERT는 512 토큰을 limit으로 가지는데, 본 논문 모델은 4096 토큰까지 가능</li>\n<li><code class=\"language-text\">text8</code>, <code class=\"language-text\">enwik8</code>, <code class=\"language-text\">Wiki-Hop</code>, <code class=\"language-text\">TriviaQA</code>에서 State-Of-The-Art 달성</li>\n</ul>\n<h2>Attention Method</h2>\n<img src=\"https://haebinshin.github.io/public/img/longformer/figure2.png\">  \n<p>본 논문에서는 위 그림과 같은 3가지 어텐션 방식을 제안</p>\n<h3>Sliding window Attention</h3>\n<ul>\n<li>크기가 w인 sliding window 내에서만 attention을 수행하는 방법</li>\n<li>이 방법은 텍스트 길이 n에 대해 O(n x w)의 복잡도를 가짐</li>\n<li>이러한 방식은 레이어가 깊어짐에 따라 receptive field가 넓어지는 CNN과 유사함</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/receptive_field.png\">  \n<ul>\n<li>예) window size가 2일 때, 레이어가 쌓일수록 w만큼 receptive field가 넓어짐.</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/text_sliding_window_receptive_field.jpg\">\n<ul>\n<li>l x w의 receptive field size를 가지게 됨.</li>\n<li>각 레이어마다 w의 크기를 다르게 주는 방법이 도움이 될 수도 있음</li>\n</ul>\n<h3>Dilated Sliding Window</h3>\n<ul>\n<li>Sliding window attention보다도 receptive field를 더 넓히기 위해 고안된 방법</li>\n<li>Dilated Convolution에서 착안</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/dilation_convolution.gif\">  \n<ul>\n<li>dilated 값을 줘서 토큰을 d만큼 건너뛰면서 어텐션하도록 하는 방법</li>\n<li>예) window size가 2이고 dilation size가 2일 때, 아래 그림과 같이 w x d만큼 receptive field가 넓어짐</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/text_dilated_sliding_window_receptive_field.jpg\">\n<ul>\n<li>l x d x w의 receptive field size를 가지게 됨.</li>\n</ul>\n<h3>Global Attention</h3>\n<ul>\n<li>BERT의 [CLS] 토큰 같은 경우는 전체 컨텍스트를 바라봐야하는데, 위의 2가지 방법만으로는 Finetuning하는 태스크에서는 부족한 부분이 있을 수 있음</li>\n<li>따라서 스페셜 토큰 몇 개에 대해서는 global attention을 수행하도록 함.</li>\n<li>전체 토큰 수에 비해서는 스페셜 토큰은 매우 적기 때문에 복잡도는 여전히 O(n)</li>\n</ul>\n<h3>Linear Projections for Global Attention</h3>\n<ul>\n<li>보통의 트랜스포머의 어텐션은 Q, K, V로 이루어 지는데, sliding window 기반 어텐션과 global 어텐션을 위해 sliding Q, K, V와 global Q, K, V 두 세트로 나눠서 어텐션을 계산하도록 구현</li>\n</ul>\n<h2>Experiments</h2>\n<p>2가지 방식으로 평가를 진행.</p>\n<h3>Autoregressive Language Modeling</h3>\n<ul>\n<li>모델 자체의 임베딩 평가를 위함</li>\n<li>character/token 단위의 language modeling을 수행.</li>\n<li><code class=\"language-text\">text8</code>, <code class=\"language-text\">enwik8</code> 데이터셋에서 SOTA를 달성</li>\n<li>본 태스크는 dilated sliding window attention 사용</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/table_2_3.png\">\n<h3>Pre-training and Fine-tuning</h3>\n<ul>\n<li>RoBERTa 체크포인트로부터 시작해서 학습</li>\n<li>sliding window attention를 사용</li>\n<li>각 태스크에 따라 스페셜 토큰을 지정하여 global attention을 사용</li>\n<li><code class=\"language-text\">WikiHop</code>과 <code class=\"language-text\">TriviaQA</code>에서 SOTA 달성</li>\n</ul>\n<img src=\"https://haebinshin.github.io/public/img/longformer/table8.png\">","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Longformer: The Long-Document Transformer"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/2004.05150"},"children":[{"type":"text","value":"Paper"}]},{"type":"text","value":" "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/allenai/longformer"},"children":[{"type":"text","value":"Code"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Iz Beltagy et al."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Introduction"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이유는 시퀀스 길이에 O(n^2)하게 늘어나는 높은 복잡도 때문"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"본 논문은 Attention 이루어지는 복잡도를 낮추는 방법을 제안 (O(n))"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT는 512 토큰을 limit으로 가지는데, 본 논문 모델은 4096 토큰까지 가능"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"text8"}]},{"type":"text","value":", "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"enwik8"}]},{"type":"text","value":", "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"Wiki-Hop"}]},{"type":"text","value":", "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"TriviaQA"}]},{"type":"text","value":"에서 State-Of-The-Art 달성"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Attention Method"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/figure2.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 위 그림과 같은 3가지 어텐션 방식을 제안"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Sliding window Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"크기가 w인 sliding window 내에서만 attention을 수행하는 방법"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이 방법은 텍스트 길이 n에 대해 O(n x w)의 복잡도를 가짐"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이러한 방식은 레이어가 깊어짐에 따라 receptive field가 넓어지는 CNN과 유사함"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/receptive_field.png"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"예) window size가 2일 때, 레이어가 쌓일수록 w만큼 receptive field가 넓어짐."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/text_sliding_window_receptive_field.jpg"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"l x w의 receptive field size를 가지게 됨."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"각 레이어마다 w의 크기를 다르게 주는 방법이 도움이 될 수도 있음"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Dilated Sliding Window"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Sliding window attention보다도 receptive field를 더 넓히기 위해 고안된 방법"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Dilated Convolution에서 착안"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/dilation_convolution.gif"},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"dilated 값을 줘서 토큰을 d만큼 건너뛰면서 어텐션하도록 하는 방법"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"예) window size가 2이고 dilation size가 2일 때, 아래 그림과 같이 w x d만큼 receptive field가 넓어짐"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/text_dilated_sliding_window_receptive_field.jpg"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"l x d x w의 receptive field size를 가지게 됨."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Global Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT의 [CLS] 토큰 같은 경우는 전체 컨텍스트를 바라봐야하는데, 위의 2가지 방법만으로는 Finetuning하는 태스크에서는 부족한 부분이 있을 수 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"따라서 스페셜 토큰 몇 개에 대해서는 global attention을 수행하도록 함."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"전체 토큰 수에 비해서는 스페셜 토큰은 매우 적기 때문에 복잡도는 여전히 O(n)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Linear Projections for Global Attention"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"보통의 트랜스포머의 어텐션은 Q, K, V로 이루어 지는데, sliding window 기반 어텐션과 global 어텐션을 위해 sliding Q, K, V와 global Q, K, V 두 세트로 나눠서 어텐션을 계산하도록 구현"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Experiments"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"2가지 방식으로 평가를 진행."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Autoregressive Language Modeling"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"모델 자체의 임베딩 평가를 위함"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"character/token 단위의 language modeling을 수행."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"text8"}]},{"type":"text","value":", "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"enwik8"}]},{"type":"text","value":" 데이터셋에서 SOTA를 달성"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"본 태스크는 dilated sliding window attention 사용"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/table_2_3.png"},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"text","value":"Pre-training and Fine-tuning"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"RoBERTa 체크포인트로부터 시작해서 학습"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"sliding window attention를 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"각 태스크에 따라 스페셜 토큰을 지정하여 global attention을 사용"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"WikiHop"}]},{"type":"text","value":"과 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"TriviaQA"}]},{"type":"text","value":"에서 SOTA 달성"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://haebinshin.github.io/public/img/longformer/table8.png"},"children":[]}],"data":{"quirksMode":false}},"excerpt":"Longformer: The Long-Document Transformer Paper Code Iz Beltagy et al. Introduction 트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음 이유는 시퀀스 길이에 O(n^…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"Longformer Paper Review","userDate":"6 February 2021","date":"2021-02-06T23:46:37.121Z","tags":["nlp","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ef32af94bd92c05cbfebe0cb00c3966e/597e6/longformer.png","srcSet":"/static/ef32af94bd92c05cbfebe0cb00c3966e/597e6/longformer.png 512w","sizes":"100vw"},"sources":[{"srcSet":"/static/ef32af94bd92c05cbfebe0cb00c3966e/3d2a6/longformer.webp 512w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.8046875000000001}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":10,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"8dee7e19-fa95-58ad-8a80-1e44402376ac","excerpt":"Tokenization 문장에서 의미있는 단위로 나누는 작업을 라고 한다. 문자 단위 토큰화 문자 단위로 토큰화를 하는 것이다. 한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 1…","frontmatter":{"title":"Tokenizer","date":"2021-09-13T23:46:37.121Z"},"fields":{"readingTime":{"text":"10 min read"},"slug":"/tokenizer/"}}},{"node":{"id":"21d42035-7177-5790-b983-c664ade00b2c","excerpt":"정규 표현식 정규표현식(regular expression)은 일종의 문자를 표현하는 공식으로, 특정 규칙이 있는 문자열 집합을 추출할 때 자주 사용되는 기법입니다. 주로 Prograaming Language나 Text Editor…","frontmatter":{"title":"정규표현식 (regex)","date":"2021-09-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"30 min read"},"slug":"/regex/"}}},{"node":{"id":"1203ae76-632c-525e-b502-707b3e57a736","excerpt":"최근 NLP 토크나이저를 만드는데 가장 많이 사용되는  라이브러와 실제 사용이 가장 많이 되는  라이브러리로의 변환에 대한 코드를 담고 있습니다. 해당 내용은  버젼에서 수행되었습니다. Train 아래 코드는 wordpiece, char-bpe…","frontmatter":{"title":"Hugging Face Tokenizers","date":"2021-08-11T15:11:55.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/tokenizers/"}}},{"node":{"id":"4a4289a0-53fc-562e-925a-097b91752eec","excerpt":"GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…","frontmatter":{"title":"P-Tuning Paper Review","date":"2021-05-13T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/p_tuning/"}}}]}},"pageContext":{"slug":"/longformer/","prev":{"excerpt":"Computer Architecture Review 오랜만에 컴퓨터 구조에서 배운 내용을 조금 복습해보며 감을 잡기 위함 컴퓨터가 코드를 처리하는 과정 Read Code Assembly 변환 CPU에서 실행 CPU에서 하나의 명령(Ex) Add…","frontmatter":{"title":"Computer Architecture Review","tags":["cs"],"date":"2021-02-05T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAACG0lEQVQ4y21UiZajMAzr///idnu3lKtcgUAS0Ehp6XRml/f8gkkiy7LNZlkW6LHjiKIoonnvYYzBbrfDX5reh2GI/uVygbUWWZYjy3N0nYnnc74nSYIN/vMs80wL8M7BT5M+wHF92ohAgM/Hv3zH85vOB+SmR9Ya5IymNW071KODCUu0ZvKo7Ih+RvQrO6F1AQ2Nbgwwk8TEgJuMm33fwzP1jmvTEaxt0RNAYgR+9zw8Og8XQvSdGOl8mGHnBXMgIP0IWIxMY7SRclmW1KTDnVo0TQNL3QYG6alhS7+p67hveMcTqCbDfwBzK8AxAtavCyN9/6FTILPT6RT3k/sdj56ByM74GTOeKS/fDB2Gro1pCEwb0mPVRKYAaZrGasvn5qt6Syyg/wR8MFLKVvlzPOF8PuPxeMQWORyP2O/3aKlnlmWxnXoWT6taxlCKge0jMJ1/V9lTeUMG0kqpqUBis9tucWEAgVVVhSMDqNcUUO9aZfqmIJJDwX/0oWircXVhf0twJKACqViHwwHX6zWmr8bWqkACk18QeJsV2Ajkt4n6Li9jKtOrQLfbs/LSaQXSpEjrkkytMjP2m+EKFrUgwCHNYFgsMUtuN3RM5z0ZDGiH/t1WCqQCFcP4BFyB1tVTy7S3sLxY1uw/FkO959hzExkNGkkelQX9Bzhtav7kk+EnoIpjOBnr6HXst4qAmoznu0NLENm6p1GsyPAL0HM88+GytqUAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/83debf50782f47536b48f20a7c0c3d46/ac5ac/computer-architecture.png","srcSet":"/static/83debf50782f47536b48f20a7c0c3d46/1d01f/computer-architecture.png 750w,\n/static/83debf50782f47536b48f20a7c0c3d46/ac5ac/computer-architecture.png 798w","sizes":"100vw"},"sources":[{"srcSet":"/static/83debf50782f47536b48f20a7c0c3d46/96ac1/computer-architecture.webp 750w,\n/static/83debf50782f47536b48f20a7c0c3d46/2afea/computer-architecture.webp 798w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.7017543859649122}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"2 min read"},"layout":"","slug":"/computer-ar-review/"}},"next":{"excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","tags":["speech","paper"],"date":"2021-03-17T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAACNElEQVQoz11TXW/TMBTN/38ExG/ggdfRFwRCg2mdhLq2Wwct2lraJO7yHcdxbMc5XCdLV3Glq/jz+J6Tcz1jDDjnqCpBWaHrOozhxm5/TLcvhDh9G6VQtx2lhaBs6bznLgrB8fzMaGRPQC6apsF+vwcLQ1hLF9r2lB3Ni1riW8Tx2U/wlb6JsfCUtvCDFNsdQ5rVcFgjYNtZ5GWJWsrhoZfHxjRUVaJbhJVE1BgoSxXyQuLX4oCH+V/s/kRUyQBmXRUEpEkOS/QsjTu3NlDAqzB01uh+zYXXFgXy5QLR7Qxis4aii464JTr57Rz7y0uEV1fgqxWaLO+BHP2RRddLVqO1g1xeqEpMohUuDnN851to2w506xqIcugggX3OgTh3SPg/ajoXBAFKksY94jUqwDGZwD9OkJWXdEefADX7AX74BOFfwMRLxD7D/c0NltdTPN1TxcQmyzL4vo8kSSBp7mminN6tEM4WEI/bXicXWjRIl78RTGdgNzPk6x2qgiOiaoLdDmWSQmvd24cx1lvPzb1yIzF/v8X03QMeP8SwatBGVRqPHxmu39zh59s1/C8JlFHg5EGXmnzpAJw/rR3tRho2kjzob8D2axTpofdXT5nsIBOJMuAQjEycNye72DMtj+yIkHzK0wKtoAotiFr1hDBZQxgGZ4izZoFUNcyLruemV9QlrmPSNEUcx6jopxinoZQNeClQ5KSBeu0UV4VWGgVpXBZ0WJuhQ172XBeNLegARw3/AdzDmImYteqsAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/7ca66667228d877c1b0d96e85b974435/ee3dd/pushing.png","srcSet":"/static/7ca66667228d877c1b0d96e85b974435/6a16f/pushing.png 750w,\n/static/7ca66667228d877c1b0d96e85b974435/ee3dd/pushing.png 928w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ca66667228d877c1b0d96e85b974435/e0c95/pushing.webp 750w,\n/static/7ca66667228d877c1b0d96e85b974435/86029/pushing.webp 928w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6142241379310345}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"3 min read"},"layout":"","slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}},"primaryTag":"nlp"}},
    "staticQueryHashes": ["3170763342","3229353822"]}