{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/las/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>「Listen, Attend and Spell」 Review</h1>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMTY4/MDAxNTgyMTI0ODQ4MTQ4.atLsczYj39_WahxOcLp2eQAvAwbO_uFY3s57CwVoKwUg.z87Ok0avCEU6v0C7L-ymtGjvjQNcgLwsrs-nVi2p4rwg.PNG.sooftware/image.png?type=w773\" alt=\"title\"></p>\n<p><a href=\"https://arxiv.org/abs/1508.01211\">https://arxiv.org/abs/1508.01211</a>  (William Chan et al. 2015)</p>\n<h1></h1>\n<h2><strong>Introduction</strong></h2>\n<p>어텐션 기반 Seq2seq 구조를 음성 인식에 적용한 논문이다.</p>\n<p>당시에는 CTC (Connectionist temporal classification) 이 음성 인식 분야를 점유하고 있던 시절이였던 터라, End-to-End 방식으로 본 논문 모델과 같은 성능을 낸 것은 굉장히 혁명적인 일이였다고 한다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMjQ2/MDAxNTgyMTI1Mjk5Njg0.9S4lvnYbptl5VZvmn2ju2k0Rlq0bjaebYS_oRGdyLfEg.1rqJhW8PZ_noH55AS2RCsDVYXWmDO3qwMl9Q41ZXqlsg.PNG.sooftware/image.png?type=w773\" alt=\"end-to-end\"></p>\n<p>본 논문 이후 Speech 분야는 CTC와 LAS로 나뉜다고 한다.</p>\n<h1></h1>\n<h2><strong>Model</strong></h2>\n<p>모델의 전체적인 구조는 Listener (encoder) 와 Speller (decoder) 로 이루어져 있다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfNDEg/MDAxNTgyMTI1MDY0MjAz.I1hSxVSWm_YNA1c9EFqe6jetEsPnEePULmJqPWOi3tkg.P_qGbmj96CmKp6RFaCvY5qhu_c7mu8jkbC4s9_UYycog.PNG.sooftware/image.png?type=w773\" alt=\"las_model\"></p>\n<h3><strong>Listener</strong></h3>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfNDcg/MDAxNTgyMTI1NTIyMDI1.g0Hl_gv9eMqMtPNCJ7V3FFgAB4Ki0NEdm9bD3WB5ZOEg.SvRIOEMLXCR-0wikkqIl0J4yHehtYyHodr0PyntyLMwg.PNG.sooftware/image.png?type=w773\" alt=\"listener\"></p>\n<p>데이터의 피쳐를 입력받는 Encoder<br>\n입력 시퀀스 x를 High level feature인 h로 변형하는 역할을 담당한다<br>\n(더 의미있는 시퀀스로 변형한다)</p>\n<h3><strong>Speller</strong></h3>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMjYw/MDAxNTgyMTI1NTI0OTg2.pg_moBT5kOqM9OsZGVlsJlTbcnm4S_yxICgbjhYu7qcg.rbNb4kqRfZT5d1UbQ5sgq9YXGezYai4pN4O4W1MNrCcg.PNG.sooftware/image.png?type=w773\" alt=\"speller\"></p>\n<p>리스너가 변형한 High Level feature인 h를 어텐션을 사용하여 문자로 출력한다. (Decoder)</p>\n<h1></h1>\n<h3><strong>pBLSTM</strong></h3>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMTkx/MDAxNTgyMTI1ODA1ODkw.8GOtCZNfdDXZuTO46FXUnHf1Fxis7zv-CSoUT6pCGU8g.mIZEx-VA4qEG0ELa0AzRZQ16vJkBvy1f6Cp8QiH9UnIg.PNG.sooftware/image.png?type=w773\" alt=\"pBLSTM\"></p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMTE3/MDAxNTgyMTI1NTMwNzg1.TIns05VCjY4QXexRHEg5s4os_ELoGylFCgIHCBdGMI0g.tBiGMeTPxS60YofPiqMItgvdwzOLu6jt__Y_E8BPq1Ig.PNG.sooftware/image.png?type=w773\" alt=\"pblstm_math\"></p>\n<p>모델의 첫 번째 특징으로는 Pyrimidal Bidirectional LSTM을 사용했다.<br>\n이전 레이어의 2i, 2i+1 번째 시퀀스를 Concatenate하여 다음 레이어의 i번째 RNN 셀의 입력으로 넣는 구조이다.</p>\n<p>상대적으로 매우 긴 시퀀스 길이를 가지는 Speech 모델의 단점을 완화 해주는 역할을 한다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMjUx/MDAxNTgyMTI2MDA1NDQy.gdw6LQR1ZZlEm8C5bl9SMqNSiaH8LrK-mU5N9yB1GPQg.uthp2jVUQdBFX5rRLDGoPLyQOp1RpZurRyeulPbMUCog.PNG.sooftware/image.png?type=w773\" alt=\"blstm_pblstm\"></p>\n<p>위의 그림처럼 기존 BLSTM 구조에 반해, 시퀀스의 길이가 상당히 줄어드는 것을 확인할 수 있다.</p>\n<p>pBLSTM 레이어 1층 당 시퀀스 길이가 반씩 줄어들게 되는데, 본 논문에서는 이러한 레이어를 3개를 둠으로써, 총 시퀀스 길이를 8 분의 1로 줄였다고 한다.</p>\n<p>이러한 시퀀스 길이의 감소는 디코딩 &#x26; 어텐션 과정에서 연산량 감소를 가능하게한다.</p>\n<h1></h1>\n<h3><strong>Exposure Bias Problem</strong></h3>\n<p>Seq2seq에서 티쳐 포싱이라는 개념이 있다.<br>\n티쳐 포싱의 개념은 <a href=\"https://blog.naver.com/sooftware/221790750668\">이글</a>을 참고하기를 바랍니다.</p>\n<p>당시에는 티쳐 포싱은 Seq2seq 아키텍쳐에서 디폴트 100%로 사용됐던 것 같다.<br>\n티쳐 포싱은 학습을 빠르게 해준다는 장점이 있지만, Exposure Bias Problem이란게 존재한다.</p>\n<p>티쳐 포싱은 학습 시에 레이블을 제공받지만, 실제 추론 시에는 레이블을 제공 받을 수가 없다.<br>\n이러한 차이점이 실제 추론과 학습시의 성능에 차이가 있을 수 있다는 점이다.<br>\n이를 Exposure Bias Problem이라고 한다.</p>\n<p>본 논문에서는 이러한 문제점을 완화 및 실험해보기 위하여 티쳐 포싱 비율이 100%인 모델과 90%인 모델 2개를 학습시켰다고 한다.</p>\n<p>( 2019년 5월에 나온 「Exposure Bias for Neural Language Generation」논문에서는 이런 노출 편향 문제가 생각만큼 큰 영향을 미치지는 않는다는 연구 결과를 냈다고 한다 )</p>\n<h1></h1>\n<h3><strong>Decoding</strong></h3>\n<p>본 논문에서는 역시 빔서치를 사용했다고 한다. (빔사이즈 = 32)<br>\n빔서치에 대한 설명은 <a href=\"https://blog.naver.com/sooftware/221809101199\">이글</a>을 참고하길 바랍니다.</p>\n<p>본 논문에서 설명하기를, 기존 음성 인식 모델들은 모든 빔이 <eos>를 만나고 나면, 가장 높은 점수를 받은 빔을 선택한 후, Dictionary (사전) 을 통해 언어 교정을 하는 방식을 많이 사용했다고 한다.</p>\n<p>하지만, 본 논문에서 실험시에, 이 DIctionary 방식은 별로 필요가 없다고 주장한다.</p>\n<p>본 논문의 모델로 실험해본 결과, 어느 정도 학습 후에는 거의 항상 단어 단위에서는 완벽한 단어를 내놓기 때문에, 이러한 교정 과정이 필요가 없다는 것이다.</p>\n<h3><strong>Rescoring</strong></h3>\n<p>그래서 본 논문에서는 Dictionary 방식이 아닌 Rescoring 방식을 제안한다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMjg5/MDAxNTgyMTI3MDQ3Mzg0.IwmLo-utBNEc-BMIkuECTa6OCU2YwCAN5pLszMvmsYEg.aiJDdfZk9tLz-MokkndcGfBqOXdjnk8T4fXx7oU5ws4g.PNG.sooftware/image.png?type=w773\" alt=\"rescoring\"></p>\n<p>빔 사이즈 만큼의 후보를 뽑아놓은 후에, 여기에 Language Model을 이용하여 점수를 매긴 뒤, 기존 점수와 LM에서 나온 점수를 적절히 결합하여 새로 점수를 매기는 것이다.<br>\n해서, 최종적으로 가장 높은 점수를 받은 빔을 최종 선택지로 사용하는 것이다.</p>\n<h1></h1>\n<h2><strong>Experiments</strong></h2>\n<p>본 논문에서 진행한 실험 환경은 아래와 같다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMTA3/MDAxNTgyMTI3MjE3ODEx.gsza0dxPeS7OsNdTBGjqqmSpYuIAlDISegj6U2khtiIg.bIp8Ru57DtM58V7q1W3ESzltTbSogWSMnQZyM_SKo5cg.PNG.sooftware/image.png?type=w773\" alt=\"experiment_environ\"></p>\n<p>위의 환경에서 진행한 실험 결과는 아래와 같다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMzAg/MDAxNTgyMTI3MjI3ODA3.FF88MejgiXadN5VqA68HiSFx8NJ6dQ0RtblcaN6M2uUg.LShKDPYxbrbwcPOKoK32JCvbV1_CHnkJmjyaO-vNWH0g.PNG.sooftware/image.png?type=w773\" alt=\"result\"></p>\n<p>Language Model을 적용하기 전에는 노이즈가 없는 환경에서는 14.1%의 WER (Word Error Rate), 노이즈가 있는 환경에서는 16.5%의 WER을 기록했다.</p>\n<p>결과를 보면, 100%의 티쳐 포싱 비율을 가진 모델보다, 90%의 티쳐 포싱 비율을 가진 모델이 더 좋은 결과를 기록한 것을 알 수 있다.</p>\n<p>그리고 본 논문에서 제안한 <strong>Beam Search + Language Model</strong>의 퍼포먼스는 매우 훌륭했다.</p>\n<p>모든 면에서 약 4%의 성능을 개선한 것을 확인할 수 있다.<br>\n인식률이 85%가 넘어가는 상황에서의 4%는 엄청난 발전이라고 볼 수 있다.</p>\n<p>본 논문은 위의 결과를 통해 새로 제안한 Rescoring 방식이 의미 있는 결과를 냈다는 것을 검증했다.</p>\n<p><img src=\"https://postfiles.pstatic.net/MjAyMDAyMjBfMTA5/MDAxNTgyMTI3MjM3NDYx.LR6q0OJM0HSqOotAz38FBPg6rTssjBg_rcEkXqQYrGIg.VGG1b4X6-RQdxp0JhZ-gFb7-Ge7fe4UzwFyPiz4ftbIg.PNG.sooftware/image.png?type=w773\" alt=\"sota\"></p>\n<p>또한, 본 논문의 모델은 당시 SOTA (State-Of-The-Art) 모델인 CLDNN-HMM 모델과 비교하여 2.3% WER 정도만의 차이를 기록했다.<br>\nCTC를 사용하지 않고도 이 정도의 성능을 낼 수 있다는 것을 보여준 셈이다.</p>\n<p>본 논문에서는 SOTA 모델과 자신들의 모델의 차이를 Convolution filter의 유무에 의해 생겼다고 추정하고 있다.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"「Listen, Attend and Spell」 Review"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMTY4/MDAxNTgyMTI0ODQ4MTQ4.atLsczYj39_WahxOcLp2eQAvAwbO_uFY3s57CwVoKwUg.z87Ok0avCEU6v0C7L-ymtGjvjQNcgLwsrs-nVi2p4rwg.PNG.sooftware/image.png?type=w773","alt":"title"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/1508.01211"},"children":[{"type":"text","value":"https://arxiv.org/abs/1508.01211"}]},{"type":"text","value":"  (William Chan et al. 2015)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Introduction"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"어텐션 기반 Seq2seq 구조를 음성 인식에 적용한 논문이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"당시에는 CTC (Connectionist temporal classification) 이 음성 인식 분야를 점유하고 있던 시절이였던 터라, End-to-End 방식으로 본 논문 모델과 같은 성능을 낸 것은 굉장히 혁명적인 일이였다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMjQ2/MDAxNTgyMTI1Mjk5Njg0.9S4lvnYbptl5VZvmn2ju2k0Rlq0bjaebYS_oRGdyLfEg.1rqJhW8PZ_noH55AS2RCsDVYXWmDO3qwMl9Q41ZXqlsg.PNG.sooftware/image.png?type=w773","alt":"end-to-end"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문 이후 Speech 분야는 CTC와 LAS로 나뉜다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Model"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델의 전체적인 구조는 Listener (encoder) 와 Speller (decoder) 로 이루어져 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfNDEg/MDAxNTgyMTI1MDY0MjAz.I1hSxVSWm_YNA1c9EFqe6jetEsPnEePULmJqPWOi3tkg.P_qGbmj96CmKp6RFaCvY5qhu_c7mu8jkbC4s9_UYycog.PNG.sooftware/image.png?type=w773","alt":"las_model"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Listener"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfNDcg/MDAxNTgyMTI1NTIyMDI1.g0Hl_gv9eMqMtPNCJ7V3FFgAB4Ki0NEdm9bD3WB5ZOEg.SvRIOEMLXCR-0wikkqIl0J4yHehtYyHodr0PyntyLMwg.PNG.sooftware/image.png?type=w773","alt":"listener"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"데이터의 피쳐를 입력받는 Encoder"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n입력 시퀀스 x를 High level feature인 h로 변형하는 역할을 담당한다"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n(더 의미있는 시퀀스로 변형한다)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Speller"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMjYw/MDAxNTgyMTI1NTI0OTg2.pg_moBT5kOqM9OsZGVlsJlTbcnm4S_yxICgbjhYu7qcg.rbNb4kqRfZT5d1UbQ5sgq9YXGezYai4pN4O4W1MNrCcg.PNG.sooftware/image.png?type=w773","alt":"speller"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"리스너가 변형한 High Level feature인 h를 어텐션을 사용하여 문자로 출력한다. (Decoder)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"pBLSTM"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMTkx/MDAxNTgyMTI1ODA1ODkw.8GOtCZNfdDXZuTO46FXUnHf1Fxis7zv-CSoUT6pCGU8g.mIZEx-VA4qEG0ELa0AzRZQ16vJkBvy1f6Cp8QiH9UnIg.PNG.sooftware/image.png?type=w773","alt":"pBLSTM"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMTE3/MDAxNTgyMTI1NTMwNzg1.TIns05VCjY4QXexRHEg5s4os_ELoGylFCgIHCBdGMI0g.tBiGMeTPxS60YofPiqMItgvdwzOLu6jt__Y_E8BPq1Ig.PNG.sooftware/image.png?type=w773","alt":"pblstm_math"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모델의 첫 번째 특징으로는 Pyrimidal Bidirectional LSTM을 사용했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이전 레이어의 2i, 2i+1 번째 시퀀스를 Concatenate하여 다음 레이어의 i번째 RNN 셀의 입력으로 넣는 구조이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"상대적으로 매우 긴 시퀀스 길이를 가지는 Speech 모델의 단점을 완화 해주는 역할을 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMjUx/MDAxNTgyMTI2MDA1NDQy.gdw6LQR1ZZlEm8C5bl9SMqNSiaH8LrK-mU5N9yB1GPQg.uthp2jVUQdBFX5rRLDGoPLyQOp1RpZurRyeulPbMUCog.PNG.sooftware/image.png?type=w773","alt":"blstm_pblstm"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 그림처럼 기존 BLSTM 구조에 반해, 시퀀스의 길이가 상당히 줄어드는 것을 확인할 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"pBLSTM 레이어 1층 당 시퀀스 길이가 반씩 줄어들게 되는데, 본 논문에서는 이러한 레이어를 3개를 둠으로써, 총 시퀀스 길이를 8 분의 1로 줄였다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"이러한 시퀀스 길이의 감소는 디코딩 & 어텐션 과정에서 연산량 감소를 가능하게한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Exposure Bias Problem"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Seq2seq에서 티쳐 포싱이라는 개념이 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n티쳐 포싱의 개념은 "},{"type":"element","tagName":"a","properties":{"href":"https://blog.naver.com/sooftware/221790750668"},"children":[{"type":"text","value":"이글"}]},{"type":"text","value":"을 참고하기를 바랍니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"당시에는 티쳐 포싱은 Seq2seq 아키텍쳐에서 디폴트 100%로 사용됐던 것 같다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n티쳐 포싱은 학습을 빠르게 해준다는 장점이 있지만, Exposure Bias Problem이란게 존재한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"티쳐 포싱은 학습 시에 레이블을 제공받지만, 실제 추론 시에는 레이블을 제공 받을 수가 없다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이러한 차이점이 실제 추론과 학습시의 성능에 차이가 있을 수 있다는 점이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n이를 Exposure Bias Problem이라고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 이러한 문제점을 완화 및 실험해보기 위하여 티쳐 포싱 비율이 100%인 모델과 90%인 모델 2개를 학습시켰다고 한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"( 2019년 5월에 나온 「Exposure Bias for Neural Language Generation」논문에서는 이런 노출 편향 문제가 생각만큼 큰 영향을 미치지는 않는다는 연구 결과를 냈다고 한다 )"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Decoding"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 역시 빔서치를 사용했다고 한다. (빔사이즈 = 32)"},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n빔서치에 대한 설명은 "},{"type":"element","tagName":"a","properties":{"href":"https://blog.naver.com/sooftware/221809101199"},"children":[{"type":"text","value":"이글"}]},{"type":"text","value":"을 참고하길 바랍니다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 설명하기를, 기존 음성 인식 모델들은 모든 빔이 "},{"type":"element","tagName":"eos","properties":{},"children":[{"type":"text","value":"를 만나고 나면, 가장 높은 점수를 받은 빔을 선택한 후, Dictionary (사전) 을 통해 언어 교정을 하는 방식을 많이 사용했다고 한다."}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"하지만, 본 논문에서 실험시에, 이 DIctionary 방식은 별로 필요가 없다고 주장한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문의 모델로 실험해본 결과, 어느 정도 학습 후에는 거의 항상 단어 단위에서는 완벽한 단어를 내놓기 때문에, 이러한 교정 과정이 필요가 없다는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h3","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Rescoring"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그래서 본 논문에서는 Dictionary 방식이 아닌 Rescoring 방식을 제안한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMjg5/MDAxNTgyMTI3MDQ3Mzg0.IwmLo-utBNEc-BMIkuECTa6OCU2YwCAN5pLszMvmsYEg.aiJDdfZk9tLz-MokkndcGfBqOXdjnk8T4fXx7oU5ws4g.PNG.sooftware/image.png?type=w773","alt":"rescoring"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"빔 사이즈 만큼의 후보를 뽑아놓은 후에, 여기에 Language Model을 이용하여 점수를 매긴 뒤, 기존 점수와 LM에서 나온 점수를 적절히 결합하여 새로 점수를 매기는 것이다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n해서, 최종적으로 가장 높은 점수를 받은 빔을 최종 선택지로 사용하는 것이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h1","properties":{},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Experiments"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서 진행한 실험 환경은 아래와 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMTA3/MDAxNTgyMTI3MjE3ODEx.gsza0dxPeS7OsNdTBGjqqmSpYuIAlDISegj6U2khtiIg.bIp8Ru57DtM58V7q1W3ESzltTbSogWSMnQZyM_SKo5cg.PNG.sooftware/image.png?type=w773","alt":"experiment_environ"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"위의 환경에서 진행한 실험 결과는 아래와 같다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMzAg/MDAxNTgyMTI3MjI3ODA3.FF88MejgiXadN5VqA68HiSFx8NJ6dQ0RtblcaN6M2uUg.LShKDPYxbrbwcPOKoK32JCvbV1_CHnkJmjyaO-vNWH0g.PNG.sooftware/image.png?type=w773","alt":"result"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Language Model을 적용하기 전에는 노이즈가 없는 환경에서는 14.1%의 WER (Word Error Rate), 노이즈가 있는 환경에서는 16.5%의 WER을 기록했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"결과를 보면, 100%의 티쳐 포싱 비율을 가진 모델보다, 90%의 티쳐 포싱 비율을 가진 모델이 더 좋은 결과를 기록한 것을 알 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"그리고 본 논문에서 제안한 "},{"type":"element","tagName":"strong","properties":{},"children":[{"type":"text","value":"Beam Search + Language Model"}]},{"type":"text","value":"의 퍼포먼스는 매우 훌륭했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"모든 면에서 약 4%의 성능을 개선한 것을 확인할 수 있다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\n인식률이 85%가 넘어가는 상황에서의 4%는 엄청난 발전이라고 볼 수 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문은 위의 결과를 통해 새로 제안한 Rescoring 방식이 의미 있는 결과를 냈다는 것을 검증했다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"https://postfiles.pstatic.net/MjAyMDAyMjBfMTA5/MDAxNTgyMTI3MjM3NDYx.LR6q0OJM0HSqOotAz38FBPg6rTssjBg_rcEkXqQYrGIg.VGG1b4X6-RQdxp0JhZ-gFb7-Ge7fe4UzwFyPiz4ftbIg.PNG.sooftware/image.png?type=w773","alt":"sota"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"또한, 본 논문의 모델은 당시 SOTA (State-Of-The-Art) 모델인 CLDNN-HMM 모델과 비교하여 2.3% WER 정도만의 차이를 기록했다."},{"type":"element","tagName":"br","properties":{},"children":[]},{"type":"text","value":"\nCTC를 사용하지 않고도 이 정도의 성능을 낼 수 있다는 것을 보여준 셈이다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"본 논문에서는 SOTA 모델과 자신들의 모델의 차이를 Convolution filter의 유무에 의해 생겼다고 추정하고 있다."}]}],"data":{"quirksMode":false}},"excerpt":"「Listen, Attend and Spell」 Review title https://arxiv.org/abs/1508.01211  (William Chan et al. 2015)  Introduction 어텐션 기반 Seq2seq…","fields":{"readingTime":{"text":"8 min read"}},"frontmatter":{"title":"Listen, Attend and Spell Paper Review","userDate":"20 September 2019","date":"2019-09-20T10:00:00.000Z","tags":["speech","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png","srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/61a36/las.png 625w","sizes":"100vw"},"sources":[{"srcSet":"/static/c6dd3a7d5b5935928a2ffb65755ccaf6/09f70/las.webp 625w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.192}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f2f95a99-ae13-5b3f-9375-508975c97e83","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"19ded62e-3e91-5733-9329-a1c7bdcf859b","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"35eadfc7-646b-5194-a711-ce20e840ba58","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"fd3185b8-63e4-5e3d-aeb3-e67ed1343af9","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"aad087b1-4b0f-5956-ab2f-d7ab33fdb8c4","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/las/","prev":{"excerpt":"MFCC (Mel-Frequency Cepstral Coefficient) ‘Voice Recognition Using MFCC Algorithm’ 논문 참고 MFCC란? 음성인식에서 MFCC, Mel-Spectrogram…","frontmatter":{"title":"MFCC (Mel-Frequency Cepstral Coefficient)","tags":["dsp","speech"],"date":"2019-06-18T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC0klEQVQ4y4VVTW/TQBD1b+AOgksR4sJ/KFIvFRJHfgRXuKCqBy6lfFyoQByo1EpIiAsUCSFUqQLa0tKEj7YOiJAmcRqXfNiO7cR2vHYeOxs7bNJUrLQa7+7Mm3lvNhul1+uBJo3Qc+EeHQKdNnpxLPbSc9nvpDUNpZds0KhZNn5UamhYDtp+iFbAjgXE0jek2AGgcIr7i41iHa+/l7Gi6iiaHl78MsV+vdNFwCIEYdRnEsU4dHxhRxkocvaFzRJurexjZvU3DkwfMxtVME794c4RVksWnufq8BnDbs3G7Icy3pVaYygnH043wuSjDM7PruHi/DY+HpiYXNzHjbcFXH6yhyvLOVxa+IZrz35i4m4Gp25v48yDL/is2UNSDAFOPc5iggNeuLOFtbyB6UUV198UMPV0H9NLOZybz+DqssrtDk7PZ3H2XhYb4wBT0KWshpuv9jC7doCC4WHukw5+irktHeuVFt6XW6g6HnbrDu5zGdYrFhEe05RkoeomXn4tYbNQQ90NkGt2EPGGZY7cQWASDa3lC337DZWaIqP/4ddF1eoIPA8RYwMqSLuYwNJWFEsJpCs0RNlzbRiVMiLXhVx5elfDMES320UURaIysv3veKTLSfZmswm9UoFWLqPT6fy7Eok1DENMTdOQy+VQLBahqiry+bxYU7Ihysy2wTqeqISmfOb7Pmq1GnRdF6BBEIi9drstkpOlaocoh40GTF5hs1o9dmEZ17TBz4mF4zgioXxG4LTXB0weAkbOHNDilEYBadicAYGZpgmD+8ZJnMebSMkIVJG7ZJEjP7Atq39JRl4VAgi5TsImkoy+PgrxJv5ImlLldCnbuKeJLPkLy9iQxulQSFwSmuz/3j2qjPSiNWNs7Hup0AFVRp1Lu0Z23JArZCdVKC9I7PRakGMsflqxaJpYczDGtUsrHQsolzvQhwJ5gDxJM2oESSP8pL8IefwFb+bx9QWkOa8AAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/e9157dcdd01342e368f885b8937a5d91/3a1d5/mfcc.png","srcSet":"/static/e9157dcdd01342e368f885b8937a5d91/52f05/mfcc.png 750w,\n/static/e9157dcdd01342e368f885b8937a5d91/3a1d5/mfcc.png 760w","sizes":"100vw"},"sources":[{"srcSet":"/static/e9157dcdd01342e368f885b8937a5d91/b6018/mfcc.webp 750w,\n/static/e9157dcdd01342e368f885b8937a5d91/2f184/mfcc.webp 760w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1.0078947368421054}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"12 min read"},"layout":"","slug":"/mfcc/"}},"next":{"excerpt":"Deep Speech: Scaling up end-to-end speech recognition title https://arxiv.org/pdf/1412.5567.pdf (Awni Hannun et al. 2014) Abstract…","frontmatter":{"title":"DeepSpeech Paper Review","tags":["speech","paper"],"date":"2019-11-11T10:00:00.000Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAAAsTAAALEwEAmpwYAAADXElEQVQ4y2VU247aVhSdz8tH9C19ywe0ldqkfalaVUrVh7RREjWK0lRqlVSK1KRMmOEyzHA1UHcwnoFhBoOvXGzMxWAbs7qPwUyY2dLBPpx9ltdea5+zB4rVagXXdbFcLjEcDsNnFKZpwvM8RHnRYDGbzTAej7drLPbYDwPRdR29Xg+Hh4cYjUbh4qDfD+eWZe0AsgiWAaqVCsrl8s7aXsTC933aaCKRSEIUxTBpNJlg/+AAjUZjh0WwYc/XakgfH4dMo/W9+XwOSZIoK8CQGAr/8Tg5OcJCldEnoHouh0w6jcl0ynYgWPmAvwB6OprlCgrJJE4JeAvImPWpNFmWkctlIYhtxNNV/Fs4R+rvGGSxDq5YCmXwPB+GtYRuLJCNZXBK+c3TGsSzs12GTD+XhK+QHvzpOZpXOiwHyBY5XFxdQlGUcINPZk3of5tGQzJQKnOQut3dktkLc5UJPxgMEP+wD9s2wwSLtI3FYlsnEWoYDYQESsXiWtcguHZ5QuIzMAaaSqWgqtraFCozkUhA07QbbbN+r5F2hULhtsuO44QtY9s2ivRF5jqLKRmRz+dvtU3k9hlpJwjCbUDGkJXFANPkaMSIMUySi0zjm33IgoEVNyXvADImrLkZMMdxW4aMOdsQNfpNhs1mE/V6faPhR4BsA+szyyJGqSQ0fc3QDBs9QW2ib0wJwrHagPM8v2W4dXlbAiUu3Tk5a2E2deBSO7nE0KT5lObeYgGP8JYsnY7dilydO3PYIxsr38Nq6a8Bgw3ge17Dg/0mXmcKeFs5wJMCh2fVCt7xeRwV/8Hv9Spq5UeQnUf4biojzmUQr50gJpXwql3DT/yAqvSJ4fqk48e0hE9eXeLhyTG+ocQv8hXcFwU8bqbxy3kSP4g57BsPcG+cwd0LAb+ev8fDRh5ftUTcrbZx/8CkioKNhtYYn79O4NNnGXz77g2+rHD4OlvCcyGOp40jfC/y+EP4C386z3GnGOBN9i1+6xzixVUWP1/y+Kx2gZc547qxXddDS+lBMkaQ+wYuDQNtowfN7EE2h1DMAdpqG91xD53BDKo1gDa2oYys9bCp5Rz/GpBdENJVC5qqYEqtoylddOgGUhUVfQLWVTWcG3SCOu0Wup0uVLpMup0O2q0WXTzXJ+l/ass0dAhgXEIAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png","srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/2241d/deepspeech.png 541w","sizes":"100vw"},"sources":[{"srcSet":"/static/360f6a5bb171f9136086a6bf108b401d/1edf8/deepspeech.webp 541w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.9149722735674677}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"10 min read"},"layout":"","slug":"/deepspeech/"}},"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}