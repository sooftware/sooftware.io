{
    "componentChunkName": "component---src-templates-post-tsx",
    "path": "/Textledd NLP: Generating expressive speech from raw audio/",
    "result": {"data":{"logo":null,"markdownRemark":{"html":"<h1>Textless NLP: Generating expressive speech from raw audio</h1>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2102.01192\">paper</a> / <a href=\"https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp/gslm\">code / pre-train model</a> / <a href=\"https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio\">blog</a></li>\n<li>Name: Generative Spoken Language Model (GSLM)</li>\n</ul>\n<h2>Intro</h2>\n<ul>\n<li>BERT, RoBERTa, GPT-3 등 최근 몇 년간 <code class=\"language-text\">텍스트</code>에 집중된 NLP 모델들이 발전되어 왔음.</li>\n<li>이건 분명한 한계다. 텍스트에 대한 디펜던시를 깨야한다.</li>\n<li>언어 == 문자가 아니다. speech가 있다.</li>\n<li>그래서 우리 GSLM이 텍스트에 대한 디펜던시를 깰 수 있는 가능성을 보였다.</li>\n<li>음성 프롬프트 시대의 시작을 알린다.</li>\n<li>음성을 프롬프트로 주면 뒤이어서 인공지능이 말을 계속 이어서 말하는 모델의 등장!</li>\n</ul>\n<h2>Background</h2>\n<ul>\n<li>음성을 입력으로 하는 NLP 어플리케이션들은 ASR => NLP를 거쳐야 했음.</li>\n<li>ASR의 정확도가 100%가 아니기 때문에 분명한 정보의 오류가 존재함.</li>\n<li>우리는 여기서 ASR + NLP 구조가 아닌 Speech to Speech로 간다.</li>\n<li>Text나 label 없이 only 음성만으로 학습한다.</li>\n</ul>\n<h2>Textless NLP’s benefits</h2>\n<ul>\n<li>언어 상관없이 학습이 가능해질 가능성이 높아짐</li>\n<li>텍스트로 표현이 안되는 말의 뉘앙스, 감정 등의 정보를 반영할 수 있음</li>\n<li>텍스트 레이블링 혹은 ASR 학습 없이 모델을 학습할 수 있음</li>\n<li>유아들이 어떻게 언어를 배우고 말을 시작하는지를 알 수 있다(? 과연?)</li>\n<li>처음으로 텍스트 없이 audio to audio 번역 시스템이 가능해졌다!</li>\n</ul>\n<h2>Data</h2>\n<ul>\n<li>6,000시간의 Libri-Light와 LibriSpeech 데이터셋 (인코더 학습)</li>\n<li>LibriSpeech and LJSpeech (디코더(TTS System) 학습)</li>\n</ul>\n<h2>Model</h2>\n<img src=\"https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241347514_376678770759983_4717648868496710483_n.jpg?_nc_cat=100&amp;ccb=1-5&amp;_nc_sid=ad8a9d&amp;_nc_ohc=igZvOorsx80AX9BQc0l&amp;_nc_ht=scontent-gmp1-1.xx&amp;oh=896c0d14d6c8709f6c3d81963268db1d&amp;oe=6147AD53\" width=\"600\">  \n<ul>\n<li>Encoder (S2u)\n<ul>\n<li>Speech를 인풋으로 받아서 discrete unit(pseudo-text라고 부름)으로 인코딩</li>\n<li>unit은 k-means clustering으로 나눔.</li>\n<li>인코더로는 CPC, wav2vec 2.0, HuBERT를 사용 (좋은 acoustic encoder들이라고 보시면 됨)</li>\n</ul>\n</li>\n<li>uLM\n<ul>\n<li>unit sequence를 생성</li>\n</ul>\n</li>\n<li>Decoder (u2S)\n<ul>\n<li>TTS System (Tacotron2 사용)</li>\n</ul>\n</li>\n<li>여기서 unit(pseudo-text)은 letter or phoneme과 매핑되지는 않음.</li>\n<li>100 이상의 유닛일 때 좋은 성능을 보였으며 unit은 보통 음소보다 짧은 단위를 인코딩했음.</li>\n</ul>\n<img src=\"https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241223788_398469455180920_2630499539056655858_n.jpg?_nc_cat=107&amp;ccb=1-5&amp;_nc_sid=ad8a9d&amp;_nc_ohc=rfiDlgtmTcYAX-EraG5&amp;_nc_ht=scontent-gmp1-1.xx&amp;oh=1c96a38f6af0ada3774380e4fd6110e6&amp;oe=61489C23\" width=\"600\">\n<ul>\n<li>생성한 음성은 pre-trained ASR 모델로 인식해서 성능 측정</li>\n<li>Pre-trained LM으로 텍스트 성능 측정</li>\n</ul>\n<h2>Result</h2>\n<img src=\"https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241364732_225715579507676_6485051182702467200_n.jpg?_nc_cat=108&amp;ccb=1-5&amp;_nc_sid=ad8a9d&amp;_nc_ohc=h45PImsz8SkAX-kM1rz&amp;_nc_ht=scontent-gmp1-1.xx&amp;oh=88949e5b3a057a6e42b8266d03171ac7&amp;oe=61492788\" width=\"600\">\n<ul>\n<li>Unit의 수가 모델 성능에 큰 영향을 미침.</li>\n<li>Unit 수가 커질수록 Acoustic의 성능은 좋아졌음. (PER이 낮아졌다)</li>\n<li>LM 점수도 비슷한 경향이었으나, 너무 많은 unit을 사용하면 오히려 안 좋았음. (NLP에서 vocab의 적당한 사이즈가 좋은 이유와 비슷한 것 같음)</li>\n<li>어떤 인코더 모델이냐에 따라 다른 결과가 나옴. HuBERT 성능이 가장 좋았음.</li>\n<li>이렇게 자동으로 측정한 성능이 사람이 평가했을 때와 correlation이 높았음. (좋은 성능 지표)</li>\n</ul>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"h1","properties":{},"children":[{"type":"text","value":"Textless NLP: Generating expressive speech from raw audio"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"element","tagName":"a","properties":{"href":"https://arxiv.org/abs/2102.01192"},"children":[{"type":"text","value":"paper"}]},{"type":"text","value":" / "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp/gslm"},"children":[{"type":"text","value":"code / pre-train model"}]},{"type":"text","value":" / "},{"type":"element","tagName":"a","properties":{"href":"https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio"},"children":[{"type":"text","value":"blog"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Name: Generative Spoken Language Model (GSLM)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Intro"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"BERT, RoBERTa, GPT-3 등 최근 몇 년간 "},{"type":"element","tagName":"code","properties":{"className":["language-text"]},"children":[{"type":"text","value":"텍스트"}]},{"type":"text","value":"에 집중된 NLP 모델들이 발전되어 왔음."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이건 분명한 한계다. 텍스트에 대한 디펜던시를 깨야한다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"언어 == 문자가 아니다. speech가 있다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"그래서 우리 GSLM이 텍스트에 대한 디펜던시를 깰 수 있는 가능성을 보였다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"음성 프롬프트 시대의 시작을 알린다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"음성을 프롬프트로 주면 뒤이어서 인공지능이 말을 계속 이어서 말하는 모델의 등장!"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Background"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"음성을 입력으로 하는 NLP 어플리케이션들은 ASR => NLP를 거쳐야 했음."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"ASR의 정확도가 100%가 아니기 때문에 분명한 정보의 오류가 존재함."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"우리는 여기서 ASR + NLP 구조가 아닌 Speech to Speech로 간다."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Text나 label 없이 only 음성만으로 학습한다."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Textless NLP’s benefits"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"언어 상관없이 학습이 가능해질 가능성이 높아짐"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"텍스트로 표현이 안되는 말의 뉘앙스, 감정 등의 정보를 반영할 수 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"텍스트 레이블링 혹은 ASR 학습 없이 모델을 학습할 수 있음"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"유아들이 어떻게 언어를 배우고 말을 시작하는지를 알 수 있다(? 과연?)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"처음으로 텍스트 없이 audio to audio 번역 시스템이 가능해졌다!"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Data"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"6,000시간의 Libri-Light와 LibriSpeech 데이터셋 (인코더 학습)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"LibriSpeech and LJSpeech (디코더(TTS System) 학습)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Model"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241347514_376678770759983_4717648868496710483_n.jpg?_nc_cat=100&ccb=1-5&_nc_sid=ad8a9d&_nc_ohc=igZvOorsx80AX9BQc0l&_nc_ht=scontent-gmp1-1.xx&oh=896c0d14d6c8709f6c3d81963268db1d&oe=6147AD53","width":600},"children":[]},{"type":"text","value":"  \n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Encoder (S2u)\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Speech를 인풋으로 받아서 discrete unit(pseudo-text라고 부름)으로 인코딩"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"unit은 k-means clustering으로 나눔."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"인코더로는 CPC, wav2vec 2.0, HuBERT를 사용 (좋은 acoustic encoder들이라고 보시면 됨)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"uLM\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"unit sequence를 생성"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Decoder (u2S)\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"TTS System (Tacotron2 사용)"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"여기서 unit(pseudo-text)은 letter or phoneme과 매핑되지는 않음."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"100 이상의 유닛일 때 좋은 성능을 보였으며 unit은 보통 음소보다 짧은 단위를 인코딩했음."}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241223788_398469455180920_2630499539056655858_n.jpg?_nc_cat=107&ccb=1-5&_nc_sid=ad8a9d&_nc_ohc=rfiDlgtmTcYAX-EraG5&_nc_ht=scontent-gmp1-1.xx&oh=1c96a38f6af0ada3774380e4fd6110e6&oe=61489C23","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"생성한 음성은 pre-trained ASR 모델로 인식해서 성능 측정"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Pre-trained LM으로 텍스트 성능 측정"}]},{"type":"text","value":"\n"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Result"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"img","properties":{"src":"https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241364732_225715579507676_6485051182702467200_n.jpg?_nc_cat=108&ccb=1-5&_nc_sid=ad8a9d&_nc_ohc=h45PImsz8SkAX-kM1rz&_nc_ht=scontent-gmp1-1.xx&oh=88949e5b3a057a6e42b8266d03171ac7&oe=61492788","width":600},"children":[]},{"type":"text","value":"\n"},{"type":"element","tagName":"ul","properties":{},"children":[{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Unit의 수가 모델 성능에 큰 영향을 미침."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"Unit 수가 커질수록 Acoustic의 성능은 좋아졌음. (PER이 낮아졌다)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"LM 점수도 비슷한 경향이었으나, 너무 많은 unit을 사용하면 오히려 안 좋았음. (NLP에서 vocab의 적당한 사이즈가 좋은 이유와 비슷한 것 같음)"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"어떤 인코더 모델이냐에 따라 다른 결과가 나옴. HuBERT 성능이 가장 좋았음."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"li","properties":{},"children":[{"type":"text","value":"이렇게 자동으로 측정한 성능이 사람이 평가했을 때와 correlation이 높았음. (좋은 성능 지표)"}]},{"type":"text","value":"\n"}]}],"data":{"quirksMode":false}},"excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","fields":{"readingTime":{"text":"4 min read"}},"frontmatter":{"title":"Textless NLP","userDate":"19 September 2021","date":"2021-09-19T10:00:00.000Z","tags":["speech","nlp","paper"],"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/afa5c/gslm.png","srcSet":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/0dee1/gslm.png 750w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/8beaa/gslm.png 1080w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/d079a/gslm.png 1366w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/afa5c/gslm.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/b91fe939e42bcc0b6f0c076dca98fcc8/a66aa/gslm.webp 750w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/65dd5/gslm.webp 1080w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/4fad6/gslm.webp 1366w,\n/static/b91fe939e42bcc0b6f0c076dca98fcc8/c512e/gslm.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5625}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"children":[{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#182828","images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/f31ef/ghost.png 40w,\n/static/7ffe238930a689e103d70f234bb00199/1f8a1/ghost.png 80w,\n/static/7ffe238930a689e103d70f234bb00199/a8b52/ghost.png 120w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/e73fe/ghost.webp 40w,\n/static/7ffe238930a689e103d70f234bb00199/61ca6/ghost.webp 80w,\n/static/7ffe238930a689e103d70f234bb00199/507b0/ghost.webp 120w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}]}}]}},"relatedPosts":{"totalCount":13,"edges":[{"node":{"id":"f8857a8d-9121-5e23-91bc-3fbe4f090418","excerpt":"Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…","frontmatter":{"title":"Textless NLP","date":"2021-09-19T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/Textledd NLP: Generating expressive speech from raw audio/"}}},{"node":{"id":"93dec710-458c-531f-acbc-28d14f762768","excerpt":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…","frontmatter":{"title":"Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review","date":"2021-03-17T10:00:00.000Z"},"fields":{"readingTime":{"text":"3 min read"},"slug":"/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/"}}},{"node":{"id":"a4f99081-c696-5b06-85b1-c2268aed1215","excerpt":"EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…","frontmatter":{"title":"EMNLP Paper Review: Speech","date":"2020-12-08T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/2020 EMNLP Speech Paper Review/"}}},{"node":{"id":"e1128eb2-c5b3-55e1-83ef-3b6478bb7d7b","excerpt":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…","frontmatter":{"title":"One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review","date":"2020-10-14T10:00:00.000Z"},"fields":{"readingTime":{"text":"4 min read"},"slug":"/one-model-many-langs/"}}},{"node":{"id":"52b143f2-f85b-5c62-911f-b1ebdf23fa89","excerpt":"wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…","frontmatter":{"title":"Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations","date":"2020-09-12T10:00:00.000Z"},"fields":{"readingTime":{"text":"5 min read"},"slug":"/wav2vec2/"}}}]}},"pageContext":{"slug":"/Textledd NLP: Generating expressive speech from raw audio/","prev":{"excerpt":"Tokenization 문장에서 의미있는 단위로 나누는 작업을 라고 한다. 문자 단위 토큰화 문자 단위로 토큰화를 하는 것이다. 한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 1…","frontmatter":{"title":"Tokenizer","tags":["nlp"],"date":"2021-09-13T23:46:37.121Z","draft":false,"excerpt":null,"image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDAAEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQH/2wBDAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQH/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAgDBQf/xAAVAQEBAAAAAAAAAAAAAAAAAAACBf/aAAwDAQACEAMQAAABaaG4zOLfUUawY//EABkQAAMBAQEAAAAAAAAAAAAAAAQFBgMBB//aAAgBAQABBQJbRywjGljp1nxl5hJcKHAUrMVDgphq0bH5kf/EAB8RAAIBBAIDAAAAAAAAAAAAAAECAwQREyEAkRQxUv/aAAgBAwEBPwGZ1qJMiq40MhllecyN9MZWZrkWHsiw78enOzAvZHP/xAAhEQACAAQHAQAAAAAAAAAAAAABAgADEiEEERMUIkFTkf/aAAgBAgEBPwFKpS01Cx46aLKCi1gEVRl3G4xHsfkf/8QAJhAAAgIBAgYBBQAAAAAAAAAAAQIDBBEFEgATISIxQTMGMlFhYv/aAAgBAQAGPwKtp1HRa1Nr6pVkk50Mt2MFzsW4qPOeS1ghYy1gl2YuqY8vYbRDYm9GhivKG69eZG9UJnd3GZpPWFPtudc+p6L461tkVgJkn7ZmoZkXOQGOfH44AoaRp1QINw5FWFH6DPy7DIT/AEWJ98W9SlWJe9oK8ataLQoG7gzS2pIX34X46sGMe88DlTbEeJX2Fd207nU4OR07c/rxx//EABkQAQEBAQEBAAAAAAAAAAAAAAERIQAxgf/aAAgBAQABPyE1ST2K/mK0gpxFZTR0NxgkSypnBmHNuJathJRhY4BMdWSL9V/67Q+JtxJA9ymrkQvANMArqYCetMvf/9oADAMBAAIAAwAAABDTP//EABkRAQEBAQEBAAAAAAAAAAAAAAERITEAUf/aAAgBAwEBPxBS8SKEpvYqQBC18JKL2EuWBgdwz4Zv/8QAGhEBAQEBAAMAAAAAAAAAAAAAAREhMQBRgf/aAAgBAgEBPxCWB0yBU8hvAq76AAAwICFhysd+vn//xAAYEAEBAQEBAAAAAAAAAAAAAAABESEAMf/aAAgBAQABPxAbBKwQ8eg53lFdGqYyYG+bDPb0VONLCVgR3Mv6vANOz8waorF18m1M7dpSMHr7EawCJsQkOAL/2Q=="},"images":{"fallback":{"src":"/static/8e92507f0141c508e4d5a30d3ca6fe53/4d1d2/writing.jpg","srcSet":"/static/8e92507f0141c508e4d5a30d3ca6fe53/6cce3/writing.jpg 750w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/fb319/writing.jpg 1080w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/6a5de/writing.jpg 1366w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/4d1d2/writing.jpg 1400w","sizes":"100vw"},"sources":[{"srcSet":"/static/8e92507f0141c508e4d5a30d3ca6fe53/d2a19/writing.webp 750w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/72f43/writing.webp 1080w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/8992a/writing.webp 1366w,\n/static/8e92507f0141c508e4d5a30d3ca6fe53/d652b/writing.webp 1400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.6678571428571428}}},"author":[{"id":"Soohwan Kim","bio":"Co-founder and AI engineer of TUNiB.","avatar":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAABCUlEQVQ4y2MQVdQiGzEMJ80iCppwBOcSpRlZHZop+DSLKWnzS6v0Tpl5/da9E2cuXL1xZ/6SlXxSyvOXrlq8cp2ArBpBzaqNnX1HT57Zvmf/4ROnJk6fwyutMnHG3GlzFgri1wxBAjKqQPthSBXoYIgIUc7umTT96vVbJ06fO3X2AhABGZev3Zgxb5GwvAYhzTJAZ/cfPXF2595Duw8cASIg4/Dx072TZxLQjOFsVTCCeoEoZ3dPnH7l2q1zF6+6BYSbO3mdPnfp8tWb0+cuEiLsbGnV5q4Jx8+cO3TspLNviKmD+/4jx4+dPtc/bRZRzhaSUxeUUwMioGphBU0IW1BOfZjnKnpoBgBuRuAIvQwT7gAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png","srcSet":"/static/7ffe238930a689e103d70f234bb00199/d6138/ghost.png 400w","sizes":"100vw"},"sources":[{"srcSet":"/static/7ffe238930a689e103d70f234bb00199/416c3/ghost.webp 400w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":1}}}}]},"fields":{"readingTime":{"text":"10 min read"},"layout":"post","slug":"/tokenizer/"}},"next":null,"primaryTag":"speech"}},
    "staticQueryHashes": ["3170763342","3229353822"]}