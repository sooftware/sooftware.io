<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Ghost's Blog]]></title><description><![CDATA[A port of the casper blog built for gatsby]]></description><link>https://gatsby-casper.netlify.com</link><generator>GatsbyJS</generator><lastBuildDate>Mon, 20 Sep 2021 07:38:51 GMT</lastBuildDate><item><title><![CDATA[Textless NLP]]></title><description><![CDATA[Textless NLP: Generating expressive speech from raw audio paper / code / pre-train model / blog Name: Generative Spoken Language Model (GSLM…]]></description><link>https://gatsby-casper.netlify.com/Textledd NLP: Generating expressive speech from raw audio/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/Textledd NLP: Generating expressive speech from raw audio/</guid><pubDate>Sun, 19 Sep 2021 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Textless NLP: Generating expressive speech from raw audio&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2102.01192&quot;&gt;paper&lt;/a&gt; / &lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp/gslm&quot;&gt;code / pre-train model&lt;/a&gt; / &lt;a href=&quot;https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio&quot;&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Name: Generative Spoken Language Model (GSLM)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BERT, RoBERTa, GPT-3 등 최근 몇 년간 &lt;code class=&quot;language-text&quot;&gt;텍스트&lt;/code&gt;에 집중된 NLP 모델들이 발전되어 왔음.&lt;/li&gt;
&lt;li&gt;이건 분명한 한계다. 텍스트에 대한 디펜던시를 깨야한다.&lt;/li&gt;
&lt;li&gt;언어 == 문자가 아니다. speech가 있다.&lt;/li&gt;
&lt;li&gt;그래서 우리 GSLM이 텍스트에 대한 디펜던시를 깰 수 있는 가능성을 보였다.&lt;/li&gt;
&lt;li&gt;음성 프롬프트 시대의 시작을 알린다.&lt;/li&gt;
&lt;li&gt;음성을 프롬프트로 주면 뒤이어서 인공지능이 말을 계속 이어서 말하는 모델의 등장!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;음성을 입력으로 하는 NLP 어플리케이션들은 ASR =&gt; NLP를 거쳐야 했음.&lt;/li&gt;
&lt;li&gt;ASR의 정확도가 100%가 아니기 때문에 분명한 정보의 오류가 존재함.&lt;/li&gt;
&lt;li&gt;우리는 여기서 ASR + NLP 구조가 아닌 Speech to Speech로 간다.&lt;/li&gt;
&lt;li&gt;Text나 label 없이 only 음성만으로 학습한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Textless NLP’s benefits&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;언어 상관없이 학습이 가능해질 가능성이 높아짐&lt;/li&gt;
&lt;li&gt;텍스트로 표현이 안되는 말의 뉘앙스, 감정 등의 정보를 반영할 수 있음&lt;/li&gt;
&lt;li&gt;텍스트 레이블링 혹은 ASR 학습 없이 모델을 학습할 수 있음&lt;/li&gt;
&lt;li&gt;유아들이 어떻게 언어를 배우고 말을 시작하는지를 알 수 있다(? 과연?)&lt;/li&gt;
&lt;li&gt;처음으로 텍스트 없이 audio to audio 번역 시스템이 가능해졌다!&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;6,000시간의 Libri-Light와 LibriSpeech 데이터셋 (인코더 학습)&lt;/li&gt;
&lt;li&gt;LibriSpeech and LJSpeech (디코더(TTS System) 학습)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;img src=&quot;https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241347514_376678770759983_4717648868496710483_n.jpg?_nc_cat=100&amp;amp;ccb=1-5&amp;amp;_nc_sid=ad8a9d&amp;amp;_nc_ohc=igZvOorsx80AX9BQc0l&amp;amp;_nc_ht=scontent-gmp1-1.xx&amp;amp;oh=896c0d14d6c8709f6c3d81963268db1d&amp;amp;oe=6147AD53&quot; width=&quot;600&quot;&gt;  
&lt;ul&gt;
&lt;li&gt;Encoder (S2u)
&lt;ul&gt;
&lt;li&gt;Speech를 인풋으로 받아서 discrete unit(pseudo-text라고 부름)으로 인코딩&lt;/li&gt;
&lt;li&gt;unit은 k-means clustering으로 나눔.&lt;/li&gt;
&lt;li&gt;인코더로는 CPC, wav2vec 2.0, HuBERT를 사용 (좋은 acoustic encoder들이라고 보시면 됨)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;uLM
&lt;ul&gt;
&lt;li&gt;unit sequence를 생성&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Decoder (u2S)
&lt;ul&gt;
&lt;li&gt;TTS System (Tacotron2 사용)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;여기서 unit(pseudo-text)은 letter or phoneme과 매핑되지는 않음.&lt;/li&gt;
&lt;li&gt;100 이상의 유닛일 때 좋은 성능을 보였으며 unit은 보통 음소보다 짧은 단위를 인코딩했음.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241223788_398469455180920_2630499539056655858_n.jpg?_nc_cat=107&amp;amp;ccb=1-5&amp;amp;_nc_sid=ad8a9d&amp;amp;_nc_ohc=rfiDlgtmTcYAX-EraG5&amp;amp;_nc_ht=scontent-gmp1-1.xx&amp;amp;oh=1c96a38f6af0ada3774380e4fd6110e6&amp;amp;oe=61489C23&quot; width=&quot;600&quot;&gt;
&lt;ul&gt;
&lt;li&gt;생성한 음성은 pre-trained ASR 모델로 인식해서 성능 측정&lt;/li&gt;
&lt;li&gt;Pre-trained LM으로 텍스트 성능 측정&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Result&lt;/h2&gt;
&lt;img src=&quot;https://scontent-gmp1-1.xx.fbcdn.net/v/t39.2365-6/241364732_225715579507676_6485051182702467200_n.jpg?_nc_cat=108&amp;amp;ccb=1-5&amp;amp;_nc_sid=ad8a9d&amp;amp;_nc_ohc=h45PImsz8SkAX-kM1rz&amp;amp;_nc_ht=scontent-gmp1-1.xx&amp;amp;oh=88949e5b3a057a6e42b8266d03171ac7&amp;amp;oe=61492788&quot; width=&quot;600&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Unit의 수가 모델 성능에 큰 영향을 미침.&lt;/li&gt;
&lt;li&gt;Unit 수가 커질수록 Acoustic의 성능은 좋아졌음. (PER이 낮아졌다)&lt;/li&gt;
&lt;li&gt;LM 점수도 비슷한 경향이었으나, 너무 많은 unit을 사용하면 오히려 안 좋았음. (NLP에서 vocab의 적당한 사이즈가 좋은 이유와 비슷한 것 같음)&lt;/li&gt;
&lt;li&gt;어떤 인코더 모델이냐에 따라 다른 결과가 나옴. HuBERT 성능이 가장 좋았음.&lt;/li&gt;
&lt;li&gt;이렇게 자동으로 측정한 성능이 사람이 평가했을 때와 correlation이 높았음. (좋은 성능 지표)&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Tokenizer]]></title><description><![CDATA[Tokenization 문장에서 의미있는 단위로 나누는 작업을 라고 한다. 문자 단위 토큰화 문자 단위로 토큰화를 하는 것이다. 한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 1…]]></description><link>https://gatsby-casper.netlify.com/tokenizer/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/tokenizer/</guid><pubDate>Mon, 13 Sep 2021 23:46:37 GMT</pubDate><content:encoded>&lt;h2&gt;Tokenization&lt;/h2&gt;
&lt;p&gt;문장에서 의미있는 단위로 나누는 작업을 &lt;code class=&quot;language-text&quot;&gt;토큰화&lt;/code&gt;라고 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;문자 단위 토큰화&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;문자 단위로 토큰화를 하는 것이다.&lt;/li&gt;
&lt;li&gt;한글 음절 수는 모두 11,172개이므로 알파벳, 숫자, 기호 등을 고려한다고 해도 단어 사전의 크기는 기껏해야 15,000개를 넘기 어렵다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;장점&lt;/code&gt; : 모든 문자를 포함시켜서 &lt;code class=&quot;language-text&quot;&gt;UNK&lt;/code&gt;토큰이 잘 발생하지 않는다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;단점&lt;/code&gt; : 의미 있는 단위가 되기 어렵고, 상대적으로 시퀀스가 길어진다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;어제 카페 갔었어 &gt; 어, 제, 카, 페, 갔, 었, 어
어제 카페 갔었는데요 &gt; 어, 제, 카, 페, 갔, 었, 는, 데, 요&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;단어 단위 토큰화&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;단어 단위로 토큰화를 하는 것이다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;장점&lt;/code&gt; : 공백으로 쉽게 분리할 수 있다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;단점&lt;/code&gt; : 모든 단어들을 다 포함시키기에는 단어 사전의 크기가 상당히 크다. 이는 메모리 문제를 야기!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;어제 카페 갔었어 &gt; 어제, 카페, 갔었어
어제 카페 갔었는데요 &gt; 어제, 카페, 갔었는데요&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;서브워드 단위 토큰화&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;문자 단위와 단어 단위 토큰화의 중간에 있는 형태이다.&lt;/li&gt;
&lt;li&gt;“자주 등장한 단어는 그대로 두고, 자주 등장하지 않은 단어는 의미있는 서브 워드 토큰들로 분절한다” 라는 원칙에 기반을 둔 알고리즘&lt;/li&gt;
&lt;li&gt;단어 사전의 크기를 지나치게 늘리지 않으면서도 &lt;code class=&quot;language-text&quot;&gt;UNK&lt;/code&gt; 문제를 해결할 수 있다.&lt;/li&gt;
&lt;li&gt;희귀 단어, 신조어와 같은 문제를 완화시킬 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;서브워드 기반 토크나이저&lt;/h2&gt;
&lt;h3&gt;BPE(Byte-Pair Encoding)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BPE(Byte pair encoding) 알고리즘은 1994년에 제안된 데이터 압축 알고리즘&lt;/li&gt;
&lt;li&gt;연속적으로 가장 많이 등장한 글자의 쌍을 찾아서 하나의 글자로 병합하는 방식을 수행&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;aaabdaaabac    # 가장 자주 등장하고 있는 바이트의 쌍(byte pair)은 &apos;aa&apos; , Z=aa
ZabdZabac      # Y=ab
ZYdZYac        # X=ZY
XdXac          # 더 이상 병합할 바이트의 쌍이 없으므로 최종 결과로 하여 종료&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;h3&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.07909&quot;&gt;자연어 처리에서의 BPE&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;글자(charcter) 단위에서 점차적으로 단어 집합(vocabulary)을 만들어 내는 Bottom up 방식의 접근을 사용&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BPE는 일반적으로 훈련 데이터를 단어 단위로 분절하는 Pre-tokenize 과정을 거쳐야한다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pre-tokenize는 공백 단위나 규칙 기반으로 수행될 수 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Example)&lt;br&gt;
&lt;code class=&quot;language-text&quot;&gt;(&apos;hug&apos;, 10), (&apos;pug&apos;, 5), (&apos;pun&apos;, 12), (&apos;bun&apos;, 4), (&apos;hugs&apos;, 5)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pre-tokenize를 거쳐서 나온 단어들이라 하고 여기서 정수 값은 각 단어가 얼마나 등장했는지를 나타내는 값이다.&lt;br&gt;
이때 기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’] 이다.&lt;br&gt;
기본 사전을 기반으로 단어들을 쪼개면 다음과 같다.&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;(&apos;h&apos; &apos;u&apos; &apos;g&apos;, 10), (&apos;p&apos; &apos;u&apos; &apos;g&apos;, 5), (&apos;p&apos; &apos;u&apos; &apos;n&apos;, 12), (&apos;b&apos; &apos;u&apos; &apos;n&apos;, 4), (&apos;h&apos; &apos;u&apos; &apos;g&apos; &apos;s&apos;, 5)&lt;/code&gt;&lt;br&gt;
“hu”는 총 15번, “ug”는 총 20번이 나와 가장 많이 등장한 쌍은 “ug”가 되고 “u”와 “g”를 합친 “ug”를 사전에 새로 추가한다.&lt;br&gt;
그럼 이때 기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’, ‘ug’] 이다.&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;(&apos;h&apos; &apos;ug&apos;, 10), (&apos;p&apos; &apos;ug&apos;, 5), (&apos;p&apos; &apos;u&apos; &apos;n&apos;, 12), (&apos;b&apos; &apos;u&apos; &apos;n&apos;, 4), (&apos;h&apos; &apos;ug&apos; &apos;s&apos;, 5)&lt;/code&gt;&lt;br&gt;
또 가장 많이 나온 쌍은 16번 등장한 “un”이므로, “un”을 사전에 추가한다. 그 다음은 15번 등장한 “hug”이므로 “hug”도 사전에 추가한다.&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;(&apos;hug&apos;, 10), (&apos;p&apos; &apos;ug&apos;, 5), (&apos;p&apos; &apos;un&apos;, 12), (&apos;b&apos; &apos;un&apos;, 4), (&apos;hug&apos; &apos;s&apos;, 5)&lt;/code&gt;&lt;br&gt;
기본 사전은 [‘b’, ‘g’, ‘h’, ‘n’, ‘p’, ‘s’, ‘u’, ‘ug’, ‘un’, ‘hug’]가 됩니다.
이렇게 처음에는 글자 단위였던 것이 의미있는 서브워드 토큰들로 분절할 수 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1909.03341.pdf&quot;&gt;Byte-level BPE(BBPE)&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54731898/133186594-e4f0a5d8-65a2-4ba5-b6a2-09be7bdc6757.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;GPT-2 논문&lt;/a&gt;에서 바이트를 사전의 기본 단위로 사용하는 트릭을 사용&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPT-2 모델은 256개의 기본 바이트 토큰과 &lt;code class=&quot;language-text&quot;&gt;&amp;lt;end-of-text&gt;&lt;/code&gt; 토큰 그리고 50,000 개의 서브 워드를 더해 총 50,257 개의 단어 집합(vocabulary)을 가진다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;256 바이트셋으로 모든 텍스트를 표현할 수 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;UNK&lt;/code&gt; 문제없이 모든 텍스트를 분절할 수 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;multilingual일 때, 언어들 사이에서 vocabulary 공유를 가장 많이 한다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54731898/133136459-f1b4fdbf-d9d4-4976-842e-c00cbf657624.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;WordPiece&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;에서 활용된 서브 워드 토크나이저 알고리즘&lt;/li&gt;
&lt;li&gt;BPE와 마찬가지로 사전을 코퍼스 내 등장한 캐릭터들로 초기화 한 후, 사용자가 지정한 횟수 만큼 서브 워드를 병합하는 방식으로 훈련&lt;/li&gt;
&lt;li&gt;하지만 WordPiece는 BPE와 같이 가장 많이 등장한 쌍을 병합하는 것이 아니라, 병합되었을 때 코퍼스의 Likelihood를 가장 높이는 쌍을 병합하게 된다.&lt;/li&gt;
&lt;li&gt;즉, WordPiece에서는 코퍼스 내에서 “ug”가 등장할 확률을 “u”와 “g”가 각각 등장할 확률을 곱한 값으로 나눈 값이 다른 쌍보다 클 경우 해당 쌍을 병합하게 된다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/54731898/133140262-f0afdedc-e54e-4564-a88d-134163c0a219.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;또 이 학생의 집에서 병든 소를 도축했던 35살 남성도 탄저병에 걸린 것으로 확인됐습니다.&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;[&apos;또&apos;, &apos;이&apos;, &apos;학생&apos;, &apos;##의&apos;, &apos;집&apos;, &apos;##에&apos;, &apos;##서&apos;, &apos;병든&apos;, &apos;소&apos;, &apos;##를&apos;, &apos;도축&apos;, &apos;##했&apos;, &apos;##던&apos;, &apos;35&apos;, &apos;##살&apos;, &apos;남성&apos;, &apos;##도&apos;, &apos;탄&apos;, &apos;##저&apos;, &apos;##병&apos;, &apos;##에&apos;, &apos;걸린&apos;, &apos;것&apos;, &apos;##으로&apos;, &apos;확인&apos;, &apos;##됐&apos;, &apos;##습&apos;, &apos;##니다&apos;, &apos;.&apos;]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;h3&gt;Unigram&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;서브 워드에서 시작해 점차 사전을 줄여나가는 top-down 방식으로 진행&lt;/li&gt;
&lt;li&gt;매 스텝마다 Unigram은 주어진 코퍼스와 현재 사전에 대한 Loss를 측정한다.&lt;/li&gt;
&lt;li&gt;또한 각각의 서브 워드에 대해 해당 서브 워드가 코퍼스에서 제거되었을 때, Loss가 얼마나 증가하는지를 측정하여 Loss를 가장 조금 증가시키는 p 개 토큰을 제거한다. (p는 보통 전체 사전 크기의 10-20% 값으로 설정)&lt;/li&gt;
&lt;li&gt;해당 과정을 사용자가 원하는 사전 크기를 지니게 될 때 까지 반복하게 되고, 기본 캐릭터들은 반드시 사전에서 제거되지 않고 유지되어야한다.&lt;/li&gt;
&lt;li&gt;매번 같은 토큰 리스트를 반환하는 BPE, WordPiece와 달리 Unigram은 다양한 토큰 리스트가 생길 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3&gt;SentencePiece&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;지금까지 살펴본 모든 방법들은 공백을 기준으로 단어를 분절할 수 없기 때문에 Pre-tokenize 과정을 필요로 했다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;하지만 sentencepiece는 공백을 기준으로 단어를 분절할 수 있기 때문에 Pre-tokenize 과정이 필요없다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;또한 디코딩 과정에서 모든 토큰들을 붙여준 후,  메타스페이스(”▁”)만 공백으로 바꿔주면 되기 때문에 원상복구가 가능하다는 특징이 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BPE 혹은 Unigram을 적용하여 사전을 구축하게 된다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;또 이 학생의 집에서 병든 소를 도축했던 35살 남성도 탄저병에 걸린 것으로 확인됐습니다.&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;[&apos;▁또&apos;, &apos;▁이&apos;, &apos;▁학생&apos;, &apos;의&apos;, &apos;▁집에서&apos;, &apos;▁병&apos;, &apos;든&apos;, &apos;▁소&apos;, &apos;를&apos;, &apos;▁&apos;, &apos;도&apos;, &apos;축&apos;, &apos;했던&apos;, &apos;▁35&apos;, &apos;살&apos;, &apos;▁남성&apos;, &apos;도&apos;, &apos;▁탄&apos;, &apos;저&apos;, &apos;병&apos;, &apos;에&apos;, &apos;▁걸린&apos;, &apos;▁것으로&apos;, &apos;▁확인&apos;, &apos;됐&apos;, &apos;습니다&apos;, &apos;.&apos;]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;br&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/22592&quot;&gt;https://wikidocs.net/22592&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://huggingface.co/transformers/master/tokenizer_summary.html&quot;&gt;https://huggingface.co/transformers/master/tokenizer_summary.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://karter.io/huggingface&quot;&gt;https://karter.io/huggingface&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/&quot;&gt;https://ratsgo.github.io/nlpbook/docs/preprocess/bpe/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1508.07909.pdf&quot;&gt;https://arxiv.org/pdf/1508.07909.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[정규표현식 (regex)]]></title><description><![CDATA[정규 표현식 정규표현식(regular expression)은 일종의 문자를 표현하는 공식으로, 특정 규칙이 있는 문자열 집합을 추출할 때 자주 사용되는 기법입니다. 주로 Prograaming Language나 Text Editor…]]></description><link>https://gatsby-casper.netlify.com/regex/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/regex/</guid><pubDate>Wed, 08 Sep 2021 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;정규 표현식&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;정규표현식(regular expression)은 일종의 문자를 표현하는 공식으로, 특정 규칙이 있는 문자열 집합을 추출할 때 자주 사용되는 기법입니다.&lt;/li&gt;
&lt;li&gt;주로 Prograaming Language나 Text Editor등에서 문자열의 검색과 치환을 위한 용도로 쓰이고 있습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3&gt;메타문자&lt;/h3&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;. ^ $ * + ? {} [] \ | ()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;메타문자는 문자를 설명하기 위한 문자로, 문자의 구성을 설명하기 위해 원래의 의미가 아닌 다른 의미로 쓰이는 문자를 말합니다.&lt;/li&gt;
&lt;li&gt;정규표현식에서는 위와 같은 메타문자를 사용합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: &lt;code class=&quot;language-text&quot;&gt;[]&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;문자 클래스인 &lt;code class=&quot;language-text&quot;&gt;[]&lt;/code&gt;는 &lt;code class=&quot;language-text&quot;&gt;&quot;[] 사이의 문자들과 매치&quot;&lt;/code&gt;라는 의미를 가지며, &lt;code class=&quot;language-text&quot;&gt;[]&lt;/code&gt;사이에는 어떤 문자도 들어갈 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[abc]&lt;/code&gt;라면 이 표현식의 의미는 &lt;code class=&quot;language-text&quot;&gt;&quot;a, b, c 중 한 개의 문자와 매치&quot;&lt;/code&gt;를 뜻합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;예시) &lt;code class=&quot;language-text&quot;&gt;[abc]&lt;/code&gt;가 &lt;code class=&quot;language-text&quot;&gt;&quot;a&quot;, &quot;before&quot;, &quot;dude&quot;&lt;/code&gt;와 어떻게 매치되는지 살펴보겠습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;a&lt;/code&gt;는 정규식과 일치하는 문자인 &lt;code class=&quot;language-text&quot;&gt;a&lt;/code&gt;가 있으므로 매치&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;before&lt;/code&gt;는 정규식과 일치하는 문자인 &lt;code class=&quot;language-text&quot;&gt;b&lt;/code&gt;가 있으므로 매치&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;dude&lt;/code&gt;는 정규식과 일치하는 문자인 &lt;code class=&quot;language-text&quot;&gt;a, b, c&lt;/code&gt;중 어느 하나도 포함하고 있지 않으므로 매치되지 않음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: &lt;code class=&quot;language-text&quot;&gt;하이픈(-), 캐럿(^)&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;하이픈(-)은 두 문자 사이의 범위(from - to)를 의미, 캐럿(^)은 반대를 의미합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[a-zA-Z]&lt;/code&gt;: 알파벳 모두 매치&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[0-9]&lt;/code&gt;: 숫자 매치&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[^0-9]&lt;/code&gt;: 숫자가 아닌 문자만 매치
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[]&lt;/code&gt;안에서는 부정의 의미로 사용&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[]&lt;/code&gt;가 없으면 문자열의 처음을 뜻함(끝은 $로 표시)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;문자클래스&lt;code class=&quot;language-text&quot;&gt;[]&lt;/code&gt;안에는 어떤 문자나 메타 문자도 사용할 수 있지만 &lt;code class=&quot;language-text&quot;&gt;^&lt;/code&gt;는 반대(not)의 의미로 사용되기 때문에 조심해야합니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: &lt;code class=&quot;language-text&quot;&gt;\역슬래쉬&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;[0-9]&lt;/code&gt;또는 &lt;code class=&quot;language-text&quot;&gt;[a-zA-Z]&lt;/code&gt;와 같은 정규표현식은 &lt;code class=&quot;language-text&quot;&gt;\역슬래쉬&lt;/code&gt;를 이용해 간단하게 표현할 수 있습니다. 이번엔 &lt;code class=&quot;language-text&quot;&gt;\역슬래쉬&lt;/code&gt;를 사용한 별도의 표기법들에 대해 알아보겠습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\d&lt;/code&gt; - 숫자와 매치, &lt;code class=&quot;language-text&quot;&gt;[0-9]&lt;/code&gt;와 동일한 표현식이다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\D&lt;/code&gt; - 숫자가 아닌 것과 매치, &lt;code class=&quot;language-text&quot;&gt;[^0-9]&lt;/code&gt;와 동일한 표현식이다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\s&lt;/code&gt; - whitespace 문자와 매치, &lt;code class=&quot;language-text&quot;&gt;[ \t\n\r\f\v]&lt;/code&gt;와 동일한 표현식이다. 맨 앞의 빈 칸은 공백문자(space)를 의미한다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\S&lt;/code&gt; - whitespace 문자가 아닌 것과 매치, &lt;code class=&quot;language-text&quot;&gt;[^\t\n\r\f\v]&lt;/code&gt;와 동일한 표현식이다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\w&lt;/code&gt; - 문자+숫자(alphanumeric)와 매치, &lt;code class=&quot;language-text&quot;&gt;[a-zA-Z0-9_]&lt;/code&gt;와 동일한 표현식이다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\W&lt;/code&gt; - 문자+숫자(alphanumeric)가 아닌 문자와 매치, &lt;code class=&quot;language-text&quot;&gt;[^a-zA-Z0-9_]&lt;/code&gt;와 동일한 표현식이다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\s&lt;/code&gt; - 스페이스(공백문자), 탭과 매치, &lt;code class=&quot;language-text&quot;&gt;[\t\n\f\r]&lt;/code&gt;과 동일한 표현식이다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\S&lt;/code&gt; - 스페이스 외 문자와 매치, &lt;code class=&quot;language-text&quot;&gt;[^\t\n\f\r]&lt;/code&gt;과 동일한 표현식이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;대문자로 사용된 것은 소문자의 반대임을 추측할 수 있습니다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;또한 정규식 상의 특별한 의미가 있는 문자들을 문자 그대로 쓸 때 앞에 붙혀 사용됩니다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: ‘Dot(.)’&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;줄바꿈 문자인 &lt;code class=&quot;language-text&quot;&gt;\n&lt;/code&gt;을 제외한 모든 문자와 매치됨을 의미합니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;다음 정규식을 보겠습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;a.b&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;위 정규식의 의미는 &lt;code class=&quot;language-text&quot;&gt;&quot;a + 모든문자 + b&quot;&lt;/code&gt;와 같습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;즉 a와b라는 문자 사이에 어떤 문자가 들어가도 모두 매치가 된다는 의미입니다.
&lt;ul&gt;
&lt;li&gt;예시로 &lt;code class=&quot;language-text&quot;&gt;&quot;aab&quot;, &quot;a0b&quot;, &quot;abc&quot;&lt;/code&gt;를 보겠습니다.
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;aab&lt;/code&gt;는 가운데 문자 &lt;code class=&quot;language-text&quot;&gt;a&lt;/code&gt;가 모든 문자를 의미하는 &lt;code class=&quot;language-text&quot;&gt;.&lt;/code&gt;과 일치하므로 정규식과 매치됩니다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;a0b&lt;/code&gt;도 동일하게 가운데 문자 &lt;code class=&quot;language-text&quot;&gt;0&lt;/code&gt;이 모든 문자를 의미하는 &lt;code class=&quot;language-text&quot;&gt;.&lt;/code&gt;과 일치하므로 정규식과 매치됩니다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;abc&lt;/code&gt;는 &lt;code class=&quot;language-text&quot;&gt;a&lt;/code&gt;와 &lt;code class=&quot;language-text&quot;&gt;b&lt;/code&gt;사이에 어떤 문자가 존재하지 않기 때문에 위 정규식과 매치되지 않습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;그렇다면 이 정규식은 어떨까요?&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;a[.]b&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;앞서 설명드렸다 싶이 문자클래스&lt;code class=&quot;language-text&quot;&gt;[]&lt;/code&gt;는 내부에 메타문자가 들어가더라도 문자 그대로 인식해주는 특징을 갖고 있습니다.
따라서 &lt;code class=&quot;language-text&quot;&gt;a.b&lt;/code&gt;와 매치되고 &lt;code class=&quot;language-text&quot;&gt;a0b&lt;/code&gt;와 같은 문자열은 매치되지 않습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: 반복 관련 메타 문자 &lt;code class=&quot;language-text&quot;&gt;* + ? {}&lt;/code&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;반복(*)
&lt;ul&gt;
&lt;li&gt;메타문자 &lt;code class=&quot;language-text&quot;&gt;*&lt;/code&gt;은 &lt;code class=&quot;language-text&quot;&gt;*&lt;/code&gt;바로 앞에 있는 문자가 0부터 무한대로 반복될 수 있다는 의미입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;ca*t&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;위 정규식은 &lt;code class=&quot;language-text&quot;&gt;c + a(0번 이상 반복) + t&lt;/code&gt;라는 것을 알 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;ct&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;cat&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;caaaat&lt;/code&gt; 모두 매치됩니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;반복(+)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;반복을 나타내는 또 다른 문자인 &lt;code class=&quot;language-text&quot;&gt;+&lt;/code&gt;입니다. &lt;code class=&quot;language-text&quot;&gt;+&lt;/code&gt;는 &lt;code class=&quot;language-text&quot;&gt;*&lt;/code&gt;과 달리 최소 1번 이상 반복될 때 사용합니다. 즉 &lt;code class=&quot;language-text&quot;&gt;*&lt;/code&gt;은 반복횟수가 0부터 시작, &lt;code class=&quot;language-text&quot;&gt;+&lt;/code&gt;는 반복횟수가 1부터 시작됩니다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;ct&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;cat&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;caaat&lt;/code&gt;중 &lt;code class=&quot;language-text&quot;&gt;ct&lt;/code&gt;는 매치되지 않습니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;반복({m,n})&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;{}&lt;/code&gt;는 원하는 반복횟수를 지정하고 싶을 때 사용됩니다. m에서 n까지 반복, m이상인 경우, n이하인 경우 등 자유롭게 원하는 만큼 조절이 가능합니다.
&lt;ol&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;{m}&lt;/code&gt;: 반드시 m번 반복
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;ca{2}t&lt;/code&gt;: “c + a(반드시 2번 반복) + t”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;{m, n}&lt;/code&gt;: m~n회 반복
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;ca{2, 5}t&lt;/code&gt;: “c + a(2~5회 반복) + t”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;{m,}, {,n}&lt;/code&gt;: m회 이상 반복, n회 이하 반복&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;반복(?)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;?&lt;/code&gt;는 반복은 아니지만 비슷한 개념으로 &lt;code class=&quot;language-text&quot;&gt;{0, 1}&lt;/code&gt;과 같은 의미를 지닙니다. 즉, 문자가 있거나 없거나 둘 다 매치 되는 경우입니다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;ab?c&lt;/code&gt; : “a + b(있어도 되고 없어도 된다) + c”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;*&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;+&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;?&lt;/code&gt;메타 문자는 모두 &lt;code class=&quot;language-text&quot;&gt;{m, n}&lt;/code&gt; 형태로 고쳐 쓰는 것이 가능하지만 가급적 이해하기 쉽고 간결한 &lt;code class=&quot;language-text&quot;&gt;*&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;+&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;?&lt;/code&gt;메타 문자를 사용하는 것이 좋습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;ex) 주민등록번호 정규표현식&lt;/h3&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;/^\d{2}[0-1]\d{1}[0-3]\d{1}\-[1-4]\d{6}$/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;^&lt;/code&gt;: Start of String&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\d{3}&lt;/code&gt;: 1-2번째(년도) 숫자 0-9&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[0-1]&lt;/code&gt;: 3번째(월도 앞자리) 0,1&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\d{1}&lt;/code&gt;: 4번째(월도 뒷자리) 숫자 0-9&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[0-3]&lt;/code&gt;: 5번째(일자 앞자리) 0,1,2,3&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\d{1}&lt;/code&gt;: 6번째(일자 뒷자리) 숫자 0-9&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\-&lt;/code&gt; : 7번째(구분자) -&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;[1-4]&lt;/code&gt;: 8번째(성별) 90년대생 1,2 2000년대생 3,4&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;\d{6}&lt;/code&gt;: 9-14번째(뒷자리 6자리) 숫자 0-9&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;$&lt;/code&gt;: End of String&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h2&gt;파이썬과 정규표현식&lt;/h2&gt;
&lt;p&gt;파이썬은 정규 표현식을 지원하기 위해 re(regular expression)모듈을 제공합니다.&lt;/p&gt;
&lt;p&gt;Usage&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;import re
pattern = re.compile(&amp;quot;정규식&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;re.compile을 사용하여 정규 표현식을 컴파일합니다. re.compile의 결과인 객체 &lt;code class=&quot;language-text&quot;&gt;pattern&lt;/code&gt;에 입력한 정규식의 대한 정보가 담겨있습니다.&lt;/p&gt;
&lt;p&gt;이제 &lt;code class=&quot;language-text&quot;&gt;pattern&lt;/code&gt;객체를 이용해 문자열 검색을 수행해보겠습니다. 컴파일된 패턴 객체는 다음과 같은 4가지 메서드를 제공합니다.
&lt;img src=&quot;https://user-images.githubusercontent.com/59256704/133471058-446759bb-eeeb-4000-b6ef-6ca795be12df.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;import re
p = re.compile(&amp;#39;[a-z]+&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;위 코드를 실행해 생성된 객체 &lt;code class=&quot;language-text&quot;&gt;p&lt;/code&gt;로 메서드를 살펴보겠습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;match&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;match 메서드는 문자열의 처음부터 정규식과 매치되는지 조사하며, 반환값으로 match 객체를 돌려줍니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; m = p.match(&amp;quot;python&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; print(m)
&amp;lt;_sre.SRE_Match object at 0x01F3F9F8&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;python&lt;/code&gt;문자열은 &lt;code class=&quot;language-text&quot;&gt;[a-z]+&lt;/code&gt;정규식에 부합되므로 match 객체를 돌려줍니다.&lt;/p&gt;
&lt;p&gt;따라서 파이썬 정규식 프로그램은 다음과 같이 작성해 매치 여부를 확인합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;p = re.compile(정규표현식)
m = p.match(문자열)

if m:
    print(&amp;quot;정규식 일치&amp;quot;)
else:
    print(&amp;quot;정규식 불일치&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;h3&gt;search&lt;/h3&gt;
&lt;p&gt;동일하게 컴파일된 객체 p를 갖고 이번엔 search 메서드를 수행해보겠습니다. 예시를 통해 match와의 차이점을 확인해보시면 좋을 것 같습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; m = p.search(&amp;quot;3 python&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; print(m)
&amp;lt;_sre.SRE_Match object at 0x01F3FA30&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;3 python&lt;/code&gt;문자열은 첫번째 문자가 숫자이므로 match메서드에서는 None을 반환합니다. 하지만 search메서드는 문자열의 처음부터 검색하는 것이 아니라 문자열 전체를 검색하기 때문에 “python”문자열과 매치돼서 match객체를 반환하게 됩니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;findall&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;findall&lt;/code&gt; 메서드는 이름 그대로 문자열에서 정규식과 일치하는 부분을 찾아 리스트로 반환시켜줍니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; result = p.findall(&amp;quot;life is too short&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; print(result)
[&amp;#39;life&amp;#39;, &amp;#39;is&amp;#39;, &amp;#39;too&amp;#39;, &amp;#39;short&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;정규식과 일치하는 부분인 각 단어들이 반환되는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;finditer&lt;/h3&gt;
&lt;p&gt;finditer는 findall과 동일하지만 그 결과로 반복 가능한 객체를 돌려줍니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; result = p.finditer(&amp;quot;life is too short&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; print(result)
&amp;lt;callable_iterator object at 0x01F5E390&amp;gt;
&amp;gt;&amp;gt;&amp;gt; for r in result: print(r)
...
&amp;lt;_sre.SRE_Match object at 0x01F3F9F8&amp;gt;
&amp;lt;_sre.SRE_Match object at 0x01F3FAD8&amp;gt;
&amp;lt;_sre.SRE_Match object at 0x01F3FAA0&amp;gt;
&amp;lt;_sre.SRE_Match object at 0x01F3F9F8&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;반복 가능한 객체가 포함하는 각각의 요소는 match 객체입니다.&lt;/p&gt;
&lt;br&gt;
&lt;h2&gt;match 객체의 메서드&lt;/h2&gt;
&lt;p&gt;계속 반환받아왔던 match객체를 써먹어야겠죠? 이번엔 match객체의 메서드들에 대해 알아보겠습니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/59256704/133474331-97272bd8-2286-45c7-b026-db5e6948cb8f.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;다음 예시로 살펴보겠습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; m = p.match(&amp;quot;python&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; m.group()
&amp;#39;python&amp;#39;
&amp;gt;&amp;gt;&amp;gt; m.start()
0
&amp;gt;&amp;gt;&amp;gt; m.end()
6
&amp;gt;&amp;gt;&amp;gt; m.span()
(0, 6)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;예상된 결과값입니다. 만약 match메서드를 사용해 반환된 match메서드라면 start()의 결과값은 항상 0일 것입니다. match메서드는 항상 문자열의 시작부터 조사하기 때문이죠.&lt;/p&gt;
&lt;p&gt;만약 search 메서드를 사용했다면 start()값은 다르게 나올 것입니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; m = p.search(&amp;quot;3 python&amp;quot;)
&amp;gt;&amp;gt;&amp;gt; m.group()
&amp;#39;python&amp;#39;
&amp;gt;&amp;gt;&amp;gt; m.start()
2
&amp;gt;&amp;gt;&amp;gt; m.end()
8
&amp;gt;&amp;gt;&amp;gt; m.span()
(2, 8)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;이처럼 문자열에서 정규식이 해당되는 부분의 첫 시작지점이 나오게 됩니다.&lt;/p&gt;
&lt;p&gt;※ 모듈 단위로 수행&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; m = re.match(&amp;#39;[a-z]+&amp;#39;, &amp;quot;python&amp;quot;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;다음과 같이 축약된 형태로도 사용가능합니다.&lt;/p&gt;
&lt;br&gt;
&lt;h2&gt;컴파일 옵션&lt;/h2&gt;
&lt;p&gt;정규식을 컴파일할 때 다음 옵션을 사용할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DOTALL(S) - . 이 줄바꿈 문자를 포함하여 모든 문자와 매치할 수 있도록 한다.&lt;/li&gt;
&lt;li&gt;IGNORECASE(I) - 대소문자에 관계없이 매치할 수 있도록 한다.&lt;/li&gt;
&lt;li&gt;MULTILINE(M) - 여러줄과 매치할 수 있도록 한다. (^, $ 메타문자의 사용과 관계가 있는 옵션이다)&lt;/li&gt;
&lt;li&gt;VERBOSE(X) - verbose 모드를 사용할 수 있도록 한다. (정규식을 보기 편하게 만들수 있고 주석등을 사용할 수 있게된다.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;옵션을 사용할 때는 &lt;code class=&quot;language-text&quot;&gt;re.DOTALL&lt;/code&gt;처럼 전체 옵션이름을 써도 되고, &lt;code class=&quot;language-text&quot;&gt;re.S&lt;/code&gt;처럼 약어를 써도 됩니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;DOTALL, S&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;.&lt;/code&gt;메타 문자가 줄바꿈 문자&lt;code class=&quot;language-text&quot;&gt;\n&lt;/code&gt;도 포함시키도록 하고 싶다면 &lt;code class=&quot;language-text&quot;&gt;re.DOTALL&lt;/code&gt;또는 &lt;code class=&quot;language-text&quot;&gt;re.S&lt;/code&gt;옵션을 사용해 정규식을 컴파일하면 됩니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; p = re.compile(&amp;#39;a.b&amp;#39;, re.DOTALL)
&amp;gt;&amp;gt;&amp;gt; m = p.match(&amp;#39;a\nb&amp;#39;)
&amp;gt;&amp;gt;&amp;gt; print(m)
&amp;lt;_sre.SRE_Match object at 0x01FCF3D8&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;re.DOTALL&lt;/code&gt;옵션으로 &lt;code class=&quot;language-text&quot;&gt;\n&lt;/code&gt;도 매치시키는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;IGNORECASE, I&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;re.IGNORECASE&lt;/code&gt; 또는 &lt;code class=&quot;language-text&quot;&gt;re.I&lt;/code&gt; 옵션은 대소문자 구별 없이 매치를 수행할 때 사용하는 옵션입니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; p = re.compile(&amp;#39;[a-z]&amp;#39;, re.I)
&amp;gt;&amp;gt;&amp;gt; p.match(&amp;#39;python&amp;#39;)
&amp;lt;_sre.SRE_Match object at 0x01FCFA30&amp;gt;
&amp;gt;&amp;gt;&amp;gt; p.match(&amp;#39;Python&amp;#39;)
&amp;lt;_sre.SRE_Match object at 0x01FCFA68&amp;gt;
&amp;gt;&amp;gt;&amp;gt; p.match(&amp;#39;PYTHON&amp;#39;)
&amp;lt;_sre.SRE_Match object at 0x01FCF9F8&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;[a-z]&lt;/code&gt; 정규식은 소문자만을 의미하지만 re.I옵션으로 대소문자 구별없이 매치되는 것을 볼 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;MULTILINE, M&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;^&lt;/code&gt;은 문자열의 처음, &lt;code class=&quot;language-text&quot;&gt;$&lt;/code&gt;은 문자열의 마지막을 의미합니다. 자세한 설명 전에 다음 예시를 살펴보겠습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;import re
p = re.compile(&amp;quot;^python\s\w+&amp;quot;)

data = &amp;quot;&amp;quot;&amp;quot;python one
life is too short
python two
you need python
python three&amp;quot;&amp;quot;&amp;quot;

print(p.findall(data))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;결과&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;[&amp;#39;python one&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;결과를 보면 알겠지만 &lt;code class=&quot;language-text&quot;&gt;^&lt;/code&gt;메타 문자에 의해 python이라는 문자열을 사용한 첫 번째 줄만 매치된 것을 알 수 있습니다.&lt;/p&gt;
&lt;p&gt;하지만 &lt;code class=&quot;language-text&quot;&gt;^&lt;/code&gt;메타 문자를 문자열 전체의 처음이 아니라 각 라인의 처음으로 인식시키고 싶은 경우가 있을 것입니다. 이럴 때 사용하는 옵션이 바로 &lt;code class=&quot;language-text&quot;&gt;re.MULTILINE&lt;/code&gt; 옵션입니다.&lt;/p&gt;
&lt;p&gt;위 코드에 &lt;code class=&quot;language-text&quot;&gt;re.MULTILINE&lt;/code&gt;옵션을 추가해보겠습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;import re
p = re.compile(&quot;^python\s\w+&quot;, re.MULTILINE)

data = &quot;&quot;&quot;python one
life is too short
python two
you need python
python three&quot;&quot;&quot;

print(p.findall(data))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;결과&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;[&amp;#39;python one&amp;#39;, &amp;#39;python two&amp;#39;, &amp;#39;python three&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;다음과 같이 &lt;code class=&quot;language-text&quot;&gt;^&lt;/code&gt;메타 문자가 문자열의 각 줄마다 적용되는 것을 확인할 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;VERBOSE, X&lt;/h3&gt;
&lt;p&gt;여태 봐왔듯이 정규식은 굉장히 가독성이 안좋은 것을 알 수 있습니다. 이런 정규식의 가독성을 조금이나마 해결해주기 위한 옵션이 바로 &lt;code class=&quot;language-text&quot;&gt;VERBOSE&lt;/code&gt;입니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;charref = re.compile(r&amp;#39;&amp;amp;[#](0[0-7]+|[0-9]+|x[0-9a-fA-F]+);&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;charref = re.compile(r&amp;quot;&amp;quot;&amp;quot;
 &amp;amp;[#]                # Start of a numeric entity reference
 (
     0[0-7]+         # Octal form
   | [0-9]+          # Decimal form
   | x[0-9a-fA-F]+   # Hexadecimal form
 )
 ;                   # Trailing semicolon
&amp;quot;&amp;quot;&amp;quot;, re.VERBOSE)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;첫 번째와 두 번째 예를 비교해보면 패턴 객체인 &lt;code class=&quot;language-text&quot;&gt;charref&lt;/code&gt;는 모두 동일한 역할을 합니다. 하지만 두번째처럼 주석을 적고 여러 줄로 표현하는 것이 가독성이 좋은 것을 알 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;re.VERBOSE&lt;/code&gt;옵션은 문자열에 사용된 whitespace가 컴파일시 제거되며, #을 이용해 주석문을 달 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h2&gt;백슬래시 문제&lt;/h2&gt;
&lt;p&gt;정규표현식을 파이썬에서 사용할 때 혼란을 주는 요소가 있습니다. 바로 백슬래시인데요.&lt;/p&gt;
&lt;p&gt;예를들어 &lt;code class=&quot;language-text&quot;&gt;\section&lt;/code&gt; 문자열을 찾기 위한 정규식을 만든다고 가정해봅시다.&lt;/p&gt;
&lt;p&gt;우리의 의도와 달리 &lt;code class=&quot;language-text&quot;&gt;\s&lt;/code&gt;문자가 whitespace로 해석되어 의도한 대로 매치가 이루어지지 않습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;[ \t\n\r\f\v]ection&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;즉 이 것과 같은 의미를 가지게 됩니다.&lt;/p&gt;
&lt;p&gt;우리가 의도한 결과를 얻기 위해선 다음과 같이 변경해줘야 됩니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;\\section&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;그런데 파이썬에서는 파이썬 문자열 리터럴 규칙에 따라 &lt;code class=&quot;language-text&quot;&gt;\\&lt;/code&gt;이 &lt;code class=&quot;language-text&quot;&gt;\&lt;/code&gt;로 변경되어 &lt;code class=&quot;language-text&quot;&gt;\section&lt;/code&gt;이 전달됩니다.&lt;/p&gt;
&lt;p&gt;따라서 우리가 원하는 결과를 얻기위해서 무려 &lt;code class=&quot;language-text&quot;&gt;\\\\&lt;/code&gt;처럼 백슬래시를 4개나 사용해야됩니다.&lt;/p&gt;
&lt;p&gt;이러한 문제를 해결하기 위해 파이썬 정규식에는 Raw String 규칙이 생겨났습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;p = re.compile(r&amp;#39;\\section&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;다음과 같이 &lt;code class=&quot;language-text&quot;&gt;r&lt;/code&gt;을 문자열 앞에 붙혀 Raw String임을 알려줄 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: 그룹&lt;/h3&gt;
&lt;p&gt;그룹화는 말 그대로 그룹으로 묶어주는 것입니다. 지금까지 사용했던 정규식들은 한 문자에만 적용됐습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; re.findall(&amp;#39;12+&amp;#39;, &amp;#39;12 1212 1222&amp;#39;)
[&amp;#39;12&amp;#39;, &amp;#39;12&amp;#39;, &amp;#39;12&amp;#39;, &amp;#39;1222&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;1212&lt;/code&gt;와 같은 문자를 찾고 싶은데, &lt;code class=&quot;language-text&quot;&gt;12&lt;/code&gt;혹은 &lt;code class=&quot;language-text&quot;&gt;1222&lt;/code&gt;만 찾아지는 경우입니다. 즉 메타문자 &lt;code class=&quot;language-text&quot;&gt;+&lt;/code&gt;가 &lt;code class=&quot;language-text&quot;&gt;2&lt;/code&gt;에만 적용된 것입니다.&lt;/p&gt;
&lt;p&gt;만약 우리가 &lt;code class=&quot;language-text&quot;&gt;12&lt;/code&gt;가 반복되는 문자를 찾고싶다면 &lt;code class=&quot;language-text&quot;&gt;12&lt;/code&gt;를 소괄호로 그룹화 시켜주면 됩니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;print(re.match(&amp;#39;(12)+&amp;#39;, &amp;#39;1212&amp;#39;))
print(re.search(&amp;#39;(12)+&amp;#39;, &amp;#39;1212&amp;#39;))
print(re.findall(&amp;#39;(12)+&amp;#39;, &amp;#39;1212&amp;#39;))
print(re.fullmatch(&amp;#39;(12)+&amp;#39;, &amp;#39;1212&amp;#39;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;결과&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;lt;_sre.SRE_Match object; span=(0, 4), match=&amp;#39;1212&amp;#39;&amp;gt;
&amp;lt;_sre.SRE_Match object; span=(0, 4), match=&amp;#39;1212&amp;#39;&amp;gt;
[&amp;#39;12&amp;#39;]
&amp;lt;_sre.SRE_Match object; span=(0, 4), match=&amp;#39;1212&amp;#39;&amp;gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;우리가 원하는 &lt;code class=&quot;language-text&quot;&gt;1212&lt;/code&gt;를 잘 찾은 것을 볼 수 있습니다.
그런데 re.findall의 결과가 이상합니다. 어째서 &lt;code class=&quot;language-text&quot;&gt;[&apos;12&apos;]&lt;/code&gt;가 나온 것일까요??&lt;/p&gt;
&lt;p&gt;이는 바로 괄호가 가진 다른 기능인 캡처 때문입니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: 캡처&lt;/h3&gt;
&lt;p&gt;캡처란 원하는 부분만을 추출하고 싶을 때 사용하는 것입니다. 예를들어 &lt;code class=&quot;language-text&quot;&gt;yyyy-mm-dd&lt;/code&gt;와 같이 날짜를 나타내는 문자열 중 월, 일을 각각 따로 빼서 쓰고 싶다면 &lt;code class=&quot;language-text&quot;&gt;mm&lt;/code&gt;과 &lt;code class=&quot;language-text&quot;&gt;dd&lt;/code&gt;부분에 캡처기능을 사용하면 됩니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;print(re.findall(&amp;#39;\d{4}-(\d\d)-(\d\d)&amp;#39;, &amp;#39;2028-07-28&amp;#39;))
print(re.findall(&amp;#39;\d{4}-(\d\d)-(\d\d)&amp;#39;, &amp;#39;1999/05/21 2018-07-28 2019.01.01&amp;#39;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;결과&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;[(&amp;#39;07&amp;#39;, &amp;#39;28&amp;#39;)]
[(&amp;#39;07&amp;#39;, &amp;#39;28&amp;#39;)]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;정규식의 내용과 일치하고, 월과 일에 해당하는 부분만 따로 빠졌음을 알 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;groups()&lt;/h3&gt;
&lt;p&gt;기존에 match객체에서 일치되는 문자열을 반환하는 &lt;code class=&quot;language-text&quot;&gt;group()&lt;/code&gt;메서드를 알아봤었습니다.
&lt;code class=&quot;language-text&quot;&gt;group()&lt;/code&gt;과는 다르게 &lt;code class=&quot;language-text&quot;&gt;groups()&lt;/code&gt;메서드는 명시적으로 캡처(&lt;code class=&quot;language-text&quot;&gt;()&lt;/code&gt;로 감싼 부분)한 부분을 반환합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; m = re.search(&amp;#39;\d{4}-(\d?\d)-(\d?\d)&amp;#39;, &amp;#39;1868-12-10&amp;#39;)
&amp;gt;&amp;gt;&amp;gt; print(m)
&amp;lt;_sre.SRE_Match object; span=(0, 10), match=&amp;#39;1868-12-10&amp;#39;&amp;gt;
&amp;gt;&amp;gt;&amp;gt; print(m.group())
1868-12-10
&amp;gt;&amp;gt;&amp;gt; print(m.groups())
(&amp;#39;12&amp;#39;, &amp;#39;10&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;이처럼 캡처시킨 부분을 반환시키는 것을 볼 수 있습니다. 또한 &lt;code class=&quot;language-text&quot;&gt;group()&lt;/code&gt;메서드로 부터 캡처된 내용을 추출할 수도 있습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;m.group(): 1868-12-10
m.group(0): 1868-12-10
m.group(1): 12
m.group(2): 10
m.groups(): (&amp;#39;12&amp;#39;, &amp;#39;10&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;h3&gt;비 캡처 그룹&lt;/h3&gt;
&lt;p&gt;그렇다면 그룹화를 위해 소괄호를 반드시 써야 되는데, 굳이 캡처하고 싶지는 않을 때가 있습니다. 그럴 경우에 바로 비캡쳐그룹을 사용합니다.&lt;/p&gt;
&lt;p&gt;비캡처그룹은 &lt;code class=&quot;language-text&quot;&gt;(?:&amp;lt;regex&gt;)&lt;/code&gt;와 같이 사용됩니다. 아까 원하는 결과였던 &lt;code class=&quot;language-text&quot;&gt;1212&lt;/code&gt;대신 &lt;code class=&quot;language-text&quot;&gt;12&lt;/code&gt;가 출력됐던 예제를 다시 가져와 적용시켜보겠습니다.&lt;/p&gt;
&lt;p&gt;기존 코드&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(re.findall(&amp;#39;(12)+&amp;#39;, &amp;#39;1212&amp;#39;))
[&amp;#39;12&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;변경 코드&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(re.findall(&amp;#39;(?:12)+&amp;#39;, &amp;#39;1212&amp;#39;))
[&amp;#39;1212&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;이런 식으로 비 캡처 그룹을 생성시켜 원하는 결과를 얻을 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;(숫자): 앞서 일치된 문자열을 다시 비교&lt;/h3&gt;
&lt;p&gt;앞뒤가 똑같은 세글자 단어를 찾아봅시다. ex)토마토, ABA&lt;/p&gt;
&lt;p&gt;이를 위해선 조금 전 사용했던 캡처가 꼭 필요합니다. 하지만 캡처된 문자열을 접근하기 위해선 한번의 match 과정이 필요했습니다.&lt;/p&gt;
&lt;p&gt;하지만 정규식 내에서 캡처된 그룹에 접근하는 방법도 존재합니다. 바로 &lt;code class=&quot;language-text&quot;&gt;\(숫자)&lt;/code&gt;입니다.
&lt;code class=&quot;language-text&quot;&gt;\ &lt;/code&gt;이후에 등장하는 숫자번호(N)가 바로 캡처 그룹의 번호로 즉 N번째의 그룹을 재참조한다는 의미입니다.&lt;/p&gt;
&lt;p&gt;예시를 하나 살펴보겠습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(re.search(r&amp;#39;(\w)\w\1&amp;#39;, &amp;#39;토마토 ABC aba xyxy &amp;#39;).group())
토마토
&amp;gt;&amp;gt;&amp;gt; print(re.findall(r&amp;#39;(\w)\w\1&amp;#39;, &amp;#39;토마토 ABC aba xyxy &amp;#39;))
[&amp;#39;토&amp;#39;, &amp;#39;a&amp;#39;, &amp;#39;x&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;첫번째는 우리가 원하는 답이지만 search는 하나밖에 찾지 못하므로 완벽한 답이 아닙니다.&lt;/p&gt;
&lt;p&gt;두번째 또한 &lt;code class=&quot;language-text&quot;&gt;()&lt;/code&gt;에 의해서 캡처 그룹만을 반환돼 우리가 원하지 않는 결과를 보입니다.&lt;/p&gt;
&lt;p&gt;이런 문제를 해결하기 위해서 캡처를 하나 더 만들어 줍니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(re.findall(r&amp;#39;((\w)\w\2)&amp;#39;, &amp;#39;토마토 ABC aba xyxy &amp;#39;))
[&amp;#39;토마토&amp;#39;, &amp;#39;aba&amp;#39;, &amp;#39;xyx&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;여기서 재참조부가 &lt;code class=&quot;language-text&quot;&gt;\2&lt;/code&gt;인 이유는 우리가 참조해야될 &lt;code class=&quot;language-text&quot;&gt;(\w)&lt;/code&gt;그룹이 바깥 괄호로 인해 두번째 그룹으로 변경됐기 때문입니다.
따라서 우리는 &lt;code class=&quot;language-text&quot;&gt;\1&lt;/code&gt;이 아닌 &lt;code class=&quot;language-text&quot;&gt;\2&lt;/code&gt;를 입력해줘야 됩니다.&lt;/p&gt;
&lt;p&gt;이러한 &lt;code class=&quot;language-text&quot;&gt;\1&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;\2&lt;/code&gt;와 같은 것들을 비 명명 그룹이라고 합니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;명명 그룹&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;\(숫자)&lt;/code&gt;와 같은 방법은 간편하지만 눈에 잘 들어오지 않습니다.&lt;/p&gt;
&lt;p&gt;많은 프로그래밍 언어의 정규표현식은 명명 그룹 기능을 지원합니다.
언어마다 쓰는 방법이 다르지만, 파이썬 기준으로는 &lt;code class=&quot;language-text&quot;&gt;(?P&amp;lt;name&gt;regex)&lt;/code&gt;형식으로 씁니다.&lt;/p&gt;
&lt;p&gt;이번에도 예시를 하나 살펴보겠습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;match_obj = re.match(
    r&amp;#39;(?P&amp;lt;year&amp;gt;\d{4})-(?P&amp;lt;month&amp;gt;\d\d)-(?P&amp;lt;day&amp;gt;\d\d) (?P=year)\.(?P=month)\.(?P=day)&amp;#39;,
    &amp;#39;2018-07-28 2018.07.28&amp;#39;)

print(match_obj.group())
print(match_obj.groups())
print(match_obj.group(1))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;결과&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;2018-07-28 2018.07.28
(&amp;#39;2018&amp;#39;, &amp;#39;07&amp;#39;, &amp;#39;28&amp;#39;)
2018&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;이런 식으로 기존 비명명 방식은 그룹의 번호를 지정해줘야 됐지만, 명명그룹은 하나의 변수처럼 그룹을 정의해 재참조해가며 사용할 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: 치환&lt;/h3&gt;
&lt;p&gt;파이썬의 &lt;code class=&quot;language-text&quot;&gt;replace&lt;/code&gt;메서드는 정규식 패턴에 대응하는 문자열을 찾아주지 못합니다. 따라서 &lt;code class=&quot;language-text&quot;&gt;re.sub&lt;/code&gt;메서드가 필요합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(re.sub(pattern=&amp;#39;Gorio&amp;#39;, repl=&amp;#39;Ryan&amp;#39;, count=2, \
             string=&amp;#39;Gorio, Gorio, Gorio keep a straight face.&amp;#39;))
Ryan, Ryan, Gorio keep a straight face.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;re.sub&lt;/code&gt;메서드의 파라미터를 살펴보겠습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;pattern: 매치시킬 패턴 
repl: 변경할 문자
string: 적용 문자열
count: 치환개수(최대값)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;번외로 &lt;code class=&quot;language-text&quot;&gt;re.subn&lt;/code&gt;메서드는 치환된 문자열과 더불어 치환된 개수의 튜플을 반환합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;(&amp;#39;Ryan, Ryan, Gorio keep a straight face.&amp;#39;, 2)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;re.sub&lt;/code&gt;에 이전에 보았던 &lt;code class=&quot;language-text&quot;&gt;\(숫자)&lt;/code&gt;를 이용할 수도 있습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(re.sub(&amp;#39;(\d{4})-(\d{2})-(\d{2})&amp;#39;, 
             r&amp;#39;\1.\2.\3&amp;#39;,
             &amp;#39;1900-01-01&amp;#39;))
1900.01.01&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;yyyy-mm-dd 형식을 yyyy.mm.dd 형식으로 변경됐습니다. 비명명방식이 된다면 명명방식도 가능하겠죠??&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(re.sub(&amp;#39;(?P&amp;lt;year&amp;gt;\d{4})-(?P&amp;lt;month&amp;gt;\d{2})-(?P&amp;lt;day&amp;gt;\d{2})&amp;#39;,
             &amp;#39;\g&amp;lt;year&amp;gt;.\g&amp;lt;month&amp;gt;.\g&amp;lt;day&amp;gt;&amp;#39;,
             &amp;#39;1900-01-01&amp;#39;))
1900.01.01&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: split&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;re.sub&lt;/code&gt;말고도 유용한 함수인 &lt;code class=&quot;language-text&quot;&gt;re.split&lt;/code&gt;입니다. 이 메서드는 파이썬 문자열의 기본 메서드인 &lt;code class=&quot;language-text&quot;&gt;split&lt;/code&gt;과 매우 유사합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;&amp;gt;&amp;gt;&amp;gt; print(re.split(&amp;#39;&amp;lt;[^&amp;lt;&amp;gt;]*&amp;gt;&amp;#39;,
               &amp;#39;&amp;lt;html&amp;gt; Wow &amp;lt;head&amp;gt; header &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; Hey &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt;&amp;#39;))
[&amp;#39;&amp;#39;, &amp;#39; Wow &amp;#39;, &amp;#39; header &amp;#39;, &amp;#39; &amp;#39;, &amp;#39; Hey &amp;#39;, &amp;#39; &amp;#39;, &amp;#39;&amp;#39;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;정규식에 일치하는 부분을 기준으로 split이 되는 것을 볼 수 있습니다.&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: 연산을 섞은 치환&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;re.sub&lt;/code&gt;를 쓸 때 일치부에 나타나지도 않고, literal text에도 나타나지 않는 문자열로 치환하고 싶은 경우가 있을 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;re.sub&lt;/code&gt;는 인자 repl을 받을때 함수로도 받을 수 있습니다. 함수는 인자로 match 객체를 받으며 문자열을 반환해야 합니다.&lt;/p&gt;
&lt;p&gt;예를 들어 소수로 표현된 숫자를 찾은다음 퍼센티지로 변환하는 것을 정규식으로 써보겠습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;def convert_percentage(match_obj):
    number = float(match_obj.group())
    return str(number * 100) + &amp;#39;%&amp;#39;

print(re.sub(pattern=r&amp;#39;\b0\.\d+\b&amp;#39;,
             repl=convert_percentage,
             string=&amp;#39;Red 0.250, Green 0.001, Blue 0.749, Black 1.5&amp;#39;))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;결과&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;Red 25.0%, Green 0.1%, Blue 74.9%, Black 1.5&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;br&gt;
&lt;h3&gt;정규표현식: 조건문&lt;/h3&gt;
&lt;p&gt;정규표현식도 까다롭지만 조건문을 만들어 줄 수 있습니다. 정규표현식에서의 조건문은 다음과 같습니다.&lt;/p&gt;
&lt;p&gt;조건문은, 캡처 그룹을 사용해 앞에서 문자열이 일치되었는지 아닌지에 따라 다음 부분에서는 다른 일치 조건을 제시해야 할 때 쓰입니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;angular2html&quot;&gt;&lt;pre class=&quot;language-angular2html&quot;&gt;&lt;code class=&quot;language-angular2html&quot;&gt;(?(숫자)맞으면|아니면)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;여기서 숫자는 캡처와 비슷합니다. 앞에서 재참조부를 쓸 때 &lt;code class=&quot;language-text&quot;&gt;\1&lt;/code&gt;과 같이 썼는데, 조건문에서는 단지 &lt;code class=&quot;language-text&quot;&gt;(1)&lt;/code&gt;로 바뀐 것입니다.&lt;/p&gt;
&lt;p&gt;예시로, &lt;code class=&quot;language-text&quot;&gt;(a)?b(?(1)c|d)&lt;/code&gt;를 살펴보겠습니다. 이건 &lt;code class=&quot;language-text&quot;&gt;abc|bd&lt;/code&gt;와 같습니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;먼저 &lt;code class=&quot;language-text&quot;&gt;a&lt;/code&gt;를 검사합니다. 만약 찾는 문자열에 ‘a’가 있으면 첫 번째 명시적 캡처에는 ‘a’가 들어가고, ‘a’가 없으면, 빈 문자열이 (1)에 저장됩니다.&lt;/li&gt;
&lt;li&gt;다음으로 &lt;code class=&quot;language-text&quot;&gt;b&lt;/code&gt;입니다. 만약 ‘b’가 없으면 정규식은 일치하지 않습니다. ‘b’가 있다고 가정하겠습니다.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;(?(1)c|d)&lt;/code&gt; 이 조건문은 (1)에 값이 존재하면 c, 존재하지 않으면 d를 뜻합니다. 즉 a가 존재했었다면 &lt;code class=&quot;language-text&quot;&gt;abc&lt;/code&gt;가 완성되고, a가 존재하지 않았다면 &lt;code class=&quot;language-text&quot;&gt;bd&lt;/code&gt;가 완성됩니다.&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
&lt;h1&gt;reference&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://greeksharifa.github.io/&quot;&gt;https://greeksharifa.github.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://wikidocs.net/4308&quot;&gt;https://wikidocs.net/4308&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Streamlit]]></title><description><![CDATA[Streamlit The fastest way to build data apps in Python github repository: https://github.com/streamlit/streamlit tutorial: https://docs…]]></description><link>https://gatsby-casper.netlify.com/streamlit/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/streamlit/</guid><pubDate>Mon, 30 Aug 2021 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Streamlit&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;The fastest way to build data apps in Python&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;img src=&quot;https://github.com/streamlit/streamlit/raw/develop/docs/_static/img/Streamlit_overview.gif&quot; width=&quot;700&quot;&gt;
&lt;ul&gt;
&lt;li&gt;github repository: &lt;a href=&quot;https://github.com/streamlit/streamlit&quot;&gt;https://github.com/streamlit/streamlit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;tutorial: &lt;a href=&quot;https://docs.streamlit.io/en/stable/getting_started.html&quot;&gt;https://docs.streamlit.io/en/stable/getting_started.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;는 파이썬 기반으로 손쉽게 웹 어플리케이션을 개발하는 오픈소스입니다. 웹에 대한 이해 없이도 간단한 파이썬 코드만으로 대부분의 웹 어플리케이션 구현이 가능합니다. 간단한 데이터 Visualization 혹은 인공지능 모델 시연 등 여러 분야에 활용될 수 있을 것 같습니다.&lt;/p&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Install command&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;$ pip install streamlit
$ streamlit hello&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;  👋 Welcome to Streamlit!

  If you&apos;re one of our development partners or you&apos;re interested in getting
  personal technical support or Streamlit updates, please enter your email
  address below. Otherwise, you may leave the field blank.

  Email: sh951011@gmail.com

  Privacy Policy:
  As an open source project, we collect usage statistics. We cannot see and do
  not store information contained in Streamlit apps. You can find out more by
  reading our privacy policy at: https://streamlit.io/privacy-policy

  If you&apos;d like to opt out of usage statistics, add the following to
  ~/.streamlit/config.toml, creating that file if necessary:

    [browser]
    gatherUsageStats = false


  Welcome to Streamlit. Check out our demo in your browser.

  Local URL: http://localhost:8501
  Network URL: http://192.168.35.186:8501

  Ready to create your own Python apps super quickly?
  Head over to https://docs.streamlit.io

  May you create awesome apps!&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Get Started&lt;/h2&gt;
&lt;h3&gt;Create your first Streamlit app&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;first_app.py&lt;/code&gt;를  만들고 streamlit를 import 합니다.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; streamlit &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; st
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; pd&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;pandas&lt;/code&gt;는 데이터프레임을 다루는 예시를 보여주기 위해 import 합니다.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;st.write()&lt;/code&gt;을 사용합니다.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;title&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;My first app&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;write&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Here&apos;s our first attempt at using data to create a table:&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;write&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;pd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;DataFrame&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;token string&quot;&gt;&apos;first column&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token string&quot;&gt;&apos;second column&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;st.write()&lt;/code&gt;는 &lt;code class=&quot;language-text&quot;&gt;str&lt;/code&gt; 타입뿐만 아니라 &lt;code class=&quot;language-text&quot;&gt;pandas&lt;/code&gt;의 &lt;code class=&quot;language-text&quot;&gt;DataFrame&lt;/code&gt;도 손쉽게 write 할 수 있습니다.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Run by streamlit&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;$ streamlit run first_app.py&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/132123087-47e303cc-bdaf-4550-ab4a-d802807c3cb9.png&quot; width=&quot;600&quot;&gt;
&lt;p&gt;여기까지만 해도 &lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;을 활용하면 상당히 편리하게 데이터를 보여줄 수 있습니다. 다음으로는 &lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;을 이용해서 데이터 visualization하는 방법에 대해 살펴보겠습니다.&lt;/p&gt;
&lt;h3&gt;Draw charts and maps&lt;/h3&gt;
&lt;p&gt;웹 어플리케이션에 bar chart, line chart, map을 추가하는 법을 대해 배워봅시다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Draw a line chart&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;파이썬에서는 &lt;code class=&quot;language-text&quot;&gt;matplotlib&lt;/code&gt;과 같은 라이브러리를 이용하면 쉽게 line chart를 그릴 수 있습니다만, 웹 어플리케이션에서 이런 이미지를 띄우려면 상황에 따라 복잡해질 수도 있습니다. 하지만 &lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;에서는 &lt;code class=&quot;language-text&quot;&gt;st.line_chart()&lt;/code&gt; 메서드로 편리하게 line chart를 추가할 수 있습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;chart_data &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;DataFrame&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
     np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;randn&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
     columns&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;a&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;b&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;c&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;line_chart&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;chart_data&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/132123628-b96bdf1c-caf1-4821-89ca-2a5ecaa9ee1a.png&quot; width=&quot;400&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Plot a map&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;지도 역시 마찬가지입니다. 노 베이스로 지도를 그린다면 굉장히 어려운 프로젝트입니다만, &lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;에서는 지도 역시 &lt;code class=&quot;language-text&quot;&gt;st.map()&lt;/code&gt; 메서드로 편리하게 지원합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;map_data &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;DataFrame&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;randn&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;37.76&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;122.4&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    columns&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;lat&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;lon&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;map_data&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/132123664-f8b4cc20-99b2-4045-b9a7-188f7fed5b22.png&quot; width=&quot;400&quot;&gt;
&lt;h3&gt;Add interactivity with widgets&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;은 체크박스, 버튼, 슬라이더 등 여러 interactive 위젯 API를 제공합니다. &lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;에서 제공하는 모든 API는 &lt;a href=&quot;https://docs.streamlit.io/en/stable/api.html&quot;&gt;여기&lt;/a&gt;에서 확인할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Checkbox show/hide data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;st.checkbox()&lt;/code&gt;를 이용하면 체크박스를 이용해서 데이터 show/hide 설정을 할 수 있습니다. &lt;code class=&quot;language-text&quot;&gt;st.checkbox()&lt;/code&gt;는 위젯명을 argument로 받아서 처리합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;checkbox&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;Show dataframe&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    chart_data &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; pd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;DataFrame&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
       np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;randn&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
       columns&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;a&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;b&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;c&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    chart_data&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/132123949-a1b434bb-4d28-4fa2-8431-9e88eb9ee372.png&quot; width=&quot;250&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Selectbox for options&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;st.selectbox()&lt;/code&gt;는 &lt;code class=&quot;language-text&quot;&gt;pandas.Series&lt;/code&gt;를 입력으로 받아서 옵션을 선택받게 할 수 있습니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; streamlit &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; st
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; pd

st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;title&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;TUNiBerse&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
option &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;selectbox&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;token string&quot;&gt;&apos;당신의 직책을 선택해주세요.&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
     pd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Series&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;CEO&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;AI Engineer&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;Intern&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;Product Manager&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token string&quot;&gt;&apos;You selected: &apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; option&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/132124085-9ebe7fe4-9e73-434b-82f0-d6e0c34d7c32.png&quot; width=&quot;600&quot;&gt;
&lt;h3&gt;Lay out your app&lt;/h3&gt;
&lt;p&gt;웹 어플리케이션을 구현할 때 중요한 점 중 하나를 레이아웃입니다. 어떻게 화면을 구성하냐에 따라서 더 깔끔하고 직관적이게 보일 수가 있습니다. &lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;은 이런 기능 또한 손쉽게 다룰 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Selectbox를 사이드로&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;방금 앞에서 했던 selectbox를 사이드로 옮기고 싶다면 어떻게 해야할까요? &lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;에서는 아래와 같이 간단하게 구현 가능합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; streamlit &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; st
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; pandas &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; pd

st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;title&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;TUNiBerse&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
option &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;sidebar&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;selectbox&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;token string&quot;&gt;&apos;당신의 직책을 선택해주세요.&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
     pd&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Series&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;CEO&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;AI Engineer&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;Intern&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;Product Manager&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token string&quot;&gt;&apos;You selected: &apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; option&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/132124325-2436bb57-85d4-4be4-b90a-41968ecce758.png&quot; width=&quot;600&quot;&gt;
&lt;p&gt;위와 같이 &lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;에서 제공하는 대부분의 element는 &lt;code class=&quot;language-text&quot;&gt;st.sidebar. [element_name]()&lt;/code&gt; 포맷으로 사용 가능합니다. 위의 위젯 외에도 button, expander 등 여러 위젯이 있으니 다양하게 사용해보시길 바랍니다.&lt;/p&gt;
&lt;h3&gt;Show progress&lt;/h3&gt;
&lt;p&gt;이번에는 웹페이지에 진행현황을 표시해봅시다. &lt;code class=&quot;language-text&quot;&gt;st.progress()&lt;/code&gt;를 이용하면 아래처럼 쉽게 사용이 가능합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; time

&lt;span class=&quot;token string&quot;&gt;&apos;Starting a long computation...&apos;&lt;/span&gt;

&lt;span class=&quot;token comment&quot;&gt;# Add a placeholder&lt;/span&gt;
latest_iteration &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;empty&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
bar &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; st&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;progress&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;token comment&quot;&gt;# Update the progress bar with each iteration.&lt;/span&gt;
  latest_iteration&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;text&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&apos;Iteration &lt;/span&gt;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;i&lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  bar&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;progress&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;i &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  time&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;sleep&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token string&quot;&gt;&apos;...and now we\&apos;re done!&apos;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/132124609-1dc526b4-5e3c-438b-8e38-4374aa6739f2.png&quot; width=&quot;600&quot;&gt;
&lt;h2&gt;Share your app&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;streamlit&lt;/code&gt;으로 개발한 app은 &lt;strong&gt;Streamlit Cloud&lt;/strong&gt;로 deploy, manage, share가 모두 가능합니다. 현재 Streamlit Cloud는 초대를 받은 멤버에 한해서 사용이 가능합니다. &lt;a href=&quot;https://streamlit.io/sharing-sign-up&quot;&gt;Request an invite&lt;/a&gt;에 몇 가지 사항을 제출하고 사용해주시면 됩니다.&lt;/p&gt;
&lt;p&gt;다음 3 스텝으로 간단하게 구현 가능합니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Put your app in a public Github repo (and make sure it has a requirements.txt!)&lt;/li&gt;
&lt;li&gt;Sign into share.streamlit.io&lt;/li&gt;
&lt;li&gt;Click ‘Deploy an app’ and then paste in your GitHub URL&lt;/li&gt;
&lt;/ol&gt;</content:encoded></item><item><title><![CDATA[Hugging Face Tokenizers]]></title><description><![CDATA[최근 NLP 토크나이저를 만드는데 가장 많이 사용되는  라이브러와 실제 사용이 가장 많이 되는  라이브러리로의 변환에 대한 코드를 담고 있습니다. 해당 내용은  버젼에서 수행되었습니다. Train 아래 코드는 wordpiece, char-bpe…]]></description><link>https://gatsby-casper.netlify.com/tokenizers/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/tokenizers/</guid><pubDate>Wed, 11 Aug 2021 15:11:55 GMT</pubDate><content:encoded>&lt;p&gt;최근 NLP 토크나이저를 만드는데 가장 많이 사용되는 &lt;code class=&quot;language-text&quot;&gt;tokenizers&lt;/code&gt; 라이브러와 실제 사용이 가장 많이 되는 &lt;code class=&quot;language-text&quot;&gt;transformers&lt;/code&gt; 라이브러리로의 변환에 대한 코드를 담고 있습니다. 해당 내용은 &lt;code class=&quot;language-text&quot;&gt;tokenizers==0.10.3&lt;/code&gt; 버젼에서 수행되었습니다.&lt;/p&gt;
&lt;h3&gt;Train&lt;/h3&gt;
&lt;p&gt;아래 코드는 wordpiece, char-bpe, byte-level bpe 방식을 지원합니다.&lt;br&gt;
센텐스피스의 경우 tokenizers에는 현재 sentencepiece가 불안정하여, 오리지널 sentencepiece 학습 방식으로 학습 후 &lt;code class=&quot;language-text&quot;&gt;tokenizers.SentencePieceUnigramTokenizer.from_spm()&lt;/code&gt; 메서드로 로드하는 방식을 사용하셔야 합니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; os
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; argparse
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tokenizers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    BertWordPieceTokenizer&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; 
    ByteLevelBPETokenizer&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; 
    CharBPETokenizer&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; 
    SentencePieceUnigramTokenizer&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

parser &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; argparse&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ArgumentParser&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
parser&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;add_argument&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;--corpus_file&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; required&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
parser&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;add_argument&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;--vocab_size&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; default&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; required&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
parser&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;add_argument&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;--limit_alphabet&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; default&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
parser&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;add_argument&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;--tokenizer&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; default&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;sentencepiece&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
parser&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;add_argument&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;--model_type&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; default&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;gpt&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
parser&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;add_argument&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;--save_dir&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; default&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;sp&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
args &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; parser&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;parse_args&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;model_type &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;bert&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    special_tokens &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|sos|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|eos|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|pad|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|unk|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|mask|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|sep|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|cls|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;elif&lt;/span&gt; args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;model_type &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;gpt&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    special_tokens &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|endoftext|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;&amp;lt;|unk|&gt;&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;raise&lt;/span&gt; ValueError&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;Unsupported model type: &lt;/span&gt;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;model_type&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;wordpiece&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    tokenizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; BertWordPieceTokenizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
        clean_text&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        handle_chinese_chars&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        strip_accents&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        lowercase&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        wordpieces_prefix&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;##&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;train&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;files&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;corpus_file&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; limit_alphabet&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;limit_alphabet&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; vocab_size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;elif&lt;/span&gt; args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;bbpe&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    tokenizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; ByteLevelBPETokenizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
        lowercase&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        add_prefix_space&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        unicode_normalizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;nfc&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;train&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
        files&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;corpus_file&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        vocab_size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        special_tokens&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;special_tokens&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;elif&lt;/span&gt; args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;lower&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&apos;char-bpe&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    tokenizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; CharBPETokenizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
        lowercase&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        add_prefix_space&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        unicode_normalizer&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;nfc&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;train&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
        files&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;corpus_file&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        vocab_size&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        special_tokens&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;special_tokens&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;raise&lt;/span&gt; ValueError&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;Error&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;token keyword&quot;&gt;not&lt;/span&gt; os&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;path&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;exists&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save_dir&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    os&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;mkdir&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;args&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save_dir&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;tokenizer.json&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Convert to transformers tokenizer&lt;/h3&gt;
&lt;p&gt;&lt;code class=&quot;language-text&quot;&gt;tokenizers&lt;/code&gt; 라이브러리로 학습한 토크나이저는 아래 코드로 간단하게 &lt;code class=&quot;language-text&quot;&gt;transformers&lt;/code&gt; 토크나이저로 변환이 가능합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python3&quot;&gt;&lt;pre class=&quot;language-python3&quot;&gt;&lt;code class=&quot;language-python3&quot;&gt;from transformers import PreTrainedTokenizerFast

tokenizer = PreTrainedTokenizerFast(tokenizer_file=&amp;#39;tokenizer.json&amp;#39;)
tokenizer.save_pretrained(&amp;#39;SAVE_DIR&amp;#39;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;sentencepiece&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; sentencepiece &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; spm

SENTENCEPIECE_MODEL_PREFIX &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;SP&quot;&lt;/span&gt;
SENTENCEPIECE_MODEL_TYPE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token string&quot;&gt;&quot;unigram&quot;&lt;/span&gt;

spm&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;SentencePieceTrainer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Train&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--input=sentencepiece_input.txt &quot;&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--model_prefix=&lt;/span&gt;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;SENTENCEPIECE_MODEL_PREFIX&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt; &quot;&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--vocab_size=&lt;/span&gt;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt; &quot;&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--model_type=&lt;/span&gt;&lt;span class=&quot;token interpolation&quot;&gt;&lt;span class=&quot;token punctuation&quot;&gt;{&lt;/span&gt;SENTENCEPIECE_MODEL_TYPE&lt;span class=&quot;token punctuation&quot;&gt;}&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt; &quot;&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--pad_id=0 &quot;&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--bos_id=1 &quot;&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--eos_id=2 &quot;&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--unk_id=3 &quot;&lt;/span&gt;&lt;/span&gt;
        &lt;span class=&quot;token string-interpolation&quot;&gt;&lt;span class=&quot;token string&quot;&gt;f&quot;--user_defined_symbols=[SEP],[CLS],[MASK]&quot;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Add post process&lt;/h3&gt;
&lt;p&gt;토크나이저를 만들게 되면 따로 설정해주지 않으면 아래와 같은 형태로 토크나이징이 진행됩니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; transformers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; AutoTokenizer

tokenizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; AutoTokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;from_pretrained&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;SAVE_DIR&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;튜닙은 자연어처리 테크 스타트업이다.&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;output&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[&apos;▁&apos;, &apos;튜&apos;, &apos;닙&apos;, &apos;은&apos;, &apos;▁자연&apos;, &apos;어&apos;, &apos;처&apos;, &apos;리&apos;, &apos;▁테&apos;, &apos;크&apos;, &apos;▁&apos;, &apos;스타&apos;, &apos;트&apos;, &apos;업&apos;, &apos;이다.&apos;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;보통 버트나 일렉트라 같은 모델들은 파인튜닝의 편리함을 위해 아래와 같은 포맷으로 문장이 토크나이징 됩니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[&apos;[CLS]&apos;, &apos;▁&apos;, &apos;튜&apos;, &apos;닙&apos;, &apos;은&apos;, &apos;▁자연&apos;, &apos;어&apos;, &apos;처&apos;, &apos;리&apos;, &apos;▁테&apos;, &apos;크&apos;, &apos;▁&apos;, &apos;스타&apos;, &apos;트&apos;, &apos;업&apos;, &apos;이다.&apos;, &apos;[SEP]&apos;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;이러한 포맷으로 자동으로 토크나이징 해주기 위해서는 &lt;code class=&quot;language-text&quot;&gt;post_processor&lt;/code&gt;를 추가해주면 됩니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tokenizers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; SentencePieceUnigramTokenizer
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; tokenizers&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;processors &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; TemplateProcessing
&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; transformers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; PreTrainedTokenizerFast
        
tokenizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; SentencePieceUnigramTokenizer&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;vocab&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;      

&lt;span class=&quot;token comment&quot;&gt;# &apos;[CLS] SENTENCE [SEP]&apos; format&lt;/span&gt;
tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;post_processor &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; TemplateProcessing&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    single&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;[CLS] $A [SEP]&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    pair&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    special_tokens&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;[CLS]&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;token_to_id&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;[CLS]&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;[SEP]&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;token_to_id&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;[SEP]&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

tokenizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; PreTrainedTokenizerFast&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;tokenizer_object&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;tokenizer&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;save_pretrained&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;SAVE_DIR&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;이렇게 저장한 &lt;code class=&quot;language-text&quot;&gt;transformers&lt;/code&gt; 토크나이저를 아까와 똑같이 로드해서 사용해주면 버트 포맷의 인풋으로 토크나이징 됩니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;from&lt;/span&gt; transformers &lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; AutoTokenizer

tokenizer &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; AutoTokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;from_pretrained&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;SAVE_DIR&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
tokenizer&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;tokenize&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&apos;튜닙은 자연어처리 테크 스타트업이다.&apos;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;output&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[&apos;[CLS]&apos;, &apos;▁&apos;, &apos;튜&apos;, &apos;닙&apos;, &apos;은&apos;, &apos;▁자연&apos;, &apos;어&apos;, &apos;처&apos;, &apos;리&apos;, &apos;▁테&apos;, &apos;크&apos;, &apos;▁&apos;, &apos;스타&apos;, &apos;트&apos;, &apos;업&apos;, &apos;이다.&apos;, &apos;[SEP]&apos;]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content:encoded></item><item><title><![CDATA[Efficient Attention Paper Review]]></title><description><![CDATA[Efficient Attention: Attention with Linear Complexities Shen Zhuoran et al. Abstract Dot-product attention은 들어오는 인풋 길이에 따라 memory…]]></description><link>https://gatsby-casper.netlify.com/efficient-attention/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/efficient-attention/</guid><pubDate>Sat, 17 Jul 2021 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Efficient Attention: Attention with Linear Complexities&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Shen Zhuoran et al.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dot-product attention은 들어오는 인풋 길이에 따라 memory &amp;#x26; computation cost가 quadratically하게 증가함&lt;/li&gt;
&lt;li&gt;어텐션 매커니즘을 조금 수정해서 memory &amp;#x26; computation cost를 상당히 줄이는 방법 제안&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Method&lt;/h2&gt;
&lt;img src=&quot;https://www.pragmatic.ml/content/images/2020/06/image-13.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;기존 Dot-product로 similarty를 구하는 방식과 다르게, Key와 value를 곱하는 방식 사용&lt;/li&gt;
&lt;li&gt;Dot-product:&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/121996703-0da3ac80-cde4-11eb-9870-e710b6b13c53.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Efficient:&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/121996782-2f9d2f00-cde4-11eb-8c73-823f775a42f7.png&quot;&gt;
&lt;h2&gt;Experiment&lt;/h2&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/121996832-4774b300-cde4-11eb-8050-b0f7e00f343d.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;기존 attention과 제안된 attention 비교 =&gt; 상당히 효율적으로 변한것을 확인 가능&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/121997009-90c50280-cde4-11eb-9387-4b4819fcb251.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;성능 면에서도 더 좋은 결과가 나왔다는 표&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[2021 AI 온라인 경진대회 1위]]></title><description><![CDATA[2021 AI 온라인 경진대회 1위 이번에 열린 2021 인공지능 온라인 경진대회 대화 감성 분류 태스크에 회사 대표로 참가 Public / Private / Final 리더보드에서 모두…]]></description><link>https://gatsby-casper.netlify.com/2021_ai_online_competition/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/2021_ai_online_competition/</guid><pubDate>Mon, 12 Jul 2021 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;2021 AI 온라인 경진대회 1위&lt;/h1&gt;
&lt;img src=&quot;https://aihub.or.kr/sites/default/files/inline-images/2021%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%20%EC%98%A8%EB%9D%BC%EC%9D%B8%20%EA%B2%BD%EC%A7%84%EB%8C%80%ED%9A%8C_%ED%8F%AC%EC%8A%A4%ED%84%B0%5B%EC%B5%9C%EC%A2%85%5D.jpg&quot; width=&quot;400&quot;&gt;
&lt;p&gt;이번에 열린 2021 인공지능 온라인 경진대회 대화 감성 분류 태스크에 회사 대표로 참가 Public / Private / Final 리더보드에서 모두 1위를 기록했습니다.  🎉 🎉&lt;/p&gt;
&lt;img src=&quot;https://scontent-gmp1-1.xx.fbcdn.net/v/t1.6435-9/207960577_2961737444070510_5324504568877735835_n.jpg?_nc_cat=103&amp;amp;ccb=1-5&amp;amp;_nc_sid=8bfeb9&amp;amp;_nc_ohc=928vWdPoSNsAX8QP_is&amp;amp;_nc_ht=scontent-gmp1-1.xx&amp;amp;oh=d4024e0788f8bbce0556445927021c6e&amp;amp;oe=616EF7B0&quot; width=&quot;600&quot;&gt;
&lt;p&gt;회사 대표님인 박규병님과 라이징 스타 고현웅님이 제 디스커션 파트너로 참여해주셨고, 학습 및 대회 진행은 저 혼자 진행했습니다.&lt;/p&gt;
&lt;p&gt;사업화와 관련이 있고 우수기업 선정시 최소 1억에서 최대 3억까지 지원해주는 큰 컴피티션이였기 때문에 약 10일 정도의 대회 기간 동안 대회에 몰입해서 임했습니다.&lt;/p&gt;
&lt;p&gt;해당 대회 종료 후 1위 노하우에 대해 발표했습니다. 해당 내용은 아래 링크에서 확인하실 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=aKKDvdel5O4&amp;#x26;t=1168s&quot;&gt;https://www.youtube.com/watch?v=aKKDvdel5O4&amp;#x26;t=1168s&lt;/a&gt;&lt;/p&gt;
&lt;img src=&quot;https://tunib.notion.site/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F39314fcb-c739-42e5-8479-f39650b2efae%2FiOS_%EC%9D%B4%EB%AF%B8%EC%A7%80.jpg?table=block&amp;amp;id=cdb4f535-efb8-49b7-902b-4bc10097ca72&amp;amp;spaceId=d2222c9a-a58e-4735-80fc-abd873fd9b70&amp;amp;width=3070&amp;amp;userId=&amp;amp;cache=v2&quot; width=&quot;500&quot;&gt;</content:encoded></item><item><title><![CDATA[Luna: Linear Unified Nested Attention]]></title><description><![CDATA[Luna: Linear Unified Nested Attention USC + CMU + Facebook AI 2021.06 code Abstract 트랜스포머의 Multi Headed Self Attention…]]></description><link>https://gatsby-casper.netlify.com/luna/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/luna/</guid><pubDate>Sat, 03 Jul 2021 23:46:37 GMT</pubDate><content:encoded>&lt;h1&gt;Luna: Linear Unified Nested Attention&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;USC + CMU + Facebook AI&lt;/li&gt;
&lt;li&gt;2021.06&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/XuezheMax/fairseq-apollo&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;트랜스포머의 Multi Headed Self Attention은 시퀀스가 길어질수록 메모리, 시간복잡도가 quadratic함&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이를 linear하게 바꾸기 위한 시도로 Luna (Linear Unified Nested Attention) 을 제안&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;핵심은 인풋을 고정 길이로 변환한다는 점과 attention을 2개로 분리한다는 점.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Positional Embedding을 별도의 고정 길이 query로 뺌.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;속도는 Performer랑 비슷한데 시퀀스가 길어질수록 좀 더 효과적이고 메모리도 적게 쓴다고 함.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;정확도 성능은 경쟁력이 높은 편.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Attention&lt;/h2&gt;
&lt;h3&gt;Traditional attention mechanism:&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127510622-5af08d8d-771c-4bb9-8e9e-53bb3f4cf85e.png&quot; height=&quot;70&quot;&gt;
&lt;ul&gt;
&lt;li&gt;X: Query sequence, C: Context sequence&lt;/li&gt;
&lt;li&gt;ω: activation function (usually softmax)&lt;/li&gt;
&lt;li&gt;Q = XW&lt;sub&gt;Q&lt;/sub&gt;, K = CW&lt;sub&gt;K&lt;/sub&gt;, V = CW&lt;sub&gt;V&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;Self attention에서는 X == C&lt;/li&gt;
&lt;li&gt;공간 &amp;#x26; 시간복잡도: O(nm) (n: X의 시퀀스 길이, m: C의 시퀀스 길이)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Linear Unified Nested Attention (LUNA)&lt;/h2&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127514004-0ca02332-8ef5-4563-b9b4-bd9bf6bb72b9.png&quot; height=&quot;400&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Goal: Attention mechanism’s complexity &lt;strong&gt;quadratic =&gt; linear&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Luna (Pack and Unpack Attention)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;이 어텐션의 핵심은 어텐션을 2개로 쪼개는 것.&lt;/li&gt;
&lt;li&gt;O(ln) (l은 P의 길이)&lt;/li&gt;
&lt;li&gt;Pack Attention:
&lt;ul&gt;
&lt;li&gt;Query를 learnable paramter인 P로 대체 (고정 길이, 길이에 대한 실험 있음)&lt;/li&gt;
&lt;li&gt;상대적으로 더 짧은 Query를 놓음으로써 complexity를 줄임&lt;/li&gt;
&lt;li&gt;P-contextual: 첫 레이어의 P는 learnable parameter이며 다음 레이어로 전달&lt;/li&gt;
&lt;li&gt;P-non-contextual: 각 레이어마다 P를 학습하고 다음 레이어로 넘기지 않음&lt;/li&gt;
&lt;li&gt;contextual &amp;#x26; non-contextual에 대한 실험은 뒤에 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127603299-44234ff8-1735-4dc2-95b0-bc8f66bfbbde.png&quot; height=&quot;60&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Unpack Attention:
&lt;ul&gt;
&lt;li&gt;Pack Attention의 결과인 &lt;code class=&quot;language-text&quot;&gt;packed context&lt;/code&gt;를 Key와 Value로 사용. Query는 기존 query.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127603352-dfb741df-ec40-4b49-8bdf-b158e55b771e.png&quot; height=&quot;70&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Luna Attention:&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127603409-554fa551-e89d-46ee-aec4-9a471a5cdc8f.png&quot; height=&quot;70&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Luna Layer&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127603246-192380b0-fde6-4941-b060-1bf639d3b8e7.png&quot; height=&quot;100&quot;&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;h3&gt;Relation to Linformer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Linformer와 비슷한 포지션. 그럼 뭐가 더 나은가?
&lt;ul&gt;
&lt;li&gt;Linformer는 인풋 시퀀스가 모두 고정 길이를 가져야하는데, Luna는 various length가 가능 (projection matrix 때문에 linformer는 길이가 고정이여야함)&lt;/li&gt;
&lt;li&gt;결정적으로 Linformer보다 성능이 좋음&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Experiment&lt;/h2&gt;
&lt;h3&gt;Long-Context Sequence Modeling&lt;/h3&gt;
&lt;h4&gt;&lt;strong&gt;Score&lt;/strong&gt;&lt;/h4&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127603947-40c6b9f0-63d7-475c-8b6d-cefdfbe5ab59.png&quot; height=&quot;500&quot;&gt;
&lt;h4&gt;&lt;strong&gt;Training Speed &amp;#x26; Memory&lt;/strong&gt;&lt;/h4&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127604072-79facf8c-7f84-4e9d-bd1e-38b3322458d0.png&quot; height=&quot;500&quot;&gt;
&lt;ul&gt;
&lt;li&gt;인풋 길이가 길어지면 Luna가 Linformer보다 더 빠름&lt;/li&gt;
&lt;li&gt;메모리 사용량에서 Luna가 Linformer를 포함한 다른 모델들보다 경쟁력이 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;NLU Task (Masked Language Modeling for Large-Scale Pretraining)&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127604344-7074edf7-ede2-4140-a6e8-320ed711d5f3.png&quot; height=&quot;300&quot;&gt;
&lt;ul&gt;
&lt;li&gt;BERT 방식으로 pre-training 후 파인튜닝 했을 때 성능 비교&lt;/li&gt;
&lt;li&gt;RoBERTa와도 비견될만큼 좋은 성능을 보임&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Machine Translation&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127604608-822e1fd6-00a1-472b-801e-24cda16efa5f.png&quot; height=&quot;250&quot;&gt;
&lt;h3&gt;Abbrebiation Study (contextual &amp;#x3C;-&gt; non-contextual)&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/127604743-ae8289cc-d420-41b0-9ff1-5604ef770b4c.png&quot; height=&quot;150&quot;&gt;
&lt;ul&gt;
&lt;li&gt;P를 각 레이어의 파라미터로 둘지, 위층으로 넘김으로써 context 정보를 넘겨줄지에 대한 실험&lt;/li&gt;
&lt;li&gt;결론: 다음 층으로 넘겨주는게 좋더라.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[P-Tuning Paper Review]]></title><description><![CDATA[GPT Understands, Too Xiao Liu et al. Tsinghua University etc. arXiv pre-print Abstract GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU…]]></description><link>https://gatsby-casper.netlify.com/p_tuning/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/p_tuning/</guid><pubDate>Thu, 13 May 2021 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;GPT Understands, Too&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Xiao Liu et al.&lt;/li&gt;
&lt;li&gt;Tsinghua University etc.&lt;/li&gt;
&lt;li&gt;arXiv pre-print&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GPT를 파인튜닝하는 방법은 Narural Language Understanding (NLU) 태스크쪽에서 좋지 않은 결과를 보임&lt;/li&gt;
&lt;li&gt;P-tuning을 이용해서 GPT로 비슷한 사이즈의 BERT와 비슷 or 웃도는 성능을 기록 (Vanilla 대비 20%이상 성능 향상)&lt;/li&gt;
&lt;li&gt;P-tuning은 BERT 성능도 향상시킴&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Language model pre-training은 contextualized text reporesentation 뿐만 아니라 grammar, syntactic, commonsense, world knowledge까지 학습이 된다고 주장하는 연구 결과들이 있음.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Language model pre-training은 다음 3가지 (uni-directional language models (e.g., GPT), bidirectional language models (e.g., BERT), hybrid language models (e.g., XLNet))로 나눌 수 있음&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;오래동안 GPT 스타일은 NLU 태스크에 적합하지 않다고 여겨져 왔음&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GPT3는 적절한 프롬프트를 이용해서 NLU 태스크를 풀 수 있지만, 매번 좋은 프롬프트를 바로바로 찾는건 현실적으로 힘들다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;automatic prompt searching (retrieval 등) 같이 discrete prompts를 찾는 방법들이 등장했지만, discrete prompt를 찾는건 차선책일 뿐이다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;그래서 continuous space에서 prompt를 찾는 P-tuning을 제안&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P-tuning은 여러 NLU 태스크에서 상당한 성능 향상을 이룸.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Big-model은 많은 문제를 해결해줬지만 poor-transferability 문제가 있음&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Big-model은 fine-tuning을 제대로 하기가 어려움.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;prompt가 조금만 바뀌어도 큰 성능 차이가 있음&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Method: P-tuning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;예시 문제: “The capital of Britain is [MASK]”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prmopt: “The capital of … is …”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Context: “Britain”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Target: “[MASK]”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;기존 Inputs: e(token 0), e(token 1), …, e(token n) (e는 embedding)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P-tuning Inputs: h(P[0]), …, h(P[i]), e(x), h(P[i+1]), …, h(P[m]), e([MASK]) (h는 lstm hidden state, x는 context)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;P = [0, 1, 2, 3, 4, 5] 이런 식으로 정의&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Anchor tokens: (b)의 “capital” 같이 태스크와 관련된 토큰을 추가로 임베딩 레이어에 넣어줄 수도 있음 (성능 개선을 위해)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prompt Encoder 구성&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;PromptEncoder&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;lstm&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; LSTM&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;hidden_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; hidden_size &lt;span class=&quot;token operator&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; num_layers&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bidirectional&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;mlp&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; Sequential&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; Linear&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;in_features&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;hidden_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; out_features&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;hidden_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; ReLU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; Linear&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;in_features&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;hidden_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; out_features&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;hidden_size&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Bi-directional LSTM + MLP 구조&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Knowledge Probing (LAMA Dataset)&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/119778212-3120b900-bf02-11eb-91b0-896d355c901e.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;SuperGLUE&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/119778331-56152c00-bf02-11eb-92fc-05acf14e8027.png&quot;&gt;
​
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GPT, BERT에 상대적으로 작은 사이즈의 prompt-encoder를 도입해서 큰 성능 향상을 얻음&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;같은 방법으로 GPT3 같은 모델에도 p-tuning을 도입해서 성능 향상을 기대해 볼 수 있음&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Megatron-LM 사이즈까지는 실험을 했으나, 그 이상의 사이즈는 실험 X&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Paper Review]]></title><description><![CDATA[Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition Yu Zhang et al., 2020 Google Research, Brain Team Reference…]]></description><link>https://gatsby-casper.netlify.com/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition/</guid><pubDate>Wed, 17 Mar 2021 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition&lt;/h1&gt;
&lt;p&gt;Yu Zhang et al., 2020&lt;br&gt;
Google Research, Brain Team&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kakaobrain/nlp-paper-reading/blob/master/notes/wav2vec%202.0.md&quot;&gt;Wav2vec 2.0 Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/speech-paper-reading/speech-paper-reading/blob/main/notes/conformer.md&quot;&gt;Conformer Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;현재 Papers with Code 기준 ASR 부문 State-Of-The-Art&lt;/li&gt;
&lt;li&gt;Conformer + Wav2vec 2.0 + Noisy Student Training&lt;/li&gt;
&lt;li&gt;1.3%/2.6%/1.4%/2.6% on the dev/dev-other/test/test-other sets&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/111322357-2f3dac80-86ac-11eb-8a05-24be848077de.png&quot; height=&quot;300&quot;&gt;
&lt;hr&gt;
&lt;h3&gt;Wav2vec 2.0 Pre-training&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/92450554-8a22b280-f1f6-11ea-8f66-0616b29d8c94.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;53,000이라는 대량의 Unlabeled speech data로 학습&lt;/li&gt;
&lt;li&gt;Pre-training 과정
&lt;ul&gt;
&lt;li&gt;Waveform에서 CNN을 이용해서 피쳐를 뽑음&lt;/li&gt;
&lt;li&gt;이를 Vector Quantization을 통해 one-hot-vector로 만들고 Embedding matrix를 내적하여 token화 함&lt;/li&gt;
&lt;li&gt;일정 비율로 Masking하고 다음 Token이 뭔지 알아맞추게하는 Masked Language Modeling (MLM) 학습 방식 적용&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vector Quantization&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://camo.githubusercontent.com/4e4253817961b5bead8072739c39bd3f3daaced98e8735018c50e8a55d78fb9c/68747470733a2f2f692e696d6775722e636f6d2f7931355175355a2e706e67&quot; height=&quot;300&quot;&gt;
  - Z를 선형변환하여 logit을 만듦
  - 여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦
  - 이후 Embedding matrix를 내적해 Z^를 만듦
&lt;h3&gt;Conformer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는 부족하다는 단점이 있음&lt;/li&gt;
&lt;li&gt;반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는 적당한 dilation과 깊은 구조를 가져야 함&lt;/li&gt;
&lt;li&gt;이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한 transformer + CNN 결합구조인 Conformer 구조 제안&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Method&lt;/h2&gt;
&lt;p&gt;본 논문에서 실험한 모델 구조 및 트레이닝 방법&lt;/p&gt;
&lt;h3&gt;Model Architecture&lt;/h3&gt;
&lt;p&gt;Conformer Encoder + LSTM decoder로 이루어진 Transducer 구조&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/111322627-77f56580-86ac-11eb-957c-d51db823e4e4.png&quot; height=&quot;600&quot;&gt;
&lt;ul&gt;
&lt;li&gt;기존 Conformer 구조에서 Wav2vec 2.0 Pre-training을 도입하기 위해 Masking하는 과정과 Linear Layer (Quantization 대체) 추가&lt;/li&gt;
&lt;li&gt;사이즈별로 L, XL, XXL로 구분&lt;/li&gt;
&lt;li&gt;XXL+는 Conformer XXL에 Conformer block을 stack (XXL보다 50M 파라미터 추가)&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/111324910-8ba1cb80-86ae-11eb-997f-12634e7b6164.png&quot;&gt;
&lt;h3&gt;Wav2vec 2.0 Pre-training&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;60k Libri-Light 데이터셋 사용&lt;/li&gt;
&lt;li&gt;기존 논문과 달리, 인풋으로 log-mel spectrogram 사용&lt;/li&gt;
&lt;li&gt;Masking 된 인풋과 예측한 context vector 간의 contrastive loss로 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Noisy Student Training with SpecAugment&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/111328868-fa345880-86b1-11eb-925c-a76cdbfd7c8c.png&quot; height=&quot;200&quot;&gt;
&lt;h2&gt;Experiiments&lt;/h2&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/111329076-2e0f7e00-86b2-11eb-8c87-17d2eca8948b.png&quot; height=&quot;400&quot;&gt;
&lt;ul&gt;
&lt;li&gt;결과적으로 Pre-training + NST가 좋은 성적을 냄&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Longformer Paper Review]]></title><description><![CDATA[Longformer: The Long-Document Transformer Paper Code Iz Beltagy et al. Introduction 트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음 이유는 시퀀스 길이에 O(n^…]]></description><link>https://gatsby-casper.netlify.com/longformer/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/longformer/</guid><pubDate>Sat, 06 Feb 2021 23:46:37 GMT</pubDate><content:encoded>&lt;h1&gt;Longformer: The Long-Document Transformer&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.05150&quot;&gt;Paper&lt;/a&gt; &lt;a href=&quot;https://github.com/allenai/longformer&quot;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Iz Beltagy et al.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;트랜스포머는 긴 시퀀스는 처리하지 못한다는 한계를 가지고 있음&lt;/li&gt;
&lt;li&gt;이유는 시퀀스 길이에 O(n^2)하게 늘어나는 높은 복잡도 때문&lt;/li&gt;
&lt;li&gt;본 논문은 Attention 이루어지는 복잡도를 낮추는 방법을 제안 (O(n))&lt;/li&gt;
&lt;li&gt;BERT는 512 토큰을 limit으로 가지는데, 본 논문 모델은 4096 토큰까지 가능&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;text8&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;enwik8&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;Wiki-Hop&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;TriviaQA&lt;/code&gt;에서 State-Of-The-Art 달성&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Attention Method&lt;/h2&gt;
&lt;img src=&quot;https://haebinshin.github.io/public/img/longformer/figure2.png&quot;&gt;  
&lt;p&gt;본 논문에서는 위 그림과 같은 3가지 어텐션 방식을 제안&lt;/p&gt;
&lt;h3&gt;Sliding window Attention&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;크기가 w인 sliding window 내에서만 attention을 수행하는 방법&lt;/li&gt;
&lt;li&gt;이 방법은 텍스트 길이 n에 대해 O(n x w)의 복잡도를 가짐&lt;/li&gt;
&lt;li&gt;이러한 방식은 레이어가 깊어짐에 따라 receptive field가 넓어지는 CNN과 유사함&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://haebinshin.github.io/public/img/longformer/receptive_field.png&quot;&gt;  
&lt;ul&gt;
&lt;li&gt;예) window size가 2일 때, 레이어가 쌓일수록 w만큼 receptive field가 넓어짐.&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://haebinshin.github.io/public/img/longformer/text_sliding_window_receptive_field.jpg&quot;&gt;
&lt;ul&gt;
&lt;li&gt;l x w의 receptive field size를 가지게 됨.&lt;/li&gt;
&lt;li&gt;각 레이어마다 w의 크기를 다르게 주는 방법이 도움이 될 수도 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Dilated Sliding Window&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Sliding window attention보다도 receptive field를 더 넓히기 위해 고안된 방법&lt;/li&gt;
&lt;li&gt;Dilated Convolution에서 착안&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://haebinshin.github.io/public/img/longformer/dilation_convolution.gif&quot;&gt;  
&lt;ul&gt;
&lt;li&gt;dilated 값을 줘서 토큰을 d만큼 건너뛰면서 어텐션하도록 하는 방법&lt;/li&gt;
&lt;li&gt;예) window size가 2이고 dilation size가 2일 때, 아래 그림과 같이 w x d만큼 receptive field가 넓어짐&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://haebinshin.github.io/public/img/longformer/text_dilated_sliding_window_receptive_field.jpg&quot;&gt;
&lt;ul&gt;
&lt;li&gt;l x d x w의 receptive field size를 가지게 됨.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Global Attention&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BERT의 [CLS] 토큰 같은 경우는 전체 컨텍스트를 바라봐야하는데, 위의 2가지 방법만으로는 Finetuning하는 태스크에서는 부족한 부분이 있을 수 있음&lt;/li&gt;
&lt;li&gt;따라서 스페셜 토큰 몇 개에 대해서는 global attention을 수행하도록 함.&lt;/li&gt;
&lt;li&gt;전체 토큰 수에 비해서는 스페셜 토큰은 매우 적기 때문에 복잡도는 여전히 O(n)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Linear Projections for Global Attention&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;보통의 트랜스포머의 어텐션은 Q, K, V로 이루어 지는데, sliding window 기반 어텐션과 global 어텐션을 위해 sliding Q, K, V와 global Q, K, V 두 세트로 나눠서 어텐션을 계산하도록 구현&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;2가지 방식으로 평가를 진행.&lt;/p&gt;
&lt;h3&gt;Autoregressive Language Modeling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;모델 자체의 임베딩 평가를 위함&lt;/li&gt;
&lt;li&gt;character/token 단위의 language modeling을 수행.&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;text8&lt;/code&gt;, &lt;code class=&quot;language-text&quot;&gt;enwik8&lt;/code&gt; 데이터셋에서 SOTA를 달성&lt;/li&gt;
&lt;li&gt;본 태스크는 dilated sliding window attention 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://haebinshin.github.io/public/img/longformer/table_2_3.png&quot;&gt;
&lt;h3&gt;Pre-training and Fine-tuning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RoBERTa 체크포인트로부터 시작해서 학습&lt;/li&gt;
&lt;li&gt;sliding window attention를 사용&lt;/li&gt;
&lt;li&gt;각 태스크에 따라 스페셜 토큰을 지정하여 global attention을 사용&lt;/li&gt;
&lt;li&gt;&lt;code class=&quot;language-text&quot;&gt;WikiHop&lt;/code&gt;과 &lt;code class=&quot;language-text&quot;&gt;TriviaQA&lt;/code&gt;에서 SOTA 달성&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://haebinshin.github.io/public/img/longformer/table8.png&quot;&gt;</content:encoded></item><item><title><![CDATA[Computer Architecture Review]]></title><description><![CDATA[Computer Architecture Review 오랜만에 컴퓨터 구조에서 배운 내용을 조금 복습해보며 감을 잡기 위함 컴퓨터가 코드를 처리하는 과정 Read Code Assembly 변환 CPU에서 실행 CPU에서 하나의 명령(Ex) Add…]]></description><link>https://gatsby-casper.netlify.com/computer-ar-review/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/computer-ar-review/</guid><pubDate>Fri, 05 Feb 2021 10:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Computer Architecture Review&lt;/h2&gt;
&lt;p&gt;오랜만에 컴퓨터 구조에서 배운 내용을 조금 복습해보며 감을 잡기 위함&lt;/p&gt;
&lt;h3&gt;컴퓨터가 코드를 처리하는 과정&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Read Code&lt;/li&gt;
&lt;li&gt;Assembly 변환&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU에서 실행&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;CPU에서 하나의 명령(Ex) Add)을 실행하는 과정 (5단계)&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;IF: Instruction Fetch (명령어 가져오기)&lt;/li&gt;
&lt;li&gt;ID: Instruction Decode (무슨 명령어인지)&lt;/li&gt;
&lt;li&gt;EX: Execute / Adress calculation (명령 실행 or 접근할 주소 계산)&lt;/li&gt;
&lt;li&gt;MEM: Access Memory (메모리에 접근)&lt;/li&gt;
&lt;li&gt;WB: Write (변수 업데이트)&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;위의 5단계가 CPU (Single Cycle Datapath)에서 실행되는 과정&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;한 번의 Cycle에 명령 하나씩 처리&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;layout: post
title: “ComputerArchitecture Review”
date: 2021-01-23 03:15:30 +300
image: x1.png
tags: open-source&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106787176-baca2380-6692-11eb-9f5f-a6c0a4ae97b3.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;시간 비용&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106787296-e3521d80-6692-11eb-8991-7f25c9d665d4.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;h3&gt;Pipeline&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;나누어진 부분을 경계로 병렬적으로 처리&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106787546-2f04c700-6693-11eb-8041-f988e7e0ce07.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;시간 비용&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106787829-8440d880-6693-11eb-84f7-6a16f957b7c4.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;h3&gt;Hazard&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Instruction을 실행하기 위한 데이터가 준비되지 않아서 Instruction을 실행할 수 없는 경우&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106788164-f44f5e80-6693-11eb-9117-6c187ced8fbf.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;해결 방법 1: Data Forwarding&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106788436-4a240680-6694-11eb-8ce1-ca67136cf0f3.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;필요한 데이터가 register나 memory에 write 될 때까지 기다리지 않고, internal buffer로부터 읽어서 Hazard를 해결하는 방법&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;해결 방법 2: Stall (Bubble)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106788736-af77f780-6694-11eb-9a0f-f92ff79d7ddd.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;Hazard를 피할 수 없는 경우 아무 명령을 처리하지 않는 nop (No-operation) 를 넣음으로써 Data Forwarding을 할 수 있도록 싱크를 맞춰주는 방법&lt;/p&gt;</content:encoded></item><item><title><![CDATA[2020년 회고]]></title><description><![CDATA[2020년 회고 다사다난했던 2020년이 지나고 어느덧 2021년 새해가 밝았습니다. 🤗 🤗 코로나라는 세계적인 재앙 때문에 생활부터 모든게 많이 달라진 한 해 였습니다. 여태까지…]]></description><link>https://gatsby-casper.netlify.com/2020/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/2020/</guid><pubDate>Thu, 31 Dec 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;2020년 회고&lt;/h1&gt;
&lt;p&gt;다사다난했던 2020년이 지나고 어느덧 2021년 새해가 밝았습니다. 🤗 🤗&lt;br&gt;
코로나라는 세계적인 재앙 때문에 생활부터 모든게 많이 달라진 한 해 였습니다.&lt;br&gt;
여태까지 1년 회고는 개인 일기장에만 적어봤지만, 이번에는 많은 일들이 있었던 만큼 제 블로그에 기록해두고 싶어서 글을 남깁니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;KoSpeech (졸업 프로젝트)&lt;/li&gt;
&lt;li&gt;연구실 인턴&lt;/li&gt;
&lt;li&gt;미래에 대한 고민&lt;/li&gt;
&lt;li&gt;갑작스러운 제안&lt;/li&gt;
&lt;li&gt;카카오브레인 인턴&lt;/li&gt;
&lt;li&gt;정직원 전환&lt;/li&gt;
&lt;li&gt;리서치 엔지니어로서의 4달&lt;/li&gt;
&lt;li&gt;Good Bye! Kwangwoon University!&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;KoSpeech (졸업 프로젝트) (1월 ~ 3월)고&lt;/h2&gt;
&lt;p&gt;언제부터였는지 모르겠지만, 저는 대학교에서의 졸업작품을 그 어떤 작품들보다 성공적으로 만들고 싶었습니다. 그래서였는지, 학점을 좋게 받는것보다는 프로젝트가 있는 수업에서 그 누구보다 프로젝트를 잘하고 싶었습니다. 시험 전날 다른 과목 시험공부를 하다가도, 프로젝트 아이디어가 떠올라서 노트북을 키고 코딩을 하다보니 어느새 시험 직전이어서 시험 범위도 못 끝낸 적도 있었네요 ㅎㅎ&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/103424527-292d4900-4bf0-11eb-85ea-2cf672fee02e.png&quot; alt=&quot;image&quot;&gt;
&lt;em&gt;c언어로 구현한 갤러그&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;첫 소프트웨어 프로젝트였던 &lt;code class=&quot;language-text&quot;&gt;c언어로 구현한 갤러그&lt;/code&gt;를 시작으로 학부 2-3학년 동안 크고 작은 소프트웨어 프로젝트를 약 20개 가까이 진행했던 것 같습니다. 지금 그 코드를 다시 보면 부끄러울 정도로 미숙하지만, 그런 삽질의 과정들이 있었기 때문에 소프트웨어 구조를 짤 때 어떻게 짜야할지에 대한 인사이트가 조금은 생긴 것 같습니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.bloter.net/wp-content/uploads/2019/08/1564657117230-800x611.png&quot; alt=&quot;image&quot;&gt;
&lt;em&gt;네이버 AI 해커톤 - Speech&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;졸업작품은 3-2학기 였던 19년 하반기부터 시작했습니다. 한국어 음성 인식을 주제로 프로젝트를 진행했는데, 마침 시기 좋게 열린 &lt;code class=&quot;language-text&quot;&gt;네이버 AI 해커톤 - Speech&lt;/code&gt;에 참가해서 전체 100팀 중 12등을 했었습니다. 당시 상위권 팀이던 1 - 3등 팀들의 소감을 들으면서 많은 자극을 받았습니다. 그래서인지 20년 1월에 한참 음성인식에 대한 열정이 불타올랐던 것 같습니다. 팀원들과 함께 상위권팀들이 썼다는 기술들에 대해 하나하나 키워드별로 조사해가며 스터디를 진행했고, 스터디 → 구현 → 스터디 → 구현을 반복했습니다. 그러다보니 겨울방학기 끝날 때 쯤에는 꽤 그럴 듯한 음성인식 툴킷이 만들어져 있었고, 그 후 계속 수정을 거치며 지금의 &lt;a href=&quot;https://github.com/sooftware/KoSpeech&quot;&gt;KoSpeech&lt;/a&gt;가 만들어졌습니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;연구실 인턴 (4월 ~ 8월)&lt;/h2&gt;
&lt;p&gt;당시 제 목표는 구체적이진 않았지만 뛰어난 AI 리서치 엔지니어가 되는 것이였습니다. AI 리서치 엔지니어가 되기 위해서는 적어도 석사 학위는 필수라고 생각했습니다. 학위도 학위지만, 대학원에서의 2년이 결코 무시 못할 경험이라고 생각했습니다. 하지만 막연히 ‘난 대학원에 진학할거야’라는 생각만 가진채, 언제 어떻게 교수님께 컨택을 하고, 어떤 연구실에 진학할지에 대해서는 정하지 않고 있었습니다. 지금 돌이켜보니, 연구실에 컨택했다가 거절당하는 것에 대한 두려움이 어느정도 있어서 계속 결정을 미뤄왔던 것 같습니다. 그러던 중에 대학원에 진학을 염두하는 저를 위해 여자친구가 선물해준 “대학원생 때 알았더라면 좋았을 것들” 책을 읽었습니다.&lt;/p&gt;
&lt;img src=&quot;https://image.yes24.com/Goods/72231788/L&quot;&gt;  
&lt;p&gt;책에 있는 대부분 내용이 유익한 내용이었지만, 그 중 ‘대학원에 진학하기전에 가능하다면 꼭 해당 연구실 인턴을 해봐라. 이미 입학하고 후회하면 늦는다’라는 구절이 제 마음을 움직였습니다. 그 구절을 곱씹으면서 되든 안되든 연구실에 컨택을 해보기로 했습니다. 그렇게 다분히 심심했던 제 생활패턴에서 자기소개서, 교수님께 보낼 메일 작성, 사진 찍기 등 몇일 간 바쁘게 보냈습니다. 준비를 마치고는 관심있었던 연구실의 교수님께 메일을 보냈습니다. 메일을 보내고는 10분마다 메일왔는지를 확인했던 것 같습니다 ㅋㅋㅋ.. 하지만 메일을 보낸지 몇일이 지나도 교수님께 답장은 없었습니다.&lt;/p&gt;
&lt;img src=&quot;https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F267D2541521A110A2D&quot;&gt;  
&lt;p&gt;메일을 보내고 답장을 기다리는 몇일 동안, 많은 고뇌가 있었습니다. PLAN-B를 생각해본 적이 없었기 때문에 앞으로 어떡해야하나…라는 생각만 들었습니다. 역시 대학원을 가려면 학점이 좀 더 좋아야 했나.. 라는 생각이 들었습니다. 그러던 중, 기다리고 기다리던 교수님으로부터의 답장이 왔습니다!&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;김수환님께
안녕하세요. **대학교 *** 입니다. 보내주신 이메일 잘 받았습니다.

제가 연구하고 있는 음성인식 분야에 대해서 많은 관심을 가지고,
좋은 연구 결과를 내고 있는 것에, 학부연구원 연구 참여/대학원 진학을 떠나
해당 분야를 연구하고 있는 교수로서 감사의 말씀을 드립니다.

우선은 학부 연구원으로 우리 연구실에 참여를 하는 것이 좋을 것 같습니다.
학부 연구원 위촉시, 저희 연구실은 1차로 대학원 학생들의 면접이 있습니다.
현재까지 진행한 연구/개발 위주로 자연스러운 분위기에서 학생들간에 이야기를
나눈다고 생각하시면 되겠습니다. 관련해서 저희 연구실 박사과정 학생이
연락을 드리겠습니다.

- *** 드림 -&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;최근 몇년 간 가장 기뻤던 일 중 하나였습니다. 기다림이 길었던 만큼, 안 좋은 결과를 예상하고 있었는데, 교수님께서 긍정적으로 답변을 주셔서 기뻤고, 제가 여태까지 한 활동이 인정받은 것 같아 기뻤습니다. 연구실 선배들과의 간단한 면접을 마친 후, 다음주부터 연구실로 출근을 했습니다. 학기중이였지만, 코로나로 인해 대부분의 강의가 온라인으로 진행됐기 때문에 연구실로 출근해서 강의를 들을 수 있었습니다. 연구실 세미나도 참석하면서, 연구실에서 진행되고 있는 프로젝트, 연구 주제 등에 대해서 귓동냥을 열심히 했습니다. 👂 👂&lt;/p&gt;
&lt;img src=&quot;https://www.galchimia.com/wp-content/uploads/2018/03/how-to-crash-your-own-presentation-1024x787.jpg&quot;&gt;
&lt;p&gt;연구실에 들어간지 3주 정도가 지나고, 교수님께서 제가 했던 음성인식 프로젝트에 대해서 발표를 한번 해달라고 요청해주셨습니다. 전문가분들 앞에서 발표한다는 생각에 많이 떨렸습니다. 그래도 제가 한 모든 것에 대해 발표하고 싶다는 생각에 최대한 열심히 발표했습니다. 다행히도 교수님과 연구실 선배들이 제 발표를 좋게 봐주셨고, 진행중이던 KoSpeech를 더 열심히 해보라며 제가 사용할 수 있는 GPU 서버도 하나 지원해주셨습니다. 👍 👍&lt;/p&gt;
&lt;p&gt;이후에도 지속적으로 세미나에 참석하면서 서당개가 된 기분으로 열심히 들었습니다. 이 외에도 연구실에 있으면서 실제로 석,박사 분들이 어떤 일을 하는지, 어떤 점이 힘든지 등 직접 보고 들으며 제 미래 석사 생활을 머리속으로 그려보곤 했었습니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;미래에 대한 고민 (6월 ~ 7월)&lt;/h2&gt;
&lt;p&gt;연구실 생활은 매우 만족스러웠지만, 제 미래에 대한 고민은 계속 커졌던 것 같습니다. 음성인식이란 분야에 내가 어떤 기여를 할 수 있을지, 음성인식이 몇년 뒤에 완전히 정복되어버려서 내가 공부하는 모든게 필요없어지는 날이 와버리는게 아닌지, 지금 있는 연구실이 내 성향과 잘 맞는 곳인지 등 올해 가장 많은 고민을 했던 시기였습니다.&lt;/p&gt;
&lt;p&gt;특히 저는 최근에 연구가 활발한 End-to-End 방식의 음성인식을 해왔고, 앞으로도 이 방법을 연구하고 싶었는데, 소속된 연구실에서는 End-to-End가 나오기 이전 방식인 Kaldi 기반의 음성인식을 주로 연구하고 있었습니다. 그리고 무엇보다도 음성인식 외에도 음성합성, 화자인식, NLP 등 좀 더 넓은 분야를 공부하고 싶은 욕심이 강했습니다. 여기 연구실로 진학하게 되면 전문성은 생기지만 제가 할 수 있는 일의 범위가 너무 협소해지는건 아닐까? 라는 생각에 답이 없는 고민을 하며 지냈습니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;갑작스러운 제안 (7월)&lt;/h2&gt;
&lt;p&gt;여느날처럼 연구실 선배들과 저녁을 먹고 연구실로 다시 돌아가던 중에, 지금 팀장님으로부터 페이스북 메시지를 받게 되었습니다. &lt;code class=&quot;language-text&quot;&gt;안녕하세요&lt;/code&gt; 5글자 였지만, 저는 속으로 말도안돼! 라고 생각했었습니다. 깃허브에서 이미 너무나 유명하신 분이셔서 이미 누구신지 잘 알고 있었습니다. 인사를 주고 받은 후에 KoSpeech를 보고 연락을 줬다는 말과 판교로 한 번 놀러오라는 한마디에 사기인가 싶어서 페이스북 프로필에 들어가서 맞는지 확인까지 해봤습니다. 그 뒤로도 메세지 몇개를 주고 받았는데, 인턴 의향이 있냐는 메세지에 얼굴이 빨개지고 심장이 쿵쾅쿵쾅 뛰었던 기억이 생생합니다.&lt;/p&gt;
&lt;p&gt;너무 기뻐서 이게 꿈이 아닌지를 몇번이고 확인해보고, 사기는 아닌지 페이스북 프로필을 몇번이나 확인하고 나서야, 실제상황임을 깨닫고 여자친구와 부모님 친구들께 동네방네 자랑했습니다. ㅋㅋㅋ&lt;/p&gt;
&lt;p&gt;아마 이 날 느낀 감정이 제가 살면서 가장 기뻤던 날로 기억합니다. 항상 제게는 어느 정도 예상 가능한 일들만 일어났었는데, 전혀 예상 못한 일이 일어났다는 그 사실이 믿기지 않았습니다. 특히 제가 석사 졸업하고 가장 가고 싶었던 카카오브레인에서 이런 제안을 해줬다는 사실이 믿기지가 않았습니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;카카오브레인 인턴 (9월 ~ 10월)&lt;/h2&gt;
&lt;img src=&quot;https://m.economictimes.com/thumb/msid-69278826,width-1200,height-900,resizemode-4,imgsize-82628/internship2-getty.jpg&quot;&gt;
&lt;p&gt;인턴 제안을 받은지 한 달 정도 후, 몸 담고 있던 연구실 정리와 간단한 인턴 면접 등을 마치고 카카오브레인에서의 인턴 생활을 시작했습니다. 코로나가 한참 심해지던 시기라 첫 날부터 재택근무로 시작을 했습니다. 인턴 프로젝트로 3개월동안 음성 관련 프로젝트를 진행하게 됐습니다. 처음에는 써보지 않은 툴킷에 적응하고, 타겟으로 잡은 논문들을 이해하느라 스트레스도 많이 받았던 것 같습니다. 어떻게든 잘해내고 싶다는 마음에 밤낮, 주말없이 일에 매진했습니다. 처음 한 2주간은 조그만 아웃풋은 커녕, 삽질의 연속이였습니다. 특히 새로운 툴킷에 익숙해지기까지의 과정이 너무나 힘겨웠습니다.&lt;/p&gt;
&lt;img src=&quot;https://post-phinf.pstatic.net/MjAxODA2MDFfMzEg/MDAxNTI3ODQxNjI5MTI4.2cfEooGQ32iBE5yQD0q1ve0PD3BLzV8fnm2fLi48TOQg.pb91iOZ_KPZP-os3R_8G838sddwZUJKy0_wLcFHywX0g.JPEG/%EC%82%BD%EC%A7%88%EC%82%BD%EC%A7%88.jpg?type=w1200&quot;&gt;
&lt;p&gt;하지만 삽질도 2주정도 해보니까, 어떻게 어떻게 툴킷에 적응이 됐고, 막혔던 일들이 기다렸다는 듯이 하나하나 뻥! 뻥! 뚫리기 시작했습니다. 한번 속도가 붙으니 그 뒤로는 한주 한주 다르게 프로젝트가 진행됐던 것 같습니다. 일에 자신감도 붙었고, 팀에도 점차 적응해가며 스트레스보다는 즐거운 마음으로 일을 할 수 있었습니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;정직원 전환 (11월)&lt;/h2&gt;
&lt;img src=&quot;https://miro.medium.com/max/6000/1*zPegOUmJJWIWTc-blEekcA.png&quot;&gt;
&lt;p&gt;인턴 기간 3개월 중 2개월이 지나던 무렵, 정직원 전환 면접을 진행하고 정직원으로 전환됐습니다!! 🎉 🎉&lt;br&gt;
인턴으로 들어갈때까지만 해도, 정직원으로의 전환은 말도안된다고 생각했는데, 꿈이 현실로 이루어진 날이였습니다. 사실 면접 보기 전까지는 웬만하면 되지 않을까? 란 생각이 있었는데, 면접을 보고나서 면접을 너무 못 봤다는 생각에 자리로 돌아가 멘탈이 나간채로 자리에 앉아있었습니다. 다시 면접장으로 불러서 팀원들이 정직원 전환 축하한다고 해줬을 때, 얼떨떨해서 좋은 티도 못냈지만 속으로는 기쁨과 안도감이 동시에 들고 긴장감이 확 풀렸었습니다. 정직원 전환됐다는 말에 부모님과 여자친구가 가장 축하해주고 좋아해줬던 것 같습니다.&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;리서치 엔지니어로서의 4달 (9월 ~ 12월)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/103439743-e36d9080-4c82-11eb-9913-6ae40e1d26c8.png&quot; alt=&quot;&quot;&gt;
&lt;em&gt;제 인생 첫 명함입니다 ㅎㅎ&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;9, 10, 11, 12월을 카카오브레인에서 리서치 엔지니어로써 보냈습니다. 4개월 동안 정말 많은 성장이 있었고, 제가 많이 부족하다는 것 또한 뼈져리게 느낀 기간이였습니다. 그리고 이렇게 훌륭한 팀장님, 훌륭한 팀원분들과 같이 일을 할 수 있다는 점에 굉장히 감사합니다. 훌륭한 팀원분들은 항상 제게 동기부여가 되는 것 같습니다. 저도 팀원들에게 동기부여가 되는 사람이 될 수 있도록 부단히 노력해야겠습니다.&lt;/p&gt;
&lt;h2&gt;Good Bye! Kwangwoon University!&lt;/h2&gt;
&lt;img src=&quot;https://www.kw.ac.kr/en/img/m_visual_01.jpg&quot;&gt;
&lt;p&gt;대학교에서의 마지막 학기를 마쳤습니다. 이제 더 이상 제가 다니던 광운대학교에서 수업을 듣는 일은 없게 됐습니다. 그런 점에서 여러모로 아쉬웠던 1년이였습니다. 코로나로 인해 4학년 1년을 대부분 온라인 수업으로 진행하면서, 학교 강의실에서 수업듣는 일, 학교 도서관에서 공부하는 일 등 지극히 당연했던 일상이 급격하게 바뀌었습니다. 편하기도 했지만, 한편으로는 시험기간에 도서관에서 친구들과 밤을 새고, 한번씩 나와서 편의점을 가고 별거 아닌 얘기들로 수다를 떨고, 어디과에 누군지는 모르지만 도서관에서 얼굴을 자주 봐서 어느새 익숙해져 버린 분들도 더 이상 볼 수 없었던 점 등 대학교에서의 마지막 1년의 추억을 많이 쌓지 못한 것 같아 아쉽습니다.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[EMNLP Paper Review: Speech]]></title><description><![CDATA[EMNLP Paper Review: Speech Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al) Incremental Text-to-Speech…]]></description><link>https://gatsby-casper.netlify.com/2020 EMNLP Speech Paper Review/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/2020 EMNLP Speech Paper Review/</guid><pubDate>Tue, 08 Dec 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;EMNLP Paper Review: Speech&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.08518&quot;&gt;Adaptive Feature Selection for End-to-End Speech Translation (Biao Zhang et al)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.02750&quot;&gt;Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework (Mingbo Ma et al)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Adaptive Feature Selection for End-to-End Speech Translation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EMNLP 2020&lt;/li&gt;
&lt;li&gt;Biao Zhang, Ivan Titov, Barry Haddow, Rico Sennrich&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;End-to-End Speech Translation (E2E ST)를 다룬 논문&lt;/li&gt;
&lt;li&gt;Speech Translation
&lt;ul&gt;
&lt;li&gt;Cascade: 음성 (source) → 음성인식 모델 → 텍스트 (source) → 번역 모델 → 텍스트 (target)&lt;/li&gt;
&lt;li&gt;E2E: 음성 (source) → 음성번역 모델 → 텍스트 (target)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cascade 방식은 음성인식에서의 오류가 기계번역으로 전파가 되는 단점이 있음&lt;/li&gt;
&lt;li&gt;E2E 번역이 최근 많이 연구되고 있으나, Cascade 방식의 성능을 따라잡지 못하고 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/101368190-32294400-38ea-11eb-924b-5b0a2e25d2e6.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E2E ST가 어려운 주된 이유로, 음성마다 단어 발화 길이가 다르며, 노이즈 혹은 중간중간 끊기는 등 일관적이지 않다는 특징 때문이라고 주장&lt;/li&gt;
&lt;li&gt;그래서 인코딩 된 피쳐를 선택적으로 사용해야 된다고 주장 (Adaptive Feature Selection)&lt;/li&gt;
&lt;li&gt;AFS는 인코더 아웃풋에서 필요없는 프레임은 제거하는 역할을 함 (L&lt;sub&gt;0&lt;/sub&gt;Drop - Zhang et al., 2020)&lt;/li&gt;
&lt;li&gt;결과적으로 본 논문은 아래와 같은 파이프라인을 제안함&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/101366218-073df080-38e8-11eb-8699-dd6ebc2d70dc.png&quot; width=&quot;300&quot;&gt;  
&lt;ul&gt;
&lt;li&gt;Training Pipeline
&lt;ol&gt;
&lt;li&gt;ASR 모델 학습 (Hybrid Cross Entropy + CTC)&lt;/li&gt;
&lt;li&gt;AFS 모델을 추가해서 ASR 모델 파인튜닝&lt;/li&gt;
&lt;li&gt;ASR &amp;#x26; AFS 모델은 Freeze한 채로 ST Encoder, ST Decoder 학습&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Result on MuST-C En-De&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/101370007-4a9a5e00-38ec-11eb-8f41-7f6de1b9d583.png&quot; width=&quot;500&quot;&gt;
&lt;ul&gt;
&lt;li&gt;AFS는 모델을 더 빠르게 하면서도 성능을 높였음&lt;/li&gt;
&lt;li&gt;성능은 Cascade보다는 살짝 낮음&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;EMNLP 2020&lt;/li&gt;
&lt;li&gt;Mingbo Ma, Baigong Zheng, Kaibo Liu, Renjie Zheng, Hairong Liu, Kainan Peng, Kenneth Church, Liang Huang  (Baidu Research)&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://inctts.github.io/&quot;&gt;Demo Page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;동시번역을 위한 빠른 음성합성 기법 제안&lt;/li&gt;
&lt;li&gt;새로 학습할 필요없이 Inference 단에서 수정하여 사용할 수 있는 파이프라인 제안 (Tacotron2 사용)&lt;/li&gt;
&lt;li&gt;기존 TTS 시스템&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/101376816-6bff4800-38f4-11eb-9dca-1592c05c6759.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;Text2Phoneme → Phoneme2Spectrogram → Spectrogram2Wave 단계를 거침&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;위와 같은 Full-sentence TTS는 문장 길이가 길어질수록 latency가 길어지는 고질적인 문제점을 가지고 있음&lt;/li&gt;
&lt;li&gt;이러한 문제점 해결을 위해 아래 파이프라인을 제안&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/101377884-bf25ca80-38f5-11eb-8098-6f0b206d01f6.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full-sentence TTS가 아닌, Incremental TTS 방식 제안&lt;/li&gt;
&lt;li&gt;먼저 만들어진 오디오를 재생하는 동안 뒷단의 오디오를 만들어나가는 방식&lt;/li&gt;
&lt;li&gt;이와 같은 파이프라인이 가능하려면 특정 단위로 쪼개야함 (E.g. Word)&lt;/li&gt;
&lt;li&gt;하지만 Word 단위로 TTS를 진행한 후, 오디오를 이어붙이게 되면 굉장히 부자연스러운 음성이 합성됨&lt;/li&gt;
&lt;li&gt;이를 극복하기 위해 lookahead-k Policy 제안
&lt;ul&gt;
&lt;li&gt;t번째 target을 만들때 t+k개의 입력 소스를 통해 생성 (첫 k+1 스텝까지는 wait)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;결과적으로 음질이 크게 떨어지지 않으면서도 latency를 줄임 (문장이 길수록 효과가 큼)&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Megatron LM Paper Review]]></title><description><![CDATA[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism ​ Mohammad Shoeybi et al. 2019. NVIDIA Corp. ​ Summary…]]></description><link>https://gatsby-casper.netlify.com/Megatron-lm/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/Megatron-lm/</guid><pubDate>Thu, 03 Dec 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism&lt;/h1&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mohammad Shoeybi et al. 2019.&lt;/li&gt;
&lt;li&gt;NVIDIA Corp.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Transformer Language Model Parallelism&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;새로운 컴파일러나 기존 라이브러리 변경 없이 Very Big-Model을 학습시킬 수 있는 Megatron-LM을 제안 (PyTorch 기반)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;8.3 billion 파라미터를 성공적으로 학습시킴 (GPT-3: 175 billion)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2&gt;Backgrounds&lt;/h2&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106808382-a9424500-66ad-11eb-9058-98716b94ae40.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;최근 Large-scale의 Transformer를 이용한 Language Model(LM)이 대세&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;하지만 너무 큰 모델은 하드웨어적 메모리 제약으로 인해 학습이 어려움&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이를 해결하기 위해 여러 방법이 나왔지만, 새로운 컴파일러 혹은 기존 라이브러리를 건드려야 한다는 불편함이 있음&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;새로운 컴파일러나 기존 라이브러리를 변경하는 일 없이 간단한 방법으로 Model Parallelism 하는 Simple하면서도 Efficient한 방법 제안&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2&gt;Model Parallel Transformers&lt;/h2&gt;
&lt;img src=&quot;https://xiandong79.github.io/downloads/ddl1.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Transformer는 self attention block과 multi-layer perceptron (MLP) 로 구성되어 있음.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h3&gt;MLP Block&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106810444-49996900-66b0-11eb-866b-062a4cad3eb6.png&quot;&gt;
&lt;p&gt;MLP Block Model Parallelism&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106808899-59b04900-66ae-11eb-93a8-58d180bc1a70.png&quot;&gt;
&lt;p&gt;위의 공식을 따르는 MLP 블록을 Model Parallel하게 적용하기 위해서는 2가지 방법이 있을 수 있음.&lt;/p&gt;
&lt;p&gt;방법 1: 입력 X를 column으로, weight matrix A를 row로 split하는 방법.&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106809185-b6136880-66ae-11eb-87d7-693a2d5e56b7.png&quot;&gt;
&lt;p&gt;하지만 이 방법은 GeLU(X1A1 + X2A2) 6= GeLU(X1A1) + GeLU(X2A2)이 성립하지 않기 때문에 GeLU 입력 전에 동기화가 필요하다는 단점이 있음.&lt;/p&gt;
&lt;p&gt;방법 2: weight matrix A를 column으로 split하는 방법이 있음.&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106810580-85ccc980-66b0-11eb-88a5-acfbbb46c111.png&quot;&gt;
&lt;p&gt;이 방법은 위의 그림에서 볼 수 있듯이, GeLU 입력 전후로도 GPU간에 통신이 필요 없다는 장점이 있음.&lt;/p&gt;
&lt;h4&gt;정리&lt;/h4&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106809827-7ef18700-66af-11eb-903b-ed0db12c2897.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Code&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; torch
&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;nn &lt;span class=&quot;token keyword&quot;&gt;as&lt;/span&gt; nn

&lt;span class=&quot;token keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;token class-name&quot;&gt;Method1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Module&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token builtin&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;__init__&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;a1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Linear&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bias&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;a2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Linear&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bias&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        x1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;chunk&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        y1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;a1&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        y2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;a2&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        y &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; y1 &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; y2
        &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; y
    

&lt;span class=&quot;token keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;token class-name&quot;&gt;Method2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Module&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token builtin&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;__init__&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;a1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Linear&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bias&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;a2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Linear&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; bias&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token boolean&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        y1 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;a1&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        y2 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;a2&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;x&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        y &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;cat&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;y1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; y2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; y

gelu &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;GELU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

a &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;FloatTensor&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
b &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;FloatTensor&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;i &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;방법1 Test&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;GeLU(X1A1) + GeLU(X2A2) != GeLU(X1A1 + X2A2)\n&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gelu&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;a &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; b&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; end&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;\n\n&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gelu&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;a&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; gelu&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;b&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
GeLU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;X1A1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; GeLU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;X2A2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;!=&lt;/span&gt; GeLU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;X1A1 &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; X2A2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

tensor&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.8413&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;2.9960&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;5.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;7.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;9.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;11.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;13.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;15.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
         &lt;span class=&quot;token number&quot;&gt;17.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;19.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

tensor&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.8413&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;2.7958&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;4.9504&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;6.9958&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;8.9999&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;11.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;13.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;15.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
         &lt;span class=&quot;token number&quot;&gt;17.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;19.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;방법2 Test&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;GeLU(concat(X1A1, X2A2)) == concat(GeLU(X1A1) + GeLU(X2A2))\n&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gelu&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;cat&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;a&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; b&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; end&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token string&quot;&gt;&quot;\n\n&quot;&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;token keyword&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;cat&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;gelu&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;a&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; gelu&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;b&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
GeLU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;concat&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;X1A1&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; X2A2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; concat&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;GeLU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;X1A1&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; GeLU&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;X2A2&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

tensor&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;0.8413&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;1.9545&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;2.9960&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;3.9999&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;5.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;6.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;7.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;token number&quot;&gt;8.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;9.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.8413&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;1.9545&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;2.9960&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;3.9999&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;5.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;6.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;7.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;8.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;token number&quot;&gt;9.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

tensor&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;0.8413&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;1.9545&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;2.9960&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;3.9999&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;5.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;6.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;7.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;token number&quot;&gt;8.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;9.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0.8413&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;1.9545&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;2.9960&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;3.9999&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;5.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;6.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;7.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;token number&quot;&gt;8.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;token number&quot;&gt;9.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;10.0000&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;위의 코드에서 방법2로 계산시, Model Parallel하게 MLP Block을 진행할 수 있음을 알 수 있음.&lt;/p&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2&gt;Attention Block&lt;/h2&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106810472-55852b00-66b0-11eb-8f4c-a0c9eca9ef28.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;MLP Block과 같은 방법으로 Model Parallelism을 적용할 수 있음.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2&gt;Model Parallelism&lt;/h2&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106812317-d6452680-66b2-11eb-88f7-76b1b62fade0.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;결과적으로 트랜스포머 블록 하나당 총 4번의 GPU간 통신만 있으면 되는 구조가 됨.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ALL-Reduce란 해당 블록의 결과물을 모든 프로세서가 Return하는 것을 의미함.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
&lt;h2&gt;Experiment&lt;/h2&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/106813129-f0cbcf80-66b3-11eb-9597-5a6249bf49e0.png&quot;&gt;
&lt;p&gt;​&lt;/p&gt;</content:encoded></item><item><title><![CDATA[One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Paper Review]]></title><description><![CDATA[One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech Tomáš Nekvinda, Ondřej Dušek Charles University INTERSPEECH, 202…]]></description><link>https://gatsby-casper.netlify.com/one-model-many-langs/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/one-model-many-langs/</guid><pubDate>Wed, 14 Oct 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;One Model, Many Languages: Meta-learning for Multilingual Text-to-Speech&lt;/h1&gt;
&lt;p&gt;Tomáš Nekvinda, Ondřej Dušek&lt;br&gt;
Charles University&lt;br&gt;
INTERSPEECH, 2020&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.00768&quot;&gt;ArXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/Tomiinek/Multilingual_Text_to_Speech&quot;&gt;Source Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://tomiinek.github.io/multilingual_speech_samples/&quot;&gt;Demo Webpage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.10135&quot;&gt;Tacotron&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1712.05884&quot;&gt;Tacotron2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1710.08969.pdf&quot;&gt;DC-TTS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/Kyubyong/css10&quot;&gt;CSS 10 Dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://commonvoice.mozilla.org/en/datasets&quot;&gt;Common Voice Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Multilingual Speech Synthesis&lt;/li&gt;
&lt;li&gt;Meta-learning&lt;/li&gt;
&lt;li&gt;Voice Cloning : Speech in multiple languages with the same voice&lt;/li&gt;
&lt;li&gt;Code switching : Speak two (or more) languages with a single utterance.&lt;/li&gt;
&lt;li&gt;Tacotron2 base architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Tacotron&lt;/h2&gt;
&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwnGyQ%2FbtqDblNauXg%2FJsSXkwgQY1yc3lIHtdgIP0%2Fimg.png&quot; width=&quot;700&quot;&gt;
&lt;ul&gt;
&lt;li&gt;딥러닝 기반 음성합성의 대표적인 모델&lt;/li&gt;
&lt;li&gt;Attention + Sequence-to-Sequence의 TTS 버전&lt;/li&gt;
&lt;li&gt;Griffin-Lim Vocoder 사용 (빠르지만 성능은 좋지 못함)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Tacotron2&lt;/h2&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/94840259-1cfbe900-0453-11eb-8803-cac2ea30b425.png&quot; width=&quot;470&quot;&gt;  
&lt;ul&gt;
&lt;li&gt;Mel-Prediction Network : Attention based Sequence-to-Sequence Network
&lt;ul&gt;
&lt;li&gt;인코더에서 Bi-directional LSTM 적용&lt;/li&gt;
&lt;li&gt;Location Sensitive Attention 적용 (음성 Alignment에 강한 어텐션)&lt;/li&gt;
&lt;li&gt;인코더, 디코더에 Convolution Layer 적용&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Stop Token 사용&lt;/li&gt;
&lt;li&gt;Vocoder : WaveNet
&lt;ul&gt;
&lt;li&gt;장점 : 상당히 고품질의 음성으로 변환&lt;/li&gt;
&lt;li&gt;단점 : 엄청나게 느림&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Model Architecture&lt;/h2&gt;
&lt;img src=&quot;https://github.com/Tomiinek/Multilingual_Text_to_Speech/raw/master/_img/generated.png&quot; width=&quot;800&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Tacotron2 기반의 모델들로 실험 진행&lt;/li&gt;
&lt;li&gt;WaveRNN Vocoder 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;This Paper`s Model: Generated (GEN)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Parameter Generation Convolutional Encoder&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이 논문에서는 Fully convolutional encoder를 사용 (from DC-TTS)&lt;/li&gt;
&lt;li&gt;Cross-lingual knowledge-sharing을 가능하게 하기 위해 인코더 컨볼루션 레이어의 파라미터를 생성하여 사용&lt;/li&gt;
&lt;li&gt;입력되는 Language ID에 따라 Fully Connected 레이어를 통해 다른 다른 파라미터를 생성&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Speaker Embedding&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-speaker, Cross-lingual voice cloning을 위해 Speaker Embedding을 사용&lt;/li&gt;
&lt;li&gt;인코더 아웃풋에 Concatenate하여 스펙트로그램 생성시에 반영되도록 함&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Adversarial Speaker Classifier&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;이상적으로 Voice cloning을 위해서는 텍스트(언어)로부터 화자의 정보가 반영되면 안됨&lt;/li&gt;
&lt;li&gt;Speaker Classifier와 나머지 모델(인코더, 디코더)은 forward에서는 독립적이지만,  backpropagation을 진행할 때, 두 loss (L2 of predict spectrogram, cross entropy of predicted speaker ID)가 인코더 파라미터 업데이트에 영향을 미침&lt;/li&gt;
&lt;li&gt;Gradient reversal layer를 통해 인코더가 speaker에 대한 정보를 반영 못하도록 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Baselines: Shared, Separate &amp;#x26; Single&lt;/h3&gt;
&lt;p&gt;※ GEN과 다른점만 비교&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Single (SGL)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Monolingual Vanilla Tacotron 2 (Code-switching에 사용 X)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Shared (SHA)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GEN과 다르게 Tacotron 2의 인코더 사용 (Multilingual)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Separate (SEP)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;GEN과 같이 Multiple convolution layer를 사용&lt;/li&gt;
&lt;li&gt;Parameter generation 사용 X&lt;/li&gt;
&lt;li&gt;Adversarial speaker classifier 사용 X&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Dataset&lt;/h2&gt;
&lt;p&gt;10개의 언어로 구성된 CSS10과 Common Voice 데이터셋의 일부를 사용
Code-switching을 학습하기 위해 multi-speaker 데이터가 필요 (언어와 화자 일치를 없애기 위해)&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/95888064-9680c900-0dbb-11eb-9967-a30b21dbfa80.png&quot; width=&quot;600&quot;&gt;  
&lt;h2&gt;Experiment&lt;/h2&gt;
&lt;p&gt;SGL, SHA, SEP, GEN을 비교했을 때 GEN이 거의 모든 결과에서 우수한 성능을 보임&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/95888889-a816a080-0dbc-11eb-81a9-9a2d036f2def.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/95888982-cbd9e680-0dbc-11eb-984f-1524ab3a9f38.png&quot; width=&quot;400&quot;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;본 논문에서 제안하는 모델은 Multilingual Voice cloning, Code-switching에 우수한 성능을 보임&lt;/li&gt;
&lt;li&gt;추후 연구로 어텐션 모듈을 수정하는 것을 생각중이라고 함&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[RoBERTa Paper Review]]></title><description><![CDATA[RoBERTa paper / code Abstract BERT를 제대로 학습시키는 법을 제안 BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음 BERT…]]></description><link>https://gatsby-casper.netlify.com/roberta/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/roberta/</guid><pubDate>Sun, 11 Oct 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;RoBERTa&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot;&gt;paper&lt;/a&gt; / &lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/roberta&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BERT를 제대로 학습시키는 법을 제안&lt;/li&gt;
&lt;li&gt;BERT는 엄청난 모델이지만, Original BERT 논문에서 하이퍼파라미터에 대한 실험이 제대로 진행되지 않음&lt;/li&gt;
&lt;li&gt;BERT를 더 좋은 성능을 내게 하기 위한 replication study.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Background (BERT)&lt;/h2&gt;
&lt;img src=&quot;https://baekyeongmin.github.io/images/RoBERTa/bert.png&quot; width=&quot;700&quot;&gt;
&lt;ul&gt;
&lt;li&gt;학습 1단계) 많은 양의 unlabeled corpus를 이용한 pre-train&lt;/li&gt;
&lt;li&gt;학습 2단계) 특정 도메인의 태스크에 집중하여 학습하는 fine-tuning&lt;/li&gt;
&lt;li&gt;“Attention Is All You Need”에서 제안된 transformer의 encoder를 사용&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Main Idea&lt;/h2&gt;
&lt;h3&gt;Dynamic Masking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;기존의 BERT는 학습 전에 데이터에 무작위로 mask를 씌움.&lt;/li&gt;
&lt;li&gt;매 학습 단계에서 똑같은 mask를 보게 됨. (static masking)&lt;/li&gt;
&lt;li&gt;이를 같은 문장을 10번 복사한 뒤 서로 다른 마스크를 씌움으로써 해결하려고 했지만, 이는 크기가 큰 데이터에 대해서 비효율적임.&lt;/li&gt;
&lt;li&gt;RoBERTa는 매 에폭마다 mask를 새로 씌우는 dynamic masking을 사용&lt;/li&gt;
&lt;li&gt;결과: static masking보다 좋은 성능을 보여줌&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Input Format / Next Sentence Prediction&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;기존 BERT에서는 Next Sentence Prediction(NSP)이라는 과정이 있었음
&lt;ul&gt;
&lt;li&gt;
&lt;ol&gt;
&lt;li&gt;두 개의 문장을 이어 붙인다&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;두 문장이 문맥상으로 연결된 문장인지를 분류하는 binary classification을 수행&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;RoBERTa는 NSP에 의문을 제기하고, Masked Language Modeling (MLM)만으로 pre-training을 수행&lt;/li&gt;
&lt;li&gt;NSP를 없앰으로써 두 문장을 이어 붙인 형태의 인풋 형태를 사용할 필요가 없어졌고, RoBERTa는 최대 토큰 수를 넘어가지 않는 선에서 문장을 최대한 이어 붙여서 input을 만들 수 있었음&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;[CLS]가나다라마바사아자카타파하[SEP]오늘 날씨가 좋은걸?[SEP]튜닙은 정말 대단한 회사야!.....[SEP]오늘은 빨리 퇴근하고 싶다.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;BERT는 짧은 인풋들을 이어붙이는 경우도 있었지만, RoBERTa는 모든 인풋 토큰 수를 최대길이에 가깝게 사용할 수 있었음 (학습 효율면에서 좋음)&lt;/li&gt;
&lt;li&gt;결과: NSP를 없앤 BERT가 기존의 BERT보다 더 나은 성능을 보임.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Batch Size&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;(batch size X step 수)가 일정하게 유지되는 선에서 배치사이즈에 따른 성능 실험
&lt;ul&gt;
&lt;li&gt;batch size가 256에 step이 1M이라면 batch size가 2K에 step이 125K가 되도록. (둘의 곱이 같도록 유지)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;배치가 클수록 성능이 좋아지는 경향을 보임.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;데이터가 많을수록 BERT의 성능이 좋아지는 경향을 이전 연구들에서 관찰됐었음.&lt;/li&gt;
&lt;li&gt;기존 BERT는 16GB로 학습했는데, RoBERT는 160GB의 데이터로 학습하였음.&lt;/li&gt;
&lt;li&gt;당연하게도 160GB로 학습한 RoBERTa가 더 좋은 성능을 기록함&lt;/li&gt;
&lt;li&gt;학습 시간을 길게 하면 할수록 더 좋은 성능을 보였다고함.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;160GB의 데이터&lt;/li&gt;
&lt;li&gt;Dynamic Masking&lt;/li&gt;
&lt;li&gt;NSP 밴&lt;/li&gt;
&lt;li&gt;최대한 문장을 구겨넣은 인풋 ([SEP]으로 분리)&lt;/li&gt;
&lt;li&gt;큰 배치사이즈&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Electra Paper Review]]></title><description><![CDATA[Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs…]]></description><link>https://gatsby-casper.netlify.com/electra/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/electra/</guid><pubDate>Wed, 23 Sep 2020 07:03:47 GMT</pubDate><content:encoded>&lt;p&gt;Below is just about everything you’ll need to style in the theme. Check the source code to see the many embedded elements within paragraphs.&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;ELECTRA&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=r1xMH1BtvB&quot;&gt;paper&lt;/a&gt; / &lt;a href=&quot;https://github.com/google-research/electra&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ICLR 2020&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BERT에서 제안한 Masked Language Modeling(MLM)은 좋은 성능을 보여줬지만, 전체 데이터의 15%만을 마스킹해서 학습 효율 측면에서 좋지 않음.&lt;/li&gt;
&lt;li&gt;ELECTRA는 &lt;strong&gt;모델의 성능&lt;/strong&gt;과 함께 &lt;strong&gt;학습의 효율성&lt;/strong&gt;도 개선할 수 있는 방법을 제안함.&lt;/li&gt;
&lt;li&gt;Replaced Token Detection(RTD)라는 새로운 pre-training 태스크 제안.&lt;/li&gt;
&lt;li&gt;ELECTRA의 장점은 Small 모델에서 두드러짐. 1개의 GPU로 4일만 학습한 모델로 계산량이 30배인 GPT를 능가.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Replaced Token Detection (RTD)&lt;/h2&gt;
&lt;img src=&quot;https://blog.pingpong.us/images/2020.05.08.electra/figure2.png&quot; width=&quot;700&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Generator: BERT의 MLM
&lt;ul&gt;
&lt;li&gt;입력된 인풋 중 15%의 토큰을 [MASK]로 가림&lt;/li&gt;
&lt;li&gt;[MASK]로 가려진 인풋의 원래 토큰을 예측&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discriminator
&lt;ul&gt;
&lt;li&gt;입력 토큰 시퀀스에 대해서 각 토큰이 original인지 replaced인지 이진 분류로 학습&lt;/li&gt;
&lt;li&gt;학습 과정
&lt;ol&gt;
&lt;li&gt;Generator에서 마스킹 된 입력 토큰들을 예측&lt;/li&gt;
&lt;li&gt;마스킹할 위치의 토큰에 대해 generator가 예측했던 softmax 분포에서 높은 순위의 토큰 중 하나로 치환 (1위: cooked, 2위: ate, 3위: … 이였으면 MLM은 cooked를 선택하지만 해당 과정에서 ate를 가져오는 방식)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Original input : [the, chef, cooked, the, meal]&lt;/li&gt;
&lt;li&gt;Input for generator : [[MASK], chef, [MASK], the, meal]&lt;/li&gt;
&lt;li&gt;Input for discriminator : [the, chef, ate, the, meal]&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;치환된 입력에 대해 discriminator는 원래 입력과 동일한지 치환된 것인지를 이진 분류로 예측&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Training Algorithm&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jointly 학습
&lt;ul&gt;
&lt;li&gt;Generator와 Discriminator를 동시에 학습시키는 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Two-stage 학습
&lt;ol&gt;
&lt;li&gt;Generator만 MLM으로 N 스텝동안 학습&lt;/li&gt;
&lt;li&gt;뒤이어 해당 모델을 Discriminator로 N 스텝동안 학습시키는 방식 (이때 Generator의 웨이트는 고정)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Adversarial 학습
&lt;ul&gt;
&lt;li&gt;Adversarial training을 모사해서 학습시키는 방식 (jointly보다 좋지 않아서 자세히 안 봤습니다.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Result&lt;/h2&gt;
&lt;h3&gt;Performance &amp;#x26; Efficiency&lt;/h3&gt;
&lt;img src=&quot;https://blog.pingpong.us/images/2020.05.08.electra/figure1.png&quot; width=&quot;700&quot;&gt;
&lt;ul&gt;
&lt;li&gt;다른 모델들에 비해 매우 빠르게 성능이 향상되는 것을 볼 수 있음&lt;/li&gt;
&lt;li&gt;그럼에도 불구하고, 기존 BERT보다 더 좋은 성느을 기록함.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Weight Sharing&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Generator와 discriminator는 모두 트랜스포머의 인코더 구조.&lt;/li&gt;
&lt;li&gt;그렇기 때문에 3가지 선택사항이 생김.
&lt;ol&gt;
&lt;li&gt;Generator, Discriminator가 서로 독립적으로 학습 (83.5)&lt;/li&gt;
&lt;li&gt;임베딩 레이어의 웨이트만 공유 (84.3)&lt;/li&gt;
&lt;li&gt;모든 레이어의 웨이트를 공유 (84.4)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;결과적으로 모든 웨이트를 공유하는 것이 가장 좋은 성능을 보임.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Training Algorithm&lt;/h3&gt;
&lt;img src=&quot;https://blog.pingpong.us/images/2020.05.08.electra/figure3.png&quot; width=&quot;700&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Jointly 방식이 가장 성능이 좋았음 (왼쪽은 discriminator와 generator의 사이즈에 따른 실험)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;더 효율적이고 효과도 좋은 Replaced Token Detection (RTD) 제안&lt;/li&gt;
&lt;li&gt;메인 아이디어는 Generator가 만들어 낸 질 좋은 negative sample로 학습함으로써 더 적은 리소스로 모델을 더욱 견고하게 만드는 것.&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations]]></title><description><![CDATA[wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael…]]></description><link>https://gatsby-casper.netlify.com/wav2vec2/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/wav2vec2/</guid><pubDate>Sat, 12 Sep 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;wav2vec 2.0 : A Framework for Self-Supervised Learning of Speech Representations&lt;/h1&gt;
&lt;p&gt;Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli&lt;br&gt;
Facebook AI Research (FAIR)&lt;br&gt;
arXiv (2020.06)&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.11477&quot;&gt;arXiv&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/wav2vec&quot;&gt;source code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.05862.pdf&quot;&gt;Wav2vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1910.05453.pdf&quot;&gt;VQ-Wav2vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://ratsgo.github.io/speechbook/docs/neuralfe/wav2vec&quot;&gt;speech book&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;BERT in Speech Recognition&lt;/li&gt;
&lt;li&gt;Representation learning with 53,000 hours of unlabeled speech data.&lt;/li&gt;
&lt;li&gt;Excellent speech recognition performance with only 40 sentences (10 minutes) of labeled data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;음성 데이터 자체만으로 학습한 후 Fine-tuning하는 간단한 방법으로 semi-supervised 방법보다 좋은 성능을 냄&lt;/li&gt;
&lt;li&gt;TIMIT, LibriSpeech 100h 데이터셋에서 State-Of-The-Art (SOTA) 를 달성&lt;/li&gt;
&lt;li&gt;LibriSpeech 100h은 단 1시간의 데이터만으로 기존 SOTA보다 높은 성능을 보임&lt;/li&gt;
&lt;li&gt;단 10분의 데이터 (40문장) 으로 LibriSpeech clean 5.7 / noisy 10.1 Word Error Rate (WER) 를 기록&lt;/li&gt;
&lt;li&gt;LibriSpeech의 전체 학습셋을 사용했을 때 (960h) clean 1.9 / noisy 3.5 WER을 기록 (현재 &lt;a href=&quot;https://github.com/syhw/wer_are_we&quot;&gt;wer-are-we&lt;/a&gt; SOTA보다 높은 기록)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Wav2vec&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;현재 어느 정도 수준 이상의 음성인식기를 만들기 위해서는 대량의 데이터가 필요함 (수천 시간)&lt;/li&gt;
&lt;li&gt;세상에는 7,000개 이상의 언어가 존재하는데 모든 언어에 대해 이 정도 수준의 데이터를 구축하기는 어려움&lt;/li&gt;
&lt;li&gt;영아 (infant) 들은 단순히듣는 것만으로 음성에 대해 학습함&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Wav2vec 1.0 (previous work)&lt;/h3&gt;
&lt;img src=&quot;https://i.imgur.com/H9X1HiX.png&quot; width=&quot;500&quot;&gt;    
&lt;ul&gt;
&lt;li&gt;wav2vec은 크게 &lt;em&gt;encoder&lt;/em&gt; network &lt;em&gt;f&lt;/em&gt;와 &lt;em&gt;context&lt;/em&gt; network &lt;em&gt;g&lt;/em&gt; 두개의 파트로 구성 (둘 모두 convolution neural network)&lt;/li&gt;
&lt;li&gt;wav2vec은 해당 입력이 Positive인지 Negative인지 이진 분류(Binary Classification)하는 과정에서 학습&lt;/li&gt;
&lt;li&gt;Positive : (C&lt;sub&gt;i&lt;/sub&gt;, Z&lt;sub&gt;i+1&lt;/sub&gt;), Negative : otherwise&lt;/li&gt;
&lt;li&gt;Negative 쌍은 입력 음성의 i번째 context representation C&lt;sub&gt;i&lt;/sub&gt;와 다른 음성의 hidden representation들 중 랜덤 추출&lt;/li&gt;
&lt;li&gt;즉, 2개의 네트워크는 입력 음성의 다음 시퀀스가 무엇일지에 관한 정보를 음성 피처에 녹여내도록 학습&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;VQ-Wav2vec (previous work)&lt;/h3&gt;
&lt;img src=&quot;https://i.imgur.com/ivviYL1.png&quot; width=&quot;500&quot;&gt;  
&lt;ul&gt;
&lt;li&gt;Wav2vec 아키텍처 중간에 &lt;strong&gt;Vector Quantization&lt;/strong&gt; 모듈을 추가한 구조&lt;/li&gt;
&lt;li&gt;VQ 모듈 : continuous representation Z를 discrete representation Z&lt;sup&gt;^&lt;/sup&gt;로 변환&lt;/li&gt;
&lt;li&gt;Discretization(이산화)는 discrete한 input을 필요로하는 NLP 알고리즘들을 바로 적용할 수 있다는 장점이 있음&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Vector Quantization&lt;/h4&gt;
&lt;img src=&quot;https://i.imgur.com/y15Qu5Z.png&quot; width=&quot;300&quot;&gt;  
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Z를 선형변환하여 logit을 만듦&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;여기에 Gumbel Softmax와 argmax를 취해 one-hot vector를 만듦&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;이후 Embedding matrix를 내적해 Z&lt;sup&gt;^&lt;/sup&gt;를 만듦&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Wav2vec 2.0&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/92450554-8a22b280-f1f6-11ea-8f66-0616b29d8c94.png&quot; width=&quot;500&quot;&gt;  
&lt;ul&gt;
&lt;li&gt;VQ-wav2vec의 모델을 Transformer로 대체&lt;/li&gt;
&lt;li&gt;Pre-training 이후 labeled data로 fine-tuning (Connectionist Temporal Classification (CTC) loss 사용)&lt;/li&gt;
&lt;li&gt;이전 연구들과의 차이점으로 Filter-Bank와 같은 피쳐추출 과정이 없음&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;p&gt;VQ-Wav2vec와 비교하여 Transformer를 사용했다는 특징이 있음&lt;/p&gt;
&lt;h3&gt;Feature Encoder&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;N x [Conv1d, Dropout, GroupNorm, GELU]&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Transformer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Positional Encoding을 conv1d로 대체&lt;/li&gt;
&lt;li&gt;Convoulation Layer 뒷단에 layer normalization 적용&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Quantization module&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Vector Quantization 부분 참고&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;BERT의 masked language modeling (MLM)과 유사하게 latent speech의 일부분을 masking하며, 모델은 quantized latent audio representation을 맞추는 방식으로 트레이닝이 진행됨. 이렇게 학습한 후 labeled 된 데이터로 fine-tuning 진행.&lt;/p&gt;
&lt;h3&gt;Masking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Maksing 방법&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;1. 전체 오디오 구간 중 6.5%를 랜덤하게 선택  
  
2. 선택된 구간부터 10 time-step만큼 masking (masking은 중복될 수 있음)   &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;전체 오디오 구간 중 약 49% 정도가 masking (평균 14.7 timestep (299ms))&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Objective&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/93301173-0474b780-f833-11ea-8206-5ab40cf418a5.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;L&lt;sub&gt;m&lt;/sub&gt; : Contrastive Loss, L&lt;sub&gt;d&lt;/sub&gt; : Diversity Loss, L&lt;sub&gt;f&lt;/sub&gt; : L2 penalty, {alpha, beta} : hyperparameter&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Contrastive Loss&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/92514957-c5040500-f24d-11ea-95c3-1183fa1145b2.png&quot; alt=&quot;contrastive-loss&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diversity Loss&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/92514997-df3de300-f24d-11ea-835c-f1367aef5ebe.png&quot; alt=&quot;diversity-loss&quot;&gt;&lt;/p&gt;
&lt;h3&gt;Fine-tuning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;이렇게 Pre-train 된 모델을 ASR 태스크로 Fine-tuning (+ projection layer)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;29 character token&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;CTC Loss&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SpecAugment&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Experiment&lt;/h2&gt;
&lt;h3&gt;Datasets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Unlabeled data 1 : LibriVox-60k (전처리하여 53.2k 사용) [&lt;a href=&quot;https://arxiv.org/abs/1912.07875&quot;&gt;Reference&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Unlabeled data 2 : LibriSpeech 960h&lt;/li&gt;
&lt;li&gt;train-10min, train-1h, train-10h, train-100h, train-960h 설정 (LibriSpeech)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Result&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/92516234-de0db580-f24f-11ea-88f3-485ee579bfda.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WER on Librispeech when using all labeled data of 960 hours&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/92516409-262cd800-f250-11ea-8e77-42fdc8761d2c.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;적은 비용으로도 좋은 성능의 음성인식기를 만들 수 있는 연구 방향을 제시함&lt;/li&gt;
&lt;li&gt;Seq2seq 구조 혹은 word-piece 단위로의 변경을 통해 성능 향상 기대&lt;/li&gt;
&lt;/ul&gt;</content:encoded></item><item><title><![CDATA[Conformer Paper Review]]></title><description><![CDATA[Conformer: Convolution-augmented Transformer for Speech Recognition Anmol Gulati et al. Google Inc. INTERSPEECH, 2020 Reference Conformer…]]></description><link>https://gatsby-casper.netlify.com/conformer/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/conformer/</guid><pubDate>Sun, 30 Aug 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Conformer: Convolution-augmented Transformer for Speech Recognition&lt;/h1&gt;
&lt;p&gt;Anmol Gulati et al.&lt;br&gt;
Google Inc.&lt;br&gt;
INTERSPEECH, 2020&lt;/p&gt;
&lt;hr&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2005.08100.pdf&quot;&gt;Conformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hichoe95.tistory.com/48&quot;&gt;Convolution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Transformer 기반 모델이 음성인식 분야에서 좋은 성능을 보이고 있음&lt;/li&gt;
&lt;li&gt;Self-attention 기반한 트랜스포머는 global-context 정보를 잘 표현하지만, local-context에서는 부족하다는 단점이 있음&lt;/li&gt;
&lt;li&gt;반면, CNN 기반 모델은 local-context는 잘 표현하지만 global-context를 반영하기 위해서는 적당한 dilation과 깊은 구조를 가져야 함&lt;/li&gt;
&lt;li&gt;이 두 방법을 결합하여 global-context와 local-context 모두 잘 표현할 수 있도록 하기 위한 transformer + CNN 결합구조인 Conformer 구조 제안&lt;/li&gt;
&lt;li&gt;Conformer Encoder + Transducer 구조&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Conformer Encoder&lt;/h2&gt;
&lt;p&gt;기존 트랜스포머 블록과 다르게 2개의 Feed Forward Network (FFN)에 쌓인 Sandwich 방식으로 구성&lt;/p&gt;
&lt;h3&gt;Conformer encoder model architecture&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105320076-16af9980-5c09-11eb-86ec-b5146ac65812.png&quot; height=&quot;500&quot;&gt;   
&lt;p&gt;기존 트랜스포머 인코더 블록은 &lt;code class=&quot;language-text&quot;&gt;Multi Head Self Attention (MHSA) → LayerNorm → Feed Forward Network (FFN) → LayerNorm&lt;/code&gt; 구조에서 &lt;code class=&quot;language-text&quot;&gt;FFN Module → MHSA Module → Conv Module → FFN Module → LayerNorm&lt;/code&gt; 구조로 변경&lt;/p&gt;
&lt;h3&gt;Multi-Headed Self-Attention Module&lt;/h3&gt;
&lt;img src=&quot;https://images.deepai.org/converted-papers/2005.08100/x3.png&quot;&gt;  
&lt;ul&gt;
&lt;li&gt;Relative positional encoding&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;절대적인 position 정보를 더하는 방식이 아닌, 상대적인 position 정보를 주는 방식
절대적인 position 정보가 a=1, b=2와 같이 값을 지정하고 그 값의 차이를 계산하는 방식이라면, 상대적인 position 정보는 a=1, b=2이든 a=5, b=6이든 상관없이 두 수(위치)의 차이가 1이라는 것만 알려주면 되는 방식&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;이런 방식은 가변적인 시퀀스 길이 인풋에 대해 인코더를 robust하게 만들어 줌&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Pre-norm&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;기존 트랜스포머는 Post-norm인데 반해, pre-norm 적용&lt;br&gt;
이전 연구들에서 pre-norm은 깊은 모델 학습이 원활하게 되도록 도와주는 효과가 있다고 알려짐&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Convolution Module&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105454437-30aeb200-5cc5-11eb-8624-1ea49b71c8cd.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Pointwise Conv&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb8hxuL%2Fbtqw5f6QxMM%2Fk4gn4DUTEqPkqbJXusPAKk%2Fimg.png&quot;&gt;  
&lt;blockquote&gt;
&lt;p&gt;kernel size가 1x1로 고정된 convolution
dimension을 맞출 때 자주 쓰임&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;GLU Activation&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://miro.medium.com/max/1400/1*EwUvi3ATcVoa9Lm-2FwNUA.png&quot; height=&quot;50&quot;&gt;  
&lt;img src=&quot;https://miro.medium.com/max/1400/1*4UZTVLQZSDV7gCsw2cn16Q.png&quot; height=&quot;200&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Depthwise Conv&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbw6Am5%2Fbtqw4n45UWN%2FqNYnywQjSGkzkOtl5Pkzc1%2Fimg.png&quot;&gt;  
&lt;blockquote&gt;
&lt;p&gt;그룹이 채널수와 같은 Group-Convolution. 각 channel마다의 spatial feature를 추출하기 위해 고안된 방법&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&gt;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Conv&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;in_channels&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; out_channels&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; group&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Swish activation&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://blog.kakaocdn.net/dn/QbxpI/btqEHxducIg/hrmYfDLHDT4N1oqCtt74CK/img.png&quot;&gt;
&lt;h3&gt;Feed Forward Module&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/1694368/103190710-1b847480-490d-11eb-8ea5-280749a32a24.png&quot;&gt;
&lt;blockquote&gt;
&lt;p&gt;Pre-norm 적용&lt;br&gt;
Swish activation : regularizing에 도움&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2&gt;Conformer Block&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.02762.pdf&quot;&gt;Macaron-Net&lt;/a&gt;에 영감을 받아서 2개의 FFN에 쌓인 Sandwich 구조로 구성.&lt;br&gt;
FFN 모듈에 half-step residual connection 적용&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105326425-13b8a700-5c11-11eb-804c-bd8efef6060b.png&quot; height=&quot;200&quot;&gt;  
&lt;blockquote&gt;
&lt;p&gt;뒤의 Ablation study에서 Macaron-net FFN과 half-step residual connection이 성능 향상에 많은 기여를 했다고 함&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h2&gt;Experiment&lt;/h2&gt;
&lt;h3&gt;Data&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;LibriSpeech&lt;/li&gt;
&lt;li&gt;80 channel filterbank, 25ms window, 10ms stride&lt;/li&gt;
&lt;li&gt;SpecAugment (F=27), ten time masks (maximum ratio 0.05)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Conformer Transducer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Three models: 10M, 30M, and 118M params&lt;/li&gt;
&lt;li&gt;Decoder: single LSTM-layer (Transducer)&lt;/li&gt;
&lt;li&gt;Dropout ratio: 0.1&lt;/li&gt;
&lt;li&gt;Adam optimizer, β1=0.9, β2=0.98 and έ=10^-9&lt;/li&gt;
&lt;li&gt;Learning rate scheduler: transformer lr scheduler, 10k warm-up steps&lt;/li&gt;
&lt;li&gt;3-layer LSTM language model (LM) with 4096 hidden dimension (shallow fusion)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Results on LibriSpeech&lt;/h3&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105327556-5cbd2b00-5c12-11eb-8714-2c0ce2c7a1b0.png&quot;&gt;  
&lt;hr&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105327620-752d4580-5c12-11eb-9091-433ce8700141.png&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Conformer Block vs Transformer Block (without external LM)&lt;/li&gt;
&lt;/ul&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105327876-c9d0c080-5c12-11eb-8b02-948f87c5f47d.png&quot;&gt;
&lt;hr&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105328157-1916f100-5c13-11eb-9473-69ac0c658e15.png&quot;&gt;  
&lt;hr&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105328196-2338ef80-5c13-11eb-9e8a-50ff45bad7b5.png&quot;&gt;
&lt;hr&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105328376-54b1bb00-5c13-11eb-9059-38bc7361ba6d.png&quot;&gt;
&lt;hr&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/105328408-5aa79c00-5c13-11eb-94b2-8ee455c8daca.png&quot;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Transformer + CNN 구조인 Conformer를 제안했고, 이를 잘 결합하기 위한 다양한 실험을 해서 결과를 냄.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[ClovaCall Paper Review]]></title><description><![CDATA[ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers image 논문링크 2020-04-2…]]></description><link>https://gatsby-casper.netlify.com/clovacall/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/clovacall/</guid><pubDate>Fri, 13 Mar 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;ClovaCall: Korean Goal-Oriented Dialog Speech Corpus for Automatic Speech Recognition of Contact Centers&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/80241423-8a367180-869e-11ea-8438-6b651ab65fb6.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.09367&quot;&gt;논문링크&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2020-04-20에 클로바에서 공개한 따끈따끈한 논문입니다. 한국어 음성 데이터셋 공개와 더불어 베이스라인 코드와 AI Hub, 공개한 데이터셋에 대한 학습 결과까지 포함하고 있습니다. 현재 제가 진행하는 프로젝트와 완전히 동일한 주제이면서도 같은 데이터셋 (AI Hub) 을 이용한 학습까지 했다고하니 굉장히 기대하면서 읽었습니다. 게다가 논문을 낸 기관이 네이버 Clova여서 더 기대가 됐네요.&lt;/p&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;ASR은 여러 어플리케이션에서 필수적입니다만, License-free한 데이터는 찾기 힘들고, 유명한 Switchboard와 같은 데이터는 조금 구식입니다. 또한 가장 큰 문제점은 이러한 오픈 데이터는 대부분 영어로 이루어져 있다는 점입니다. 그래서 본 논문에서는 대용량의 한국어 음성 데이터셋을 공개한다고 밝힙니다. 11,000명의 화자에게서 얻은 60,000 쌍의 음성 데이터셋입니다. (1,000시간의 AI Hub 데이터셋이 2,000명의 화자로 이루어진 점과 비교하여 다양한 화자들로 구성된 데이터셋임을 알 수 있습니다.) 또한 해당 데이터셋을 검증하기 위한 베이스라인 코드를 같이 공개했습니다. 해당 데이터셋 및 코드는 &lt;a href=&quot;https://github.com/ClovaAI/ClovaCall&quot;&gt;이곳&lt;/a&gt;에 공개되어 있습니다.&lt;/p&gt;
&lt;h2&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;음성인식 서비스는 다양한 분야에 적용가능한 핵심 기술입니다. 하지만 이러한 음성인식 서비스 개발을 위한 오픈 데이터들은 (Wall Street Journal (WSJ), TIMIT, Switchboard, CallHome, Librispeech) 공개된 지 오래된 구식의 데이터들이며, 모두 영어로 이루어져 있습니다. 물론 &lt;a href=&quot;http://www.aihub.or.kr/aidata/105&quot;&gt;AI Hub&lt;/a&gt;와 &lt;a href=&quot;https://github.com/goodatlas/zeroth&quot;&gt;zeroth&lt;/a&gt;와 같이 오픈된 한국어 음성 데이터셋도 있지만, 이 데이터셋은 일상대화와 같은 주제들을 다룬 데이터셋입니다. 이러한 데이터셋들을 사용하게 되면 특정 도메인을 타겟으로 한 태스크에서는 성능이 좋지 않습니다. 그래서 본 논문에서는 음식점을 도메인으로한 데이터셋을 공개했다고 밝힙니다. 대부분의 문장은 10초 이내의 짧은 발화로 구성되어 있으며, End-point detection이나 alignment가 이슈가 되지 않는다고 합니다.&lt;/p&gt;
&lt;p&gt;또한 본 논문에서는 이러한 데이터셋이 어느 정도의 성능을 낼 수 있는지를 보이기 위해 Deep Speech 2 (DS2)와 Listen, Attend and Spell (LAS)의 2개의 모델로 실험을 진행했다고 밝힙니다. 이때 &lt;strong&gt;Pretraining-finetuning&lt;/strong&gt;, &lt;strong&gt;from-scratch training&lt;/strong&gt;, &lt;strong&gt;scratch training with data augmentation&lt;/strong&gt; 의 3가지 방법을 통해 비교했다고 합니다. 이에 더하여, 비교를 위해 이미 공개되어 있는 2개의 한국어 음성 데이터셋에 대해서도 같이 실험을 진행했습니다. 2개의 데이터셋은 QA Dataset (task-specific)과 AI Hub Dataset (dialog)를 의미합니다.&lt;/p&gt;
&lt;h2&gt;2. Related Work&lt;/h2&gt;
&lt;p&gt;본 장에서는 기존의 여러 데이터셋들에 대해 소개하고 있습니다. WSJ, TIMIT, Switchboard, CallHome, LibriSpeech 등의 데이터셋을 소개하고 있으며, 해당 데이터셋들은 여러 ASR 모델들의 성능을 비교하는 벤치마크로 사용되고 있습니다. 하지만 다시 강조하듯이, 이러한 데이터셋들은 일상 대화라는 주제를 가진 데이터셋이며, 특정 도메인을 주제로한 데이터셋들은 거의 오픈되지 않습니다. 또한 이렇게 오픈된 일상 대화 데이터셋들은 특정 도메인을 타겟으로 한 모델의 Pretraining 데이터셋이 될 수 있지만, 일상 대화라는 도메인과 특정 도메인을 타겟으로하는 데이터셋은 서로 꽤나 상이하여 Pretraining 하더라도 좋은 결과를 내지 못하고 있다고 합니다.&lt;/p&gt;
&lt;h2&gt;3. Korean Clova Call Speech Corpus&lt;/h2&gt;
&lt;h3&gt;3.1 Naver Clova AI for Contact Center&lt;/h3&gt;
&lt;p&gt;ClovaCall 데이터셋 구축은 &lt;em&gt;NAVER Clova AI for Contact Center (AICC)&lt;/em&gt; 프로젝트의 메인 주제였습니다. ClovaCall 데이터셋은 자연어 이해, 음성인식, 음성합성 등에 활용될 수 있으며, ‘음식점 예약’이라는 시나리오를 주제를 목적으로한 대용량 데이터셋임을 다시 한번 강조합니다.&lt;/p&gt;
&lt;h3&gt;3.2 Data Cconstruction from Humans&lt;/h3&gt;
&lt;p&gt;데이터 구축 과정에 대해 설명합니다. 데이터 구축은 다음 과정을 거쳤다고 합니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;making a Sentence Pool (어떤 문장들로 데이터셋을 구성할지)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;call-based recording (전화상으로 들어온 소리를 녹음)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;refining the recorded speech data (데이터 정제)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;자세한 내용은 논문을 참고해주시면 되겠습니다. (데이터 구축 과정이 제 관심분야는 아닌지라 ㅎㅎ;;)&lt;/p&gt;
&lt;h3&gt;3.3 Statistical Analysis&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/80275445-3a968b00-871c-11ea-86f4-296e88ba1269.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;본 논문에서는 ClovaCall 데이터셋과 AI Hub 데이터셋을 분석한 결과를 보이고 있습니다. &lt;strong&gt;단어, 문자, 음소, 발화 길이&lt;/strong&gt;를 분석하여 막대그래프로 표현했습니다.&lt;/p&gt;
&lt;h2&gt;4. Speech Recognition Result&lt;/h2&gt;
&lt;p&gt;이제 드디어 제가 관심있는 파트가 등장했습니다.&lt;/p&gt;
&lt;h3&gt;4.1 Experimental Setup&lt;/h3&gt;
&lt;h4&gt;Dataset &amp;#x26; Training Schemes&lt;/h4&gt;
&lt;p&gt;앞서 언급했듯이, &lt;strong&gt;Pretraining-finetuning&lt;/strong&gt;, &lt;strong&gt;from-scratch training&lt;/strong&gt;, &lt;strong&gt;scratch training with data augmentation&lt;/strong&gt;와 같은 3가지 방식으로의 학습방식 결과를 비교했습니다.&lt;/p&gt;
&lt;p&gt;데이터셋은 총 3가지를 사용했습니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;AI Hub&lt;/strong&gt; for Pretraining&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;QA Call&lt;/strong&gt; for Finetuning&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clova Call&lt;/strong&gt; for Finetuning&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;AI Hub 데이터셋은 Pretraining을 위해 사용되었고, QA Call 데이터셋은 Clova Call 데이터셋과의 성능 비교를 위해 사용되었습니다. 또한 Noise-Augmentation &amp;#x26; Spec-Augmentation 2가지 기법을 각자 사용하여 결과를 비교했습니다.&lt;/p&gt;
&lt;h4&gt;Data preprocessing&lt;/h4&gt;
&lt;p&gt;먼저 AI Hub 데이터셋과 CLova Call 데이터셋의 샘플링 레이트가 서로 다릅니다. AI HUb는 16k의 샘플링 레이트를 가지는데 반해, Clova Call 데이터는 통화로부터 수집하다보니, 8k의 샘플링 레이트를 가집니다. 그래서 Clova Call 데이터를 Upsampling을 통해 8k =&gt; 16k의 샘플링 레이트를 가지도록 수정했습니다. 그리고 모든 모델은 &lt;strong&gt;log-spectrogram&lt;/strong&gt;을 인풋으로 가지며, 20ms의 &lt;em&gt;window_size&lt;/em&gt;와 10ms의 &lt;em&gt;stride&lt;/em&gt;를 가집니다. (librosa 라이브러리를 사용했습니다.) 또한 모든 스펙트로그램은 &lt;em&gt;instance-wise standardazation&lt;/em&gt; 방식으로 정규화를 진행했습니다.&lt;/p&gt;
&lt;p&gt;개인적으로 왜 Mel-Spectrogram이 아닌, 그냥 Spectrogram을 사용했는지에 대해 궁금증이 남아서, 공동 제 1저자 분 중 한분께 메일로 문의를 드렸습니다. 혹시 답변 해주신다면 답변 내용도 남기겠습니다.&lt;/p&gt;
&lt;h4&gt;ASR Models&lt;/h4&gt;
&lt;p&gt;ASR 모델로는 DeepSpeech2, Listen, Attend and Spell 2개의 아키텍처로 비교했습니다.&lt;/p&gt;
&lt;p&gt;DeepSpeech2 아키텍처는 &lt;em&gt;2D-Convolutional layer with 32 channel&lt;/em&gt;를 포함하고 있으며, 5개의 Bidirectional LSTM layer로 구성되어 있습니다. (각 방향당 800개의 unit을 사용했습니다.) 또한 트레이닝은 CTC loss를 이용하여 학습했다고 합니다.&lt;/p&gt;
&lt;p&gt;LAS 아키텍쳐는 DeepSpeech2와 같은 &lt;em&gt;Convolution layer&lt;/em&gt;를 포함시키고, 각 방향당 512개의 unit을 가진 3층의 &lt;em&gt;Bidirectional LSTM layer&lt;/em&gt;로 인코더를 구성하며, 512개의 unit을 가진 2층의 &lt;em&gt;Unidirectional LSTM layer&lt;/em&gt;로 디코더를 구성했다고 합니다. 또한 어텐션 매커니즘으로는 「Attention based Models for Speech Recognition」- &lt;a href=&quot;https://github.com/sooftware/Paper-Review/blob/master/Review/Attention-Based%20Models%20for%20Speech%20Recognition.md&quot;&gt;Review Link&lt;/a&gt;에서 제안한 Location Aware 어텐션 매커니즘을 사용했습니다.&lt;/p&gt;
&lt;p&gt;모델 평가 지표로는 Character Error Rate (CER)을 사용했으며 다음과 같은 Metric을 따릅니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/80277350-42a8f780-8729-11ea-9609-665ea1944f7b.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;h3&gt;4.2 Comparison Results on Datasets&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/80277359-60765c80-8729-11ea-928f-1f9941c7f36a.png&quot; alt=&quot;image&quot;&gt;&lt;/p&gt;
&lt;p&gt;이제 결과를 비교하는 시간입니다. 결과만 놓고보자면, AI Hub 데이터셋으로 Pretraining 한 후, ClovaCall 데이터셋으로 Finetuning한 LAS 모델이 가장 좋은 성능 (Error Rate 7%) 을 냈습니다.&lt;/p&gt;
&lt;p&gt;또한 주목할만한 점은, AI Hub 데이터셋만으로 학습시킨 모델의 경우, Specific-domain의 데이터로 테스트 했을 때, 성능이 매우 저조했다는 점입니다. 가장 좋은 성능을 낸 LAS 모델로 비교를 하자면 Error Rate 69.2%를 기록했네요.&lt;/p&gt;
&lt;p&gt;또한 본 실험에서는 Data Augmentation이 의미있는 결과를 내지 못했습니다. 본 논문에서는 이를 이미 노이즈가 낀 상황에서 데이터를 만들었다는 점과, 파라미터 수가 작은 LAS 모델에서 Spec Augmentation을 적용했다는 점이 의미있는 효과를 내지 못한 점으로 꼽았습니다.&lt;/p&gt;
&lt;h2&gt;5. Concluding Remarks&lt;/h2&gt;
&lt;p&gt;결국 이 논문의 핵심을 특정 도메인을 대상으로 한 ASR 시스템을 구축하기 위해서는 해당 도메인의 데이터셋이 필요한데, 저희가 그런 ‘음식점 예약’ 도메인에서의 큰 데이터셋을 공개했고, 이 데이터셋을 사용했더니 결과가 좋았습니다 ! 를 강조한 논문이였습니다.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODEL Paper Review]]></title><description><![CDATA[「STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODEL」 Review title https://arxiv.org/abs/1712.0176…]]></description><link>https://gatsby-casper.netlify.com/sota_sr_speech/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/sota_sr_speech/</guid><pubDate>Mon, 03 Feb 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;「STATE-OF-THE-ART SPEECH RECOGNITION WITH SEQUENCE-TO-SEQUENCE MODEL」 Review&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMjRfNjEg/MDAxNTg1MDI5MjM0NzE4.Erz3moRU6RYrvzJltHM-pZ8VS454S_ix0MMob30IT9Ig.0wAmtiJGW_QIIqWabuAESuw0B-kJCGa4C6Mvb7vQPVQg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;title&quot;&gt;&lt;br&gt;
&lt;a href=&quot;https://arxiv.org/abs/1712.01769&quot;&gt;https://arxiv.org/abs/1712.01769&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;본 논문은 제가 진행하는 &lt;a href=&quot;https://github.com/sh951011/Korean-Speech-Recognition&quot;&gt;프로젝트&lt;/a&gt;의 Contributor 분께서 추천해주신 논문으로, 본 논문에서 적용한 Multi-Head Attention을 적용하여 인식률이 향상되었습니다. 또한 본 논문에서는 Word-Piece를 사용하지만, 한국어에서는 Word-Piece 적용시 성능이 저하된다고 합니다. &lt;a href=&quot;https://github.com/sh951011/Korean-Speech-Recognition/pull/9&quot;&gt;Show Issue&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;논문 이름에서부터 밝히듯이, 500h Voice Search 분야에서 &lt;strong&gt;State-Of-The-Art (SOTA)&lt;/strong&gt; 를 달성한 논문입니다.&lt;br&gt;
어텐션 기반의 Seq2seq구조인 Listen, Attend and Spell (LAS) 아키텍쳐를 사용했습니다. LAS 구조는 이전에 음향 모델, 발음 모델, 언어 모델로 구성된 방식에서 하나의 Neural Network로 End-to-End 학습이 가능한 구조입니다.&lt;/p&gt;
&lt;p&gt;본 논문에서는 grapheme (문자 단위) 모델이 아닌 word-piece (단어 단위) 모델을 사용했으며, Multi-Head Attention을 도입했다고 합니다. 그 외에도 최적화를 위해 Synchronous training, scheduled sampling, label smoothing 등을 사용했다고 밝힙니다.&lt;/p&gt;
&lt;p&gt;특이한 점으로 빠른 인식 및 학습을 위해 인코더 부분에 Bidirectional-LSTM이 아닌 Unidirectional-LSTM을 사용했다고 합니다.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sequence-to-Sequence 모델은 Autonomic Speech Recognition (ASR) Task에서 탁월한 성능을 보여주었습니다.&lt;br&gt;
2015년에 「Listen, Attend and Spell」 논문에서 Seq2seq 아키텍처를 도입한 이후로 이 글을 쓰는 지금 2020년까지도 관련 논문들이 많이 나오고 있고 SOTA 모델에서도  Sequence-to-Sequence 모델이 많이 점유하고 있습니다.&lt;/p&gt;
&lt;p&gt;Neural Machine Translation 분야에서 엄청난 성능을 자랑하는 Transformer가 Speech 분야에서는 Transformer 모델이 다소 부진한 듯 합니다. (NMT에서의 압도적인 성능에 비해서입니다 ㅎㅎ..)&lt;/p&gt;
&lt;p&gt;제가 네이버 AI 해커톤 참여 당시, 멘토분께서 Transformer는 데이터가 적을 때 성능이 그렇게 좋지 않다고 하셨습니다. 아마 데이터가 적은 음성 분야라 더 두드러지는 특징이 아닐까 싶습니다. 실제로 네이버 대회 당시 100시간이라는 한정된 데이터로 진행을 하다보니, Transformer를 사용한 팀들이 어텐션 기반의 Seq2seq 모델을 사용한 팀들에게 밀리는 현상이 있었습니다. 본 논문도 Transformer보다 뒤에 나온 논문이지만, Transformer가 아닌 Seq2seq 기반으로 모델을 구성한 것을 볼 수 있습니다. 단, Transformer를 제안한 「Attention Is All You Need」 논문에서 제안된 &lt;strong&gt;Multi-Head Attention&lt;/strong&gt;을 사용했습니다. 또한 뒤에서 다룰 &lt;strong&gt;Word-Piece Model (WPM)&lt;/strong&gt;, &lt;strong&gt;Scheduled Sampling (SS)&lt;/strong&gt;, &lt;strong&gt;label smoothing&lt;/strong&gt;, &lt;strong&gt;Asynchronous SGD&lt;/strong&gt;, &lt;strong&gt;language model&lt;/strong&gt; 등을 사용했습니다. 자세한 내용은 뒤에서 다루겠습니다.&lt;/p&gt;
&lt;h2&gt;System Overview&lt;/h2&gt;
&lt;h3&gt;Basic LAS Model&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMjRfMzgg/MDAxNTg1MDI5MjUyNzIx.A_WNIScwOfaYzJad-l7Hd62NVOkizEQZQhla-zH-lGUg.wwiDAnHkhNJmd_pVcs8sCp4mC68pxGpIDPb7dOvDJW4g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;las-model&quot;&gt;&lt;/p&gt;
&lt;p&gt;LAS 모델을 간략하게 설명해줍니다. Encoder(Listener)는 입력으로 들어온 특징 벡터를 &lt;em&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;/em&gt;라는 higher-level feature로 변환해줍니다. 이러한 &lt;em&gt;&lt;strong&gt;h&lt;/strong&gt;&lt;/em&gt;를 가지고 Decoder(Speller)는 어텐션 매커니즘과 함께 출력을 만들어 냅니다. 이에 대한 자세한 설명은 &lt;a href=&quot;https://github.com/sh951011/Paper-Review/blob/master/Review/Listen%2C%20Attend%20and%20Spell.md&quot;&gt;「Listen, Attend and Spell」 Paper Review&lt;/a&gt;을 참고하시면 좋을 것 같습니다.&lt;/p&gt;
&lt;h3&gt;Word-Piece Models&lt;/h3&gt;
&lt;p&gt;LAS 모델은 아웃풋의 단위가 보통 grapheme (character)이였습니다.&lt;/p&gt;
&lt;p&gt;하지만 이러한 구조는 OOV (Out-Of-Vocabulary) 문제가 발생시킬 수 있습니다. 이를 위한 대안으로, grapheme 단위가 아닌 phoneme (음소) 단위가 있습니다만, phoneme 단위의 단점은 추가적인 발음 모델과 언어 모델이 필요하다는 점입니다. 또한 본 논문에서 실험한 바로는, phoneme 단위는 grapheme 단위 모델보다 성능이 좋지 못했다고 합니다.&lt;/p&gt;
&lt;p&gt;그래서 본 논문은 Word-Piece Model (WPM)을 사용했다고 합니다. WPM은 OOV 문제를 해결할 수 있습니다. 또한, 일반적으로 word-level의 언어 모델은 graphme-level의 언어 모델보다 Perplexity가 낮다고 합니다. (Perplexity가 낮을수록 우수한 모델입니다.) 그래서 본 논문에서는 이러한 경향을 봤을 때, WPM을 사용하게 되면 디코딩 과정에서 더 우수한 성능이 나오지 않을까라고 예상했다고 합니다.&lt;/p&gt;
&lt;p&gt;또한 이러한 WPM으로 진행하게 되면, 기존 문자 단위에 비해 더 적은 디코딩 스텝으로 계산되기 때문에 학습 및 추론 속도가 향상되는 장점도 가지게 됩니다. 그리고 결과적으로, WPM을 사용한 모델이 다른 모델보다 더 좋은 성능을 보였다고 합니다.&lt;/p&gt;
&lt;p&gt;주의할 점으로 Word-Piece 같은 경우, 한국어에서는 오히려 성능이 저하된다고 합니다.&lt;/p&gt;
&lt;h3&gt;Multi-Head Attention&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMjRfMTQ2/MDAxNTg1MDI5MjY0MTU1.UHNwxE6qRO7tM9SrtoPhXoZz-thBq8hLzIgCkokhbe0g.kRmkmI0X3ZCu-AVN9CPpAH4JjPW1GSSFmvl9xRRsQKkg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;MHA&quot;&gt;&lt;/p&gt;
&lt;p&gt;Multi-Head Attention (MHA)은 유명한 「Attention Is All You Need」 논문에서 기계번역 분야를 위해 제안되었습니다. 본 논문은 이러한 MHA를 Speech 분야로 확장해보았다고 합니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAxMjVfMTIg/MDAxNTc5ODg4NDkxMDQz.miNLNdmdj0t3Ll12purypbOIE6PWRFijlxAF4ci5K28g.c-UT98v0QJumGmehmlwGkQ0bQxxV_jCKOCjOVH17ZcYg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;attn-distribution&quot;&gt;&lt;/p&gt;
&lt;p&gt;MHA는 기존의 어텐션을 multiple head로 확장한 구조입니다. 기존 어텐션이 1개의 어텐션 분포를 만들었다면, MHA의 각 head는 서로 다른 어텐션 분포를 만들어 내게 됩니다. 이러한 구조는 각 head가 encoder output의 서로 다른 곳을 Attend하게 해주는 효과가 있습니다.&lt;/p&gt;
&lt;p&gt;본 논문에서는 MHA를 적용 전, 후를 따로 비교하지는 않을 것 같습니다만, 제가 진행하는 음성 인식 프로젝트에서 비교해본 결과 MHA를 적용했던 모델이 압도적으로 좋은 성능을 보였습니다.&lt;/p&gt;
&lt;h3&gt;Scheduled Sampling&lt;/h3&gt;
&lt;p&gt;본 논문에서 적용한 Scheduled Sampling이라는 개념에 대해 서술합니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAxMzFfMTcg/MDAxNTgwMzk4NTc3MzE2.rfhepBTuNa7UuGl7t4O2AAtVytd3Yd2d731im7KZ_jwg.kO58sY_DL9sBLx1LlZzq5A3hAplPA0gJA-6q4ZDr7Owg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;teacher_forcing&quot;&gt;&lt;/p&gt;
&lt;p&gt;Seq2seq 구조에서는 학습을 빠르게 시키기 위해 &lt;a href=&quot;https://blog.naver.com/sooftware/221790750668&quot;&gt;티쳐포싱&lt;/a&gt;이라는 기법을 사용합니다. Seq2seq구조는 원래 이전 타임스텝의 추론 char / word를 다음 타임스텝의 입력으로 넣어야 합니다만, 학습 초기에는 대부분 잘못된 추론 결과가 나오게 됩니다. 이러한 부분을 개선해주기 위하여 이전 타임스텝 추론 결과가 아닌, Ground Truth를 넣어줌으로써 빠른 학습을 가능하게끔 해주는 기법입니다.&lt;/p&gt;
&lt;h4&gt;Exposure Bias Problem&lt;/h4&gt;
&lt;p&gt;하지만 이러한 티쳐포싱 기법에는 단점이 있습니다. 학습 중에는 Ground Truth를 가지고 있지만, 실제 추론 과정에서는 Ground Truth가 없습니다. 그렇기 때문에 학습 과정과 추론 과정에서 차이(discrepancy)가 발생하게 됩니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMjZfMjc2/MDAxNTg1MjI1MjgwMDY2.4R2zFNqKvHotWWe7owD9cDRVO8dUpYZ7A2Sl7D6LbNEg.1DPR5iaZpI3yrEN9Mzzgr9v6KoA43Li4qDb9ZrrD5jog.PNG.sooftware/image.png?type=w773&quot; alt=&quot;ss&quot;&gt;&lt;/p&gt;
&lt;p&gt;본 논문은 이러한 Exposure Bias Problem의 차이를 줄이기 위해서 스케쥴링을 해줍니다.&lt;br&gt;
학습 초기에는 티쳐포싱 100%로 진행이 되지만, 학습이 진행될수록 비율을 점점 낮춰서 최종적으로는 티쳐포싱 60%까지 줄여서 학습을 진행했다고 합니다.&lt;br&gt;
이렇게 스케쥴링 해줌으로써 실제 추론과 학습 단계에서의 차이를 줄였다고 합니다.&lt;/p&gt;
&lt;h3&gt;Label-Smoothing&lt;/h3&gt;
&lt;p&gt;또한 본 논문은 Label-Smoothing을 적용했다고 밝힙니다. Label-Smoothing은 데이터에 대한 Over-Confidence를 조금 덜어주는 역할을 합니다. 아마 Overfitting은 많이 봤겠지만, Over-Confidence는 생소한 분들이 많으실 겁니다. Over-Confidence란 데이터를 너무 믿는다는 겁니다. 아무래도 레이블링이라는 작업이 결국은 사람이 하는 것이다 보니, 어느 정도의 오류가 있습니다. 이러한 오류가 있는 데이터를 학습하다보면 아무래도 정확한 학습하기가 힘듭니다. 그래서 이러한 Over-Confidence를 줄여주기 위하여 Label-Smoothing이라는 개념이 있습니다.&lt;/p&gt;
&lt;p&gt;정확히 말하자면 Label-Smoothing loss입니다. loss를 계산할 때 적용이 됩니다. loss 계산시에, 원-핫 인코딩 되어 있는 레이블링에 의해 정답에 대해서만 loss가 계산되지만, 이때 정답 레이블은 1, 나머지 레이블은 0으로 되어 있는 것이 아니라, 정답 레이블은 confidence, 나머지 레이블은 uncertainty로 바꾸어 loss 계산을 합니다.&lt;/p&gt;
&lt;p&gt;confidence + uncertainty = 1.0이 되도록 설정을 합니다.&lt;/p&gt;
&lt;p&gt;아래는 이를 PyTorch로 이를 구현한 코드입니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;token class-name&quot;&gt;LabelSmoothingLoss&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Module&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; ignore_index&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; smoothing&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token builtin&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;LabelSmoothingLoss&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;__init__&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;confidence &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; smoothing
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;smoothing &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; smoothing
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; vocab_size
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;dim &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; dim
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ignore_index &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; ignore_index

    &lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; logit&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; target&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;no_grad&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            label_smoothed &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;zeros_like&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;logit&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
            label_smoothed&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fill_&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;smoothing &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab_size &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
            label_smoothed&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;scatter_&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; target&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;data&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;unsqueeze&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;confidence&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
            label_smoothed&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;target &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ignore_index&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;label_smoothed &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; logit&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token operator&quot;&gt;&gt;&gt;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; criterion &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; LabelSmoothingLoss&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; ignore_index&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; smoothing&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;  &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Second-Pass Rescoring&lt;/h3&gt;
&lt;p&gt;물론 Seq2seq의 Decoder가 어느 정도의 language model의 성격을 갖습니다만, 훈련 데이터의 텍스트만이 반영되기 때문에 language model로서의 한계점은 분명합니다. 그래서 다른 논문에서도 그러하듯이, 방대한 텍스트 데이터로 학습한 external language model과 결합을 합니다. 다만 이러한 결합은 훈련 과정이 아닌, 추론 과정에서만 결합을 합니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMjZfMTgy/MDAxNTg1MjI2Mjc1Mjg2.29UeaSIPa-Q8Zj240gWng9JP6MDGWKheLHTyEDL8sUsg.iiwPV2VpCFjjsMYlOAck_qHJrrtH_WTntaxBovbhkH8g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;equation&quot;&gt;&lt;/p&gt;
&lt;p&gt;위의 식과 같이, Acoustic Model에서 나온 확률과 Language Model에서 나온 확률, 단어의 개수를 고려하여 Rescoring을 해줍니다.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Attention-Based Models for Speech Recognition Paper Review]]></title><description><![CDATA[Attention-Based Models for Speech Recognition Paper Review title http://papers.nips.cc/paper/5847-attention-based-models-for-speech…]]></description><link>https://gatsby-casper.netlify.com/loc-attention/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/loc-attention/</guid><pubDate>Mon, 20 Jan 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Attention-Based Models for Speech Recognition Paper Review&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMjAw/MDAxNTgxODY4NTQ4MDYw.h3fmR1DnirrDCC-wkSrHptgHrlPX2GPQsnIhI1ulGecg.bIlam2xHjyx9Fdet1be9FvurzHMMNIfxLa2_cY2hapsg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;title&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf&quot;&gt;http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;본 논문에서는 최근 도입된 (당시는 최근이였음) 어텐션 매커니즘이 여러 분야에서 좋은 성능을 보였지만, 음성 인식 분야의 특성을 충분히 반영한 매커니즘은 없었다고 주장한다.&lt;/p&gt;
&lt;p&gt;음성 인식은 NMT 등의 task에 비해 상당히 긴 input sequence를 가진다.&lt;br&gt;
단어 단위로 수개에서 수십개의 인풋을 가지는 NMT에 비해 음성 인식에서는 20 ~ 40ms로 자른 프레임들이 수백~수천개의 인풋으로 들어가게 된다&lt;/p&gt;
&lt;p&gt;본 논문은 이러한 음성 인식 분야의 특성에 맞게 새로운 어텐션 매커니즘을 제안한다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMjAz/MDAxNTgxODY5MjE1NTMx.xNh4qwldRqCtKonZGRH8c1E0yk22yEvYOHIlwAPNbzcg.nZZBOnwPrdlpsKWSqnqVqXslTHfw8noeqGN59MPh2Rwg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;2paper&quot;&gt;&lt;/p&gt;
&lt;p&gt;참고로 본 논문은 2015년 당시 음성 인식 분야에서 “Listen, Attend and Spell” 논문과 함께 Innovation이라고 불릴만큼 큰 파장을 준 논문이였다.
기존 CTC 방식이 압도적이였던 당시에, End-to-End 방식의 포문을 열어준 논문이였기 때문이다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h2&gt;General Framework&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfNiAg/MDAxNTgxODY4NTU3Nzg4.YCElS0j5R6lXK_ryX4m_jkAzjFrZPsEekbDna6LCr4Qg.RxewbBWd3eAVDFcasyCT8VafU2oDZqo_yIyh6RNosbIg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;base_attention&quot;&gt;&lt;/p&gt;
&lt;p&gt;기본적인 어텐션에 대한 큰 그림이다.&lt;br&gt;
(본 논문에서는 α는 alignment, g는 glimpse라고 칭함 )&lt;/p&gt;
&lt;p&gt;어떠한 매커니즘을 거쳐서 alignment (α) 를 구하고 나면, alignment와 인코더의 아웃풋들을 곱해서 glimpse를 구한다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h3&gt;Attention의 개념&lt;/h3&gt;
&lt;p&gt;본 논문에서는 나와 있지 않지만 간단하게 개념을 정리하고 가자면, “alignment는 어떤 인코더를 고려해야 할까?”를 수치화해준 벡터이고, glimpse는 수치화 된 alignment와 인코더의 아웃풋들을 각각 곱해서 현재 디코딩에 필요한 인코더의 정보를 압축한 벡터이다. 그리고 glimpse와 디코더의 아웃풋을 고려해서 현재 스텝의 값을 예측한다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMTIw/MDAxNTgxODY4NTY1MDcy.94IsqEbUQd9q_A2RBdgwWIzrN2ngrPqGdAJAaUVOQDog.ScXnpprsxqBELeteJFikGmXv3RoH2DcxECcDxSkoRe8g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;alignment&quot;&gt;&lt;/p&gt;
&lt;p&gt;그럼 alignment는 어떻게 구하지?&lt;br&gt;
란 물음에 답해주는 부분이다.&lt;/p&gt;
&lt;p&gt;특정 방식으로 Score를 구한 뒤, 해당 점수를 Softmax 함수에 넣어서 전체 값을 0~1의 값으로, 전체 합을 1로 만들어 준다.&lt;br&gt;
=&gt; 각 인코더 아웃풋을 얼마씩 참고할지를 수치화하는 것이다.&lt;/p&gt;
&lt;p&gt;그럼 Score를 구하는 특정 방식은 무엇이냐??&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfODUg/MDAxNTgxODY4NTY5Mzgy.Nbp_BKh56TwrUeOA9GuBNny_OwX2ZfRzbHYz-Oag4dYg.BGNBoYP9qwvFL26yr5AZhQ3RlE17GAER9pBnA7dblhEg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;score_func&quot;&gt;&lt;/p&gt;
&lt;p&gt;어텐션 스코어를 구하는 방법은 위와 같이 다양하다. 사실 위는 정말 몇 개만 뽑아온 것이다.&lt;/p&gt;
&lt;p&gt;어텐션 매커니즘의 종류는 이 스코어 함수가 무엇이냐에 따라 달라진다.&lt;/p&gt;
&lt;p&gt;그리고, 본 논문은 새로운 “스코어 함수”를 제안한 논문인 것이다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;p&gt;본 논문에서는 2가지 어텐션 방식에 주목했다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Content-Based Attention&lt;/li&gt;
&lt;li&gt;Location-Based Attention&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Content-Based Attention&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfNjEg/MDAxNTgxODY4NTkxNzY4.pP5KJnqRAe0qT2fp90t59QJnh7q1cRjETJhDhEdxr4Mg.wK2gs1u_poWfTkwxjPPStLauOB_jVN6Itkz6wrGQHnYg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;content-based&quot;&gt;&lt;/p&gt;
&lt;p&gt;아마 어텐션을 처음 공부할 때에 대부분 Dot-Product Attention으로 배웠을 것이다.&lt;/p&gt;
&lt;p&gt;해당 스텝의 디코더의 출력과 인코더의 모든 출력들을 내적하여 어텐션 스코어를 구하는 방식이다.&lt;/p&gt;
&lt;p&gt;Content-Based Attention은 Dot-Product보다 조금 더 복잡한 수식으로 점수를 낸다.&lt;/p&gt;
&lt;p&gt;단순한 내적이 아닌, 해당 스텝의 디코더의 출력과 인코더의 모든 출력들에 웨이트를 준다.&lt;/p&gt;
&lt;p&gt;그리고 편향 및 Hyperbolic tangent를 걸어주고, 마지막으로 웨이트를 다시 걸어준다.&lt;/p&gt;
&lt;p&gt;Dot-Product Attention에 비해서는 진보된 방법이지만, Content-Based 방식의 문제점은 시퀀스에서의 자신의 위치에 상관없이 스코어링을 한다는 점이다.&lt;br&gt;
이를 “similar speech fragments” 문제라고 한다고 한다.&lt;/p&gt;
&lt;h2&gt;Location-Based Attention&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMTcg/MDAxNTgxODY4NjAyMzU5.csBoosFKJVgVBeHsRCD3fSHokS4MYajHh4lssnQ2bHMg.9P1RL402Y4qyMp6Vfex01uiBTWmUxgAu9zpEArFFvi8g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;location-based&quot;&gt;&lt;/p&gt;
&lt;p&gt;그럼 이번에는 Location-Based 방식을 살펴보자.&lt;br&gt;
이 방식은 alignment 계산시, 해당 스텝 디코어의 출력과, 이전 alignment를 고려해줌으로써, 현재 시퀀스에서 어느 위치인지를 알 수 있게끔 해주는 방식이다.&lt;/p&gt;
&lt;p&gt;하지만 이 방식은 인코더의 아웃풋을 전혀 고려하지 않고, 디코더의 아웃풋만을 가지고 예측하기 때문에 분명한 한계점이 존재한다.&lt;/p&gt;
&lt;h2&gt;Hybrid Attention&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMTQz/MDAxNTgxODY4NjE1Mzc3.SoD_ilkO_r7XfC-kNb36iNq7wR78iAGl7_HBfznB0VIg.czrqHp7DSqJfOuccfHUq91HPXcdpu2MouEDDg4jWfVog.PNG.sooftware/image.png?type=w773&quot; alt=&quot;hybrid&quot;&gt;&lt;/p&gt;
&lt;p&gt;본 논문은 이러한 2 방식의 어텐션을 적절히 결합한 음성 인식용 어텐션을 제안한다.&lt;/p&gt;
&lt;p&gt;( 해당 어텐션을 Hybrid, Location-Aware, Location-Sensitive 등 여러 이름으로 불린다 )&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMjg2/MDAxNTgxODY4NjI5MzA4.IHXwfdQs3EC_YQ4mafpM0XLYbDtcPTg2eGjFDjQnTX8g.Dl63xBYsbahH4MyrwGjmbQgI57WTAbXXsAFjFgHhEJ0g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;hybrid-attention&quot;&gt;&lt;/p&gt;
&lt;p&gt;기존 Content-Based 방식에서 약간의 수식만이 추가됐을 뿐이다.&lt;/p&gt;
&lt;p&gt;기존 Content-Based 방식에서 이전 스텝의 alignment를 고려해준다.&lt;/p&gt;
&lt;p&gt;이때 이전 alignment에 웨이트를 주기 이전에, Convolution으로 1xC의 형상에서 KxC의 형상으로 늘려준다. (C: Classfication Number)&lt;/p&gt;
&lt;p&gt;그리고 해당 행렬에 웨이트를 주어서 Content + location 방식을 완성한다.&lt;/p&gt;
&lt;h2&gt;3 Potential Issue&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMTI0/MDAxNTgxODcxNTc5MzU0.9AzhrTq5SoHmgIIrCcmYBB837p3yL8K09QVH3P6gDrkg.YWKb3qLysPBy6oGkm4MBQa1ty9u7-ktEmPgyjRbee7gg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;Eq6&quot;&gt;&lt;/p&gt;
&lt;p&gt;앞에서 살펴봤던 위의 수식에는 3가지의 이슈가 있다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;인풋 시퀀스가 길다면, glimpse에는 노이즈가 섞여있을 가능성이 크다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;만약 인풋 시퀀스가 길다면, 어떤 시점 t에서 멀리 떨어져 있는 t + k라는 시점에서의 음성과는 서로 관련이 없을 것이다. 하지만 Softmax 함수 특성상, 모든 인풋들에 값을 부여한다. 이러한 Softmax의 특성에 의해 많은 관련없는(irrelevant) 인코더의 출력들이 고려될 것이다. 이는 Noise로 작용된다.&lt;/p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;시간 복잡도가 크다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;인풋 시퀀스의 길이가 L이라고 할 때, 디코더는 매 타임 스텝마다 이 L개의 frame을 고려해주어야 한다. 그리고, 디코딩 길이를 T라 할 때, 위의 과정을 T만큼 반복하게 된다.  이는 O(LT) 라는 높은 시간 복잡도를 만들게 된다.&lt;/p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Softmax 함수는 Single Vector에만 집중 (focus) 하는 경향이 있다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;이러한 경향은 top-score를 받은 여러 프레임을 고려할 수 없게 한다.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Sharpening&lt;/strong&gt; &amp;#x26; &lt;strong&gt;Windowing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;본 논문은 위의 문제를 간단하게 해결하기 위해 “Sharpening”이라는 개념의 제안했다. Softmax 수식을 약간 수정하는 것이다.&lt;br&gt;
&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMjMz/MDAxNTgxODcyNDMyNjU0.drdyx8zV1DOSV-6rezOYqgDQmcFpiqe4U04da8kcvWcg.w_vqXCxu1nd-uCZ-VW395v4AX76Z5hNuK1HYh15c8iMg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;sharpening&quot;&gt;&lt;br&gt;
when, β &gt; 1&lt;/p&gt;
&lt;p&gt;본 논문에서는 inverse temperature를 걸어준다고 표현했다.&lt;br&gt;
위의 수식이 왜 1번 문제를 해결해 주는지에 대해서는 아직 이해를 하지 못하였다.&lt;/p&gt;
&lt;p&gt;그리고 본 논문은 위의 방식이거나, top-k개의 프레임만을 뽑아서 re-normalization을 해주는 방식으로도 해결 가능하다고 말한다.&lt;br&gt;
하지만, 위의 2 방식 모두 2번째 시간복잡도의 문제는 해결하지 못했으며, 2번째 방법의 경우는 오히려 시간 복잡도를 더 늘리게 된다.&lt;/p&gt;
&lt;p&gt;그리고 Windowing이라는 방법이 나오게 되는데, 이전 alignment의 중간값(median)을 기준으로 윈도우 크기 만큼만 고려해주는 방식이다. 해당 방법은 O(L+T)로 시간 복잡도를 낮춰준다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;p&gt;Sharpening은 long-utterance (긴 발화)에서의 퍼포먼스는 개선했지만, 전체적인 퍼포먼스면에서는 좋지 못한 결과로 이어졌다.&lt;br&gt;
(짧은 발화에서는 퍼포먼스가 별로였다)&lt;br&gt;
하지만 해당 실험은 최상위 점수를 받은 프레임들을 선택하여 집계하는 방식이 좋을 것이라는 가정을 하도록 만들었다고 한다.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Smoothing&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;그래서 나오게 된 방법이 Smoothing 방법이다.&lt;br&gt;
&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfNjIg/MDAxNTgxODY4NjM0ODMz.jYTFOEd93R5-IagaKWOyTg3i07Pk7Rwdl1LxsbZAPS8g.r7A0IGeNFdXeSUUl9EB_QWM6EOzb_6N1eHuDGz8_JrIg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;smoothing&quot;&gt;&lt;/p&gt;
&lt;p&gt;위의 식처럼 기존 Softmax 식에 Sigmoid를 추가해준 방식이다.&lt;br&gt;
Sigmoid로 Top-k frame과 아닌 frame들을 구분해주는 방식이라고 나는 이해했다.&lt;br&gt;
이러한 방식은 다양성을 가져온다고 본 논문은 말한다.&lt;/p&gt;
&lt;h2&gt;Result&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMTdfMjkw/MDAxNTgxODY4NjcwNDM4.hknmkkv3qrF8llD9vB2AUALkhuYkUHcuNewXoHv-R-gg.vPQyt_knw2_429fP4jUbdUFU4aMsyexsNCQ7iJi4xb0g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;result&quot;&gt;&lt;/p&gt;
&lt;p&gt;본 논문에서 진행한 실험의 결과이다.&lt;br&gt;
기본 모델보다는 Convolution을 적용한 모델이 더 좋은 결과를 내었고,&lt;br&gt;
Smoothing까지 적용한 모델이 최상의 성적을 내었다.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[SpecAugment Paper Review]]></title><description><![CDATA[SpecAugment: 「A Simple Data Augmentation Method for Automatic Speech Recognition」  Review title https://arxiv.org/abs/1904.08779 Abstract…]]></description><link>https://gatsby-casper.netlify.com/specaugment/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/specaugment/</guid><pubDate>Sun, 12 Jan 2020 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;SpecAugment:&lt;/h1&gt;
&lt;h2&gt;「A Simple Data Augmentation Method for Automatic Speech Recognition」  Review&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTBfMTgw/MDAxNTgzODQ1NTM5MjI3.U9mG8Tl8fKXJU38N7nlTtTKjnZSrXRxUmEPkL7091xgg.Z_56cPISeZT234kYVSOZFChSH32sURm3NE6FVDJCu0Eg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;title&quot;&gt;&lt;br&gt;
&lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;&gt;https://arxiv.org/abs/1904.08779&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;모델의 Overfitting을 막기 위해 가장 좋은 방법은 데이터가 많은 것입니다. 하지만 데이터가 뿅! 하고 생기는 것이 아니기 때문에 기존 데이터를 활용하여 새로운 데이터를 만들어내는 Augmentation이라는 기법을 사용합니다. 본 논문에서는 음성인식을 위한 간단한 Data-Augmentation을 제안하고, 이를 SpecAugment라고 명명했습니다. 본 논문은 오디오에서 뽑은 피쳐 벡터 (MFCC or Mel-Spectrogram etc ..) 를 input으로 Time warping, Frequency masking, Time masking 3가지 방법으로 Augmentation을 적용했습니다. 성능 테스트를 위한 모델로는 &lt;a href=&quot;https://github.com/sh951011/Paper-Review/blob/master/Review/Listen%2C%20Attend%20and%20Spell.md&quot;&gt;「Listen, Attend and Spell」&lt;/a&gt; (LAS) 모델을 사용했으며, Language Model과의 &lt;strong&gt;Shallow Fusion&lt;/strong&gt;을 통해 인식률 개선을 이뤄냈다고 밝히고 있습니다. 본 논문의 모델은 &lt;a href=&quot;http://www.openslr.org/12/&quot;&gt;LibriSpeech 960h&lt;/a&gt; 데이터셋과 &lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC97S62&quot;&gt;Swichboard 300h&lt;/a&gt; 데이터셋에서 &lt;strong&gt;State-Of-The-Art (SOTA)&lt;/strong&gt; 를 달성했습니다. 달성한 결과는 아래 표에 정리했습니다.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;Dataset&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;LibriSpeech 960h&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;LibriSpeech 960h&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Swichboard 300h&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;Swichboard 300h&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;Method&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;No LM&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;With LM&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;No LM&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;With LM&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;Previous&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;-&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;7.5&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;-&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;8.3 / 17.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;strong&gt;LAS + SpecAugment&lt;/strong&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;6.8&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;5.8&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;7.2 / 14.6&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;6.8 / 14.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Data Augmentation&lt;/h3&gt;
&lt;p&gt;자세히 들어가기 앞서, Data-Augmentation이 뭔지 부터 살펴봅시다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTBfMzYg/MDAxNTgzODQ2NjA2MzUy.B4mA43yYLqG_oUSRy1djtBTGUYAI1X4GUFScWfkKsmog.g0SLMSyoMnfneosZJyvJbDiVj7AjiosFxwvs6QRGMdAg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;Augmentation&quot;&gt;&lt;/p&gt;
&lt;p&gt;Augmentation이란, 데이터를 부풀려서 모델의 성능을 향상시키는 기법입니다.&lt;br&gt;
이미지 인식 분야에서 많이 쓰이는 방법으로, 좌우 반전, 사진의 일부 발췌, 밝기 조절 등을 적용하여 한정된 데이터를 조금씩 변형시켜 새로운 데이터처럼 활용하는 방법입니다.&lt;/p&gt;
&lt;h3&gt;Augmentation을 하는 이유&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Preprocessing 및 .Augmentation을 하면 대부분의 경우 성능이 향상된다.&lt;/li&gt;
&lt;li&gt;원본 데이터를 활용하여 추가하는 개념이므로 성능이 저하될 염려가 없다.&lt;/li&gt;
&lt;li&gt;방법이 간단하며 패턴이 정해져 있다.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;단기간에 성능 향상을 원한다면, Ensemble, Augmentation을 활용하라는 말이 있을 정도로 그 효과가 검증됐다고 합니다.&lt;br&gt;
저번 네이버 해커톤 - Speech 대회 참여 당시에도, 상위권 팀들은 Ensemble, Augmentation을 거의 모두 적용했었습니다. 또한 Augmentation을 적용하는 방법은 매우 다양하기 때문에, 여러 방법도 적용이 가능하다는 장점이 있습니다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;딥러닝은 음성인식 분야에 성공적으로 적용이 되었습니다. 하지만, 음성 인식 분야의 연구는 대부분 모델에 초점이 맞춰져서 진행이 되었는데, 본 논문은 이러한 모델들은 쉽게 Overfitting 현상이 발생하며, 많은 양의 데이터를 필요로 한다고 지적하고 있습니다.&lt;/p&gt;
&lt;h3&gt;Traditional Data-Augmentation for Audio&lt;/h3&gt;
&lt;p&gt;그리고 본 논문은 기존의 Augmentation이 어떤 방식으로 적용되었었는지에 대한 설명을 간략하게 합니다.&lt;/p&gt;
&lt;h4&gt;Noise injection&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMTky/MDAxNTgzODYyMTgxNjEz.4-taA67o4zYethS2nkXI7mmgsWVFpBTWGsFSB4eDTNsg.E0EYGEG6zzK9zJ4n7pXLrVu1zYD5ZnGnU14b4NQtnOcg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;noise-ingection&quot;&gt;&lt;/p&gt;
&lt;p&gt;기존 데이터에 임의의 난수를 더하여 Noise를 추가해주는 방법입니다.&lt;/p&gt;
&lt;h4&gt;Shifting Time&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfODYg/MDAxNTgzODYyMjA2ODU3.cdo4B6N7B_3ut6Cg-fB2XhKnXRyM7t_inYtCJ_11PYQg.LRzvVgjn3bKR-maieujxTC-XF5BVTNb8LdZcJzamkqQg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;shiftting-time&quot;&gt;&lt;/p&gt;
&lt;p&gt;임의의 값만큼 음성 신호를 좌/우로 shift하고 빈 공간은 0으로 채우는 방법입니다.&lt;/p&gt;
&lt;h4&gt;Changing Pitch&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMjI3/MDAxNTgzODYyMjI2Mzk2.ebxn9cuq8oDWMfWZTaDH-oncrBjRr4A-SWVYB9ozbtQg.OtYDyy_sMrgTgDl3-6b-_TW61Nq80NEYzPfdEGf6oR4g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;changing-pitch&quot;&gt;&lt;/p&gt;
&lt;p&gt;기존 음성 신호의 Pitch(음높이, 주파수)를 랜덤하게 변경해주는 방법입니다.&lt;/p&gt;
&lt;h4&gt;Changing Speed&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMTEg/MDAxNTgzODYyMjQ0ODU3.3UCMz-mY72XLLtQOuKn5_RmR_W7aB2o827b73Qa1i20g.ltl5l-7WVHOba5HsxSuU1QXjJ2Pcoyymh4blItMZb5Ig.PNG.sooftware/image.png?type=w773&quot; alt=&quot;changing-speed&quot;&gt;&lt;/p&gt;
&lt;p&gt;기존 음성 신호의 속도를 빠르게 / 느리게 바꿔주는 방법입니다.&lt;/p&gt;
&lt;p&gt;기존 음성 신호에 대한 Augmentation은 위와 같이 raw audio를 변형하는 방법들이었습니다.&lt;br&gt;
하지만 본 논문에서는 이와 같이 주장합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;어차피 사용하는 피쳐는 MFCC / log mel spectrogram인데, 이쪽을 변형하는게 쉽고 빠르지 않아?&quot;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;또한 이러한 방법을 이와 같이 표현합니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;text&quot;&gt;&lt;pre class=&quot;language-text&quot;&gt;&lt;code class=&quot;language-text&quot;&gt;This method is simple and computationally cheap to apply.&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;log mel spectrogram을 이미지처럼 다루는 겁니다. 이렇게 계산 비용이 적게 들기 때문에 학습을 하면서 바로바로 Augmentation을 적용할 수 있었다고 합니다. SpecAugment는 앞에서 언급했듯이 3가지 종류의 변형을 적용했습니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Time Warping&lt;/li&gt;
&lt;li&gt;Frequency Masking&lt;/li&gt;
&lt;li&gt;Time Masking&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Augmentation Policy&lt;/h2&gt;
&lt;p&gt;그럼 이제 본 논문에서 제안하는 SpecAugment에 대해 상세하게 알아봅시다.&lt;br&gt;
별로 어렵지 않은 내용이라, 쉽게 이해가 되실거라 생각합니다.&lt;/p&gt;
&lt;h3&gt;Time Warping&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfOTIg/MDAxNTgzODYzOTk4NTc5.aKgX-flYzAN8VNF5lmJc7NntL8DjJpch06p0Ut7JqnMg.ds3DwlQ4tkTsQBXGigU2galWpbF4ViVZeNOBGr3DUoYg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;time-warping&quot;&gt;&lt;/p&gt;
&lt;p&gt;Computer Vision에서 사용되는 Image Warping을 응용한 방법입니다.&lt;br&gt;
축의 중심을 이동한다(?)라고 생각하시면 되는데 아마 감이 잘 안오실 겁니다.&lt;/p&gt;
&lt;p&gt;쉽게 생각해보자면 다음과 같습니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMTc5/MDAxNTgzODY0Mzk3Mzky.3pC-fk4QAM6wsnU4g7vlnAsFt35N7OF1y_YCHeTYztEg.Yq9RrVZ8kGKhZ01ct1Hw0EBrcVHn-I1Bdgo8av0jZiYg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;cloth-warp&quot;&gt;&lt;/p&gt;
&lt;p&gt;위와 같이 보자기의 중심에 손가락을 가져다가 한쪽으로 밀게되면 우측의 이미지와 같이 보자기가 꾸겨지게 됩니다.&lt;br&gt;
(보자기가 없어 수건으로 사진을 찍었습니다 ㅎㅎ..)&lt;/p&gt;
&lt;p&gt;하지만, 우측 이미지를 보더라도 우리는 보자기라는 것을 알 수 있습니다.&lt;br&gt;
이러한 점을 이용해서 Vision에서는 Image Warp라는 Augmentation 방법을 성공적으로 적용하였고, 본 논문은 여기에 영감을 받아, log mel spectrogram을 이미지라 생각하고, Time Warp를 적용합니다.&lt;/p&gt;
&lt;h3&gt;Frequency Masking&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMTAg/MDAxNTgzODY1MzI0MzA4.oTiZPL3trPDPxFtY5AUqHtZ6k84rgbHoxT2zasWh-xog.AvXKw4UbnQULe6SAxIp9x56HmyQUOhVhG29GqFcESIgg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;freq-mask&quot;&gt;&lt;/p&gt;
&lt;p&gt;굉장히 간단한 방법입니다.&lt;br&gt;
주파수와 시간 축으로 이루어진 Spectrogram의 주파수 축을 따라 일정 영역을 0으로 마스킹해버립니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;freq_masking&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;feat&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; F &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; freq_mask_num &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    feat_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; feat&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;size&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    seq_len &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; feat&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;size&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# freq mask&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; _ &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;freq_mask_num&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        f &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;low&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; high&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;F&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        f &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;f&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        f0 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;randint&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; feat_size &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; f&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        feat&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; f0 &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; f0 &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; f&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; feat&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;Time Masking&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMjgz/MDAxNTgzODY1MzQ5Mjg1.QTTK1udl7K0Pj6CmZIIKBxGOWF9EGN4uLotWePBzwUgg.NN_pxzM6tsxuVOD0NtItMf0LcLPnK891uCT_kfyXyikg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;time-mask&quot;&gt;&lt;/p&gt;
&lt;p&gt;Frequency Masking과 똑같습니다.&lt;br&gt;
다만, 주파수 축기 아닌, 시간 축에 대해서 일정 영역을 0으로 마스킹해버립니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;time_masking&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;feat&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; T &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; time_mask_num &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    feat_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; feat&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;size&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    seq_len &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; feat&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;size&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# time mask&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; _ &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;time_mask_num&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        t &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;low&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; high&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;T&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        t &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;t&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        t0 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;randint&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; seq_len &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; t&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        feat&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;t0 &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; t0 &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; t&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; feat&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Frequency Masking과 Time Masking 적용 시 주의점은, 마스킹하는 영역의 범위를 적당하게 지정해주어야 합니다.&lt;br&gt;
너무 많이 / 적게 적용한다면 Augmentation의 효과가 덜하거나 심한 경우 Noise가 될 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMjQ0/MDAxNTgzOTI5MTczODMw.h_LZ2BHfuKenmYoYm03R39JkHzkwb5pwJFr5Anevk94g.3z7LCzVNzqyvP5RbyJy6CLJgvSr6JiIm2v-Q3qBg3K0g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;single-apply&quot;&gt;&lt;/p&gt;
&lt;p&gt;Figure 1은 위에서 아래 방향으로 기존 Spectrogram, Time Warp, Frequency Mask, Time Mask가 각각 적용된 Spectrogram입니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMTMx/MDAxNTgzOTI5MTg0MTM4.U2ewywMqX-5_QiZhBSzjqiaDCbbN2htdwSKqy4hBiPgg.NKf6UT6SZdaj6ChHloytSNkvIDvmaUgdy-ZgH-QDvbgg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;double-apply&quot;&gt;&lt;/p&gt;
&lt;p&gt;본 논문은 Frequency Masking과 Time Masking을 동시에 적용하는 것을 고려했다고 합니다. 2 마스킹을 동시에 적용하게 되면 Figure 2와 같은 Spectrogram이 나오게 됩니다.&lt;/p&gt;
&lt;p&gt;본 논문은 각각 적용하는 것과 동시에 적용하는 실험을 진행했고, 결과로 나온 파라미터는 다음과 같습니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTBfMTcg/MDAxNTgzODQ1NTc2OTA0.oSNOpjK3E4FZxkQrBwq6g9b_2fhCrgOlZ8P14vj48Vog.bVBDzCGI5z10AI5BfjSkLaEzaWW-sFFM8WGq0zla-EUg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;experiment-table1&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LB : LibriSpeech Basic&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LD : LibriSpeech Doucle&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SM : Switchboard Mild&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SS : Switchboard String&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Frequency Masking과 Time Masking을 동시에 적용하는 코드는 아래와 같이 사용하시면 됩니다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;code&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;spec_augment&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;feat&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; T &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; F &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; time_mask_num &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; freq_mask_num &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    feat_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; feat&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;size&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    seq_len &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; feat&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;size&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# time mask&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; _ &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;time_mask_num&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        t &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;low&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; high&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;T&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        t &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;t&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        t0 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;randint&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; seq_len &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; t&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        feat&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;t0 &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; t0 &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; t&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;token comment&quot;&gt;# freq mask&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;for&lt;/span&gt; _ &lt;span class=&quot;token keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;freq_mask_num&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        f &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; np&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;uniform&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;low&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; high&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;F&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        f &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;f&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        f0 &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; random&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;randint&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; feat_size &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; f&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        feat&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; f0 &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; f0 &lt;span class=&quot;token operator&quot;&gt;+&lt;/span&gt; f&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; feat&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Model&lt;/h2&gt;
&lt;p&gt;본 논문은 &lt;a href=&quot;https://github.com/sh951011/Paper-Review/blob/master/Review/Listen%2C%20Attend%20and%20Spell.md&quot;&gt;「Listen, Attend and Spell」&lt;/a&gt; 모델을 사용했습니다. LAS 모델 같은 경우는 음성 인식 분야에서 end-to-end의 대표적인 모델로써, 구조가 간단하며, 관련 연구도 많이 진행된 구조입니다. 첫번째 절에서 이 모델에 대한 Review 및 파라미터들에 대해 소개하고, 2번째 절에서는 Learning Rate Schedules에 대해 다룹니다. 이 Learning Rate Schedule은 퍼포먼스에 많은 영향을 미쳤다고 소개합니다. 또한 앞에서 언급했던 shallow fusion에 대해서 3번째 절에서 다룹니다.&lt;/p&gt;
&lt;h3&gt;LAS Network Architectures&lt;/h3&gt;
&lt;p&gt;본 논문은 LAS Network 중 &lt;a href=&quot;https://arxiv.org/abs/1902.01955&quot;&gt;「Model Unit Exploration for Sequence-to-Sequence Speech Recognition」&lt;/a&gt;에서 사용된 구조를 사용했다고 밝힙니다. ( 제가 진행하고 있는 한국어 음성인식 프로젝트도 역시 LAS Network를 사용하기 때문에 해당 논문도 읽고 리뷰를 쓸 예정입니다. )&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/7529838/33699263-69206498-db55-11e7-8295-029e0b012f32.png&quot; width=&quot;500&quot;&gt;  
&lt;p&gt;해당 논문은 log mel spectrogram을 입력으로 받아, 2-Layer의 maxpooling이 적용된 CNN을 거칩니다. (stride = 2) 그리고 이렇게 CNN을 거쳐서 나온 아웃풋을 인코더의 stacked Bi-LSTM의 입력으로 넣습니다. 그리고 인코딩을 거친 아웃풋을 어텐션 기반의 디코더에 넣어 예측 시퀀스를 뽑아냅니다. (디코더 레이어 사이즈 = 2)&lt;/p&gt;
&lt;h3&gt;Learning Rate Schedules&lt;/h3&gt;
&lt;p&gt;이 섹션에서는 학습율을 어떻게 관리했는지에 대해서 소개하고 있습니다. 이렇게 하나의 학습율을 사용하는 것이 아닌, 학습 도중 학습율을 조정하면서 사용하는 것을 Multi-step Learning Rate라고 합니다. 본 논문에서는 총 4단계의 Learning Rate Scheduling을 적용했습니다.&lt;/p&gt;
&lt;p&gt;다음 그림으로 보시면 이해가 조금 더 쉬울 겁니다.&lt;/p&gt;
&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMjc5/MDAxNTgzOTMxNTM4ODE1.MwAId31eYiiH2o8B4F_JB4alld4r_h2EbkX9I6LJzZsg.enwL-u3ws1Y3Xz9hJRPqLGsJ3h9uX-lSSsB1WCpNR1Ig.PNG.sooftware/image.png?type=w773&quot; width=&quot;500&quot;&gt;  
&lt;p&gt;좌측의 lr의 특정 값은 제가 진행하고 있는 프로젝트에서 적용한 값이므로 무시하셔도 좋습니다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ramp-up&lt;/strong&gt;: 학습율이 0부터 시작하여 특정 값까지 급격하게 증가시키는 구간입니다. [0, s_r]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;High Plateau&lt;/strong&gt;: 특정 값에 다다르면 학습율을 유지시키는 구간이 High Plateau입니다. [s_r, s_i]&lt;br&gt;
&lt;strong&gt;Exponential Decay&lt;/strong&gt;: 스텝이 s_i에 다다르면, s_f까지 High Plateau에서 사용한 학습율의 1 / 100로 지수적으로 감소시키면서 진행합니다. [s_i, s_f]&lt;br&gt;
&lt;strong&gt;Low Plateau&lt;/strong&gt;: 이 시점 이후에는 학습률을 계속 유지합니다. [s_f, ~]&lt;/p&gt;
&lt;p&gt;High Plateau 구간 중 [s_r, s_noise]까지는 학습율에 deviation이 0.075인 noise를 끼워서 진행하고, s_noise 이후에는 기존 학습율을 유지한다고 합니다. 학습율이 가장 중요한 하이퍼파라미터라는 말답게 상당히 많은 고민을 한 모습입니다.&lt;/p&gt;
&lt;p&gt;그리고 본 논문에서는 이러한 구간을 총 3개로 나눠서 실험을 진행했습니다.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;B&lt;/strong&gt;(asic): (s_r, s_noise, s_i, s_f) = (0.5K, 10K, 20K, 80K)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;D&lt;/strong&gt;(ouble): (s_r, s_noise, s_i, s_f) = (0.5K, 20K, 40K, 160K)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;L&lt;/strong&gt;(ong): (s_r, s_noise, s_i, s_f) = (1K, 20K, 140K, 320K)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;이에 대한 실험의 결과는 뒤에서 살펴보겠습니다.&lt;/p&gt;
&lt;h3&gt;Label-Smoothing&lt;/h3&gt;
&lt;p&gt;또한 본 논문은 Label-Smoothing을 적용했다고 밝힙니다. Label-Smoothing은 데이터에 대한 Over-Confidence를 조금 덜어주는 역할을 합니다. 아마 Overfitting은 많이 봤겠지만, Over-Confidence는 생소한 분들이 많으실 겁니다. Over-Confidence란 데이터를 너무 믿는다는 겁니다. 아무래도 레이블링이라는 작업이 결국은 사람이 하는 것이다 보니, 어느 정도의 오류가 있습니다. 이러한 오류가 있는 데이터를 학습하다보면 아무래도 정확한 학습하기가 힘듭니다. 그래서 이러한 Over-Confidence를 줄여주기 위하여 Label-Smoothing이라는 개념이 있습니다.&lt;/p&gt;
&lt;p&gt;정확히 말하자면 Label-Smoothing loss입니다. loss를 계산할 때 적용이 됩니다. loss 계산시에, 원-핫 인코딩 되어 있는 레이블링에 의해 정답에 대해서만 loss가 계산되지만, 이때 정답 레이블은 1, 나머지 레이블은 0으로 되어 있는 것이 아니라, 정답 레이블은 confidence, 나머지 레이블은 uncertainty로 바꾸어 loss 계산을 합니다.&lt;/p&gt;
&lt;p&gt;confidence + uncertainty = 1.0이 되도록 설정을 합니다.&lt;/p&gt;
&lt;p&gt;아래는 이를 PyTorch로 이를 구현한 코드입니다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;token class-name&quot;&gt;LabelSmoothingLoss&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;nn&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;Module&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; ignore_index&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; smoothing&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token builtin&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;LabelSmoothingLoss&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;__init__&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;confidence &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; smoothing
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;smoothing &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; smoothing
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab_size &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; vocab_size
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;dim &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; dim
        self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ignore_index &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; ignore_index

    &lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; logit&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; target&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;with&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;no_grad&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
            label_smoothed &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;zeros_like&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;logit&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
            label_smoothed&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;fill_&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;smoothing &lt;span class=&quot;token operator&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;vocab_size &lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
            label_smoothed&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;scatter_&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; target&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;data&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;unsqueeze&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;confidence&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
            label_smoothed&lt;span class=&quot;token punctuation&quot;&gt;[&lt;/span&gt;target &lt;span class=&quot;token operator&quot;&gt;==&lt;/span&gt; self&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;ignore_index&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; torch&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;token builtin&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;label_smoothed &lt;span class=&quot;token operator&quot;&gt;*&lt;/span&gt; logit&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;token operator&quot;&gt;&gt;&gt;&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;&gt;&lt;/span&gt; criterion &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; LabelSmoothingLoss&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;vocab_size&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; ignore_index&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; smoothing&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; dim&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;token operator&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;token number&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;  &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;본 논문은 confidence는 0.9, uncertainty는 0.1을 적용했다고 합니다.&lt;/p&gt;
&lt;h3&gt;Shallow Fusion with Language Model&lt;/h3&gt;
&lt;p&gt;Augmentation만으로도 State-Of-The-Art를 달성했지만, 조금 더 개선하기 위해 Language Model과 Shallow Fusion을 진행했다고 합니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTFfMTM2/MDAxNTgzOTMzMjUyMzE5.OeKVkHOPDc8bAkcPAWfKnzdZTrlBgN_YyFmMJC2OcpYg.rX6CAAKNiOA0wE4koc2p7dlArfXJ2iqbElNHmP_Zf6Ug.PNG.sooftware/image.png?type=w773&quot; alt=&quot;shallow-fusion&quot;&gt;&lt;/p&gt;
&lt;p&gt;ASR 모델에서 나온 log-probability와 LM 모델에서 나온 log-probability를 적절히 고려해주어서 y_hat을 결정하게 됩니다. 앞에서 언급했었던 성능향상을 위해 적용하는 기법 중 하나인 Ensemble과 비슷한 효과를 내는 방법이라고 합니다.&lt;/p&gt;
&lt;h2&gt;Experiments&lt;/h2&gt;
&lt;p&gt;실험 결과에 대한 자세한 설명은 생략하겠습니다.&lt;br&gt;
아래 표를 참고 혹은 &lt;a href=&quot;https://arxiv.org/abs/1904.08779&quot;&gt;본 논문&lt;/a&gt;을 참고하시면 자세한 결과를 보실 수 있습니다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTBfMTcx/MDAxNTgzODQ1NTk4MzE2.tMzhyDck9DbiCaujYGfjzqKPD7gmqqtbFqiYSw5zQIIg.q-FVlIQo_F3F1iDPPECf2SHlczIV9KO-tK5oOQhI5RIg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;table-2&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTBfMjYw/MDAxNTgzODQ1NjEyODgw.-45_JNKhgLATY6TdLVOpdSNWbWf4KURTYQWT1np7oY4g.DjX7ThjfaXDKEareVgQ0J_A0X_rlcSN8C39l3fxnBM4g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;table-3&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAzMTBfMjgg/MDAxNTgzODQ1NjU0NzU0.8NOTM44yvEVFRPGaoG0XLUSUkDFaHWhhryHoPgloVYgg.kmqtc_t6koHaA16c8ji-1X2VSev0pbprlrlAlcT-AXMg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;table-4&quot;&gt;&lt;/p&gt;
&lt;h2&gt;Discussion&lt;/h2&gt;
&lt;p&gt;자, 이제 얻어진 결과에 대해 해석해보는 시간입니다.&lt;/p&gt;
&lt;h3&gt;Time waiping contributes, but is not a major factor in improving performance.&lt;/h3&gt;
&lt;p&gt;제안한 Time Warp, Frequency Masking, Time Masking 중 Time Warp는 계산은 오래 걸리는데 반하여, 성능이 그리 좋지는 않습니다. 그래서 학습시간이 넉넉치 않다면 Frequency Masking, Time Masking만을 적용하더라도 충분한 결과를 얻을 수 있을 것이라고 언급하고 있습니다.&lt;/p&gt;
&lt;h3&gt;Label smoothing introduces instability to training.&lt;/h3&gt;
&lt;p&gt;Label Smoothing은 Augmentation과 같이 적용될 때 눈에 띄는 성과를 냈다고 언급합니다. 그 이유에 대해 추측해보자면, Masking, Warp와 같은 조작이 들어가게 되면 어느 정도의 변형이 된 것이기 때문에 완벽하게 ~~한 데이터라고 표현할 수는 없을 것입니다. 그래서 이러한 Confidence를 줄여주는 Label-Smoothing과 Collaboration이 되면 더 큰 효과를 내는 것이 아닐까 추측해봅니다 ㅎㅎ..&lt;/p&gt;
&lt;h3&gt;Augmentation converts an over-fitting problem into an under-fitting problem.&lt;/h3&gt;
&lt;p&gt;Augmentation은 오버피팅 되는 문제를 언더피팅으로 바꿔주는 효과가 있다는 말입니다. Augmentation이 적용 되지 않은 데이터셋으로만 학습을 하게 되면, 아무래도 오버피팅이 날 확률이 높습니다. 하지만, Augmentation을 적용해주게 되면 아무래도 기존의 Training 데이터셋에 대하여 Overfitting이 나기 힘든 환경이 될 것입니다. 본 논문에서는 이를 over-fitting =&gt; under-fitting 되는 효과가 있다고 표현했습니다.&lt;/p&gt;
&lt;h3&gt;Common methods of addressing under-fitting yield improvements.&lt;/h3&gt;
&lt;p&gt;그럼 이때 발생하는 under-fitting 문제를 어떻게 해결했는지에 대한 답입니다. 간단합니다. 네트워크를 깊게 만들고 학습을 오래시키면 됩니다. 보통 over-fitting이 문제지, under-fitting이 문제라면 전통적인 방법인 네트워크를 깊게하고, 학습을 오래시키면 해결 가능합니다.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;다른 논문들은 인식률 개선을 위해 &lt;strong&gt;Network&lt;/strong&gt;에 집중할 때, Augmentation, Learning Rate Schedule, Loss 계산 등에 집중해서 &lt;strong&gt;State-Of-The-Art&lt;/strong&gt;를 달성한 “기본에 충실하자”라는 깨달음을 준 논문입니다. 또한 제가 진행하고 있는 한국어 음성 인식 프로젝트에 많은 영감을 줬고, 실제로 논문에 등장한 거의 대부분의 내용을 적용하여 학습을 진행중입니다. 기회가 된다면 해당 모델로 나온 결과에 대해서도 리뷰하겠습니다. 읽어주셔서 감사합니다.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[DeepSpeech Paper Review]]></title><description><![CDATA[Deep Speech: Scaling up end-to-end speech recognition title https://arxiv.org/pdf/1412.5567.pdf (Awni Hannun et al. 2014) Abstract…]]></description><link>https://gatsby-casper.netlify.com/deepspeech/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/deepspeech/</guid><pubDate>Mon, 11 Nov 2019 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;Deep Speech: Scaling up end-to-end speech recognition&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjNfMTQ1/MDAxNTgyMzkwNTAyMzI5.qYtgph7nxA4sOZlHd8-dw9dOmXeEZvz3zifBjyMYNaUg.uX_2ZheLYRPxPHJogipB50IrpYX7yYi5jNPWbWGv2sog.PNG.sooftware/image.png?type=w773&quot; alt=&quot;title&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1412.5567.pdf&quot;&gt;https://arxiv.org/pdf/1412.5567.pdf&lt;/a&gt; (Awni Hannun et al. 2014)&lt;/p&gt;
&lt;h2&gt;Abstract&lt;/h2&gt;
&lt;p&gt;본 논문은 2014년도에 나온 논문이다. 논문을 읽어본 결과, 당시에는 음성인식에 End-to-End 방식의 딥러닝을 적용한 사례가 없던 모양이다. (또는 주목할만한 성과가 나오지 않았던 모양이다.) 본 논문에서는 End-to-End 방식으로 Switchboard Hub5’00 데이터셋에서 16.0%의 Error Rate를 기록하며 &lt;strong&gt;State-Of-The-Art&lt;/strong&gt; (SOTA) 를 달성한 성과를 밝히고 있다. 기존 음성인식의 traditional한 방식은 전처리 과정이 상당히 많이 필요했지만, 이러한 과정 없이 데이터와 레이블만을 이용한 End-to-End 방식으로 이러한 성과를 냈음을 거듭 강조하고 있다. 그리고 이전 방식으로는 노이즈가 있는 환경에서 급격히 떨어졌는데 반하여, 본 논문 방식으로는 노이즈가 있는 환경에서도 좋은 성능을 기록했다고 한다. 즉, 기존 방식보다 더 간단하면서도 좋은 성능을 기록한 End-to-End 방식의 Speech-Recognition 모델을 소개하는 논문이다.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Abstract에서 강조한 내용을 다시 한번 강조한다. 기존 traditional한 방식은 전문가들의 손이 많이 가야했다. (노이즈 필터링 등..) 하지만 그러한 노력에도 불구하고, 실제 노이즈가 낀 상황에서의 인식률은 좋지 못했다. 하지만 End-to-End 방식으로 이러한 2가지의 단점을 개선할 수 있다고 주장한다.&lt;/p&gt;
&lt;p&gt;물론 End-to-End 방식으로 가기 위해서는 몇가지 고려해야할 사항들이 있었지만, 본 논문에서는 기존 연구 결과들을 참고하여 End-to-End 방식을 시도할 수 있었다고 말한다. 그리고, 본 논문에서는 RNN 모델을 사용했다. 뒤에 더 자세히 설명하겠지만 본 논문에서는 학습 시간을 단축시키기 위해 많은 고려를 한 것으로 보인다.&lt;/p&gt;
&lt;h2&gt;RNN Training Setup&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjNfMjcy/MDAxNTgyMzkwNTUyMDky.Btvd1sq4aVjkLzhMoXlSY5zrjNa8WyfqUAHi7NeFtcog.mFCsfZv-R3VDe_Gt-PZYr7R-Ymc3x84GWlcRX7-SZyIg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;model&quot;&gt;&lt;/p&gt;
&lt;p&gt;해당 챕터에서는 자신들이 어떤 식으로 모델을 구성했는지에 대해 설명한다.&lt;br&gt;
모델의 핵심은 RNN으로 구성되어 있으며, 트레이닝 셋은 ${(x_1, y_1), (x_2, y_2), … (x_t, y_t)}$ 와 같은 딕셔너리 형식으로 구성했다고 한다. (x는 스펙트로그램, y는 문자로 구성 )&lt;/p&gt;
&lt;p&gt;모델은 총 5개의 히든 레이어로 구성했다고 한다.
논문을 읽으면서 모델 아키텍쳐가 상당히 특이하다고 생각했다.&lt;/p&gt;
&lt;p&gt;현재 내가 알고있는 방식과는 사뭇 다른 방식이였는데, 히든 레이어 중 1, 2, 3번째 레이어는 병렬적 (Parallel) 하게 처리하기 위해 서로 독립적으로 포워딩 된다고 한다. RNN 아키텍쳐를 사용하게 되면 이전 셀의 아웃풋이 필요하기 때문에 어쩔 수 없이 병렬처리의 한계점이 있기 때문에 학습 속도 개선을 위해 각 인풋을 독립적으로 처리했다고 한다.&lt;br&gt;
여기서도 나는 본 논문이 학습 시간을 단축시키기 위해 상당히 노력했다는 인상을 받았다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjNfMTgz/MDAxNTgyMzkwNTIyMDQ0.Tq2hNX4U6a4mmT4V3f2sWemnruUnzCuLJYP5v5x8LlEg.8oF99BbkoyhYbmWQL0dD5LiWhNl61qe33P8OnGOG_Hsg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;forward&quot;&gt;&lt;/p&gt;
&lt;p&gt;위의 수식을 통해 포워딩이 진행되는데, 여기서 g는 최소 0, 최대 20의 값을 가지는 ReLU 함수이다.&lt;br&gt;
그리고 4번째 레이어에서는 Bidirectional-RNN으로 구성했다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjNfMjI0/MDAxNTgyMzkwNTMyNzM2.yNOOR59yjQmH7I1v8h-YtNJ-S8JSf5q9oSiMHn1JGcAg.mrjf5ofIgIa2CQBbzZZG0ncV7JxxxR9pRcXY5ZzaG00g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;Bi-RNN&quot;&gt;&lt;/p&gt;
&lt;p&gt;(f)는 정방향 (forward), (b)는 역방향 (backward)을 표현한 것이다.&lt;br&gt;
이때 주의할 점으로는, forward는 t = 1 에서 t = T 방향으로 흐르고, backward는 t = T에서 t = 1 방향으로 흐른다는 점이다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjVfMzkg/MDAxNTgyNjIxNjY0MzMw.CdMnrGnCNt8FrFb102hZFmRldRA1Xp_0kpWoFUbUv4sg.-VSEzem8ZH_3ry2W29awhSRASuz4Cb0MjIJnmwp6zcEg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;5-layer&quot;&gt;&lt;/p&gt;
&lt;p&gt;그리고 마지막 5번째 레이어는 이렇게 forward, backward의 결과에 웨이트를 주고 1, 2, 3 번째 레이어와 동일한 ReLU를 활성화 함수로 사용했다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjNfODIg/MDAxNTgyMzkwNTQxMjI0.bn6ow6MHo58KaEaVu33JM8rzSvIYZqxjRpJMeKauUgsg.QumDhUt_Kx0Gqv_psT9xcHXmr44HLcGtELb7BkphlKsg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;softmax&quot;&gt;&lt;/p&gt;
&lt;p&gt;그리고 이렇게 나온 결과는 Softmax 함수에 넣어서 최종적으로 Classfication을 진행한다. 또한 loss 계산시에는 CTC loss를 사용했다.&lt;/p&gt;
&lt;p&gt;여기서 또 특이했던 점으로, LSTM이 아닌 기본 RNN을 사용했다는 점이다.&lt;br&gt;
그 이유로 본 논문에서는 LSTM의 단점은 메모리가 많이 소요되고, 학습이 오래걸린다는 점을 꼽았다.&lt;/p&gt;
&lt;p&gt;총 5개의 히든 레이어 중 실제 RNN 계층은 1개 뿐이라는 점과, LSTM이 아닌 RNN을 사용했다는 점이 인상깊었다.&lt;br&gt;
최근 연구에서는 기본 RNN을 사용하는 모습을 거의 볼 수 없는데, 당시에는 아직 GPU의 성능이 그리 좋지 않던 때라 그런지 학습 속도에 대해 굉장히 고려를 많이한 모습이 보였다.&lt;/p&gt;
&lt;h3&gt;Regularization&lt;/h3&gt;
&lt;p&gt;본 논문은 학습 시, 드랍아웃 비율을 5 - 10%정도를 유지했다고 한다.&lt;br&gt;
그리고 Spectrogram의 프레임 길이는 10ms, 포워딩은 5ms를 사용했다.&lt;br&gt;
(해당 부분은 오류가 있을수도 있습니다)&lt;/p&gt;
&lt;p&gt;통상적으로 음성 인식에서 프레임 길이는 20 - 40ms를 사용하기 때문에 프레임 길이가 상당히 짧다고 생각했다.&lt;br&gt;
해당 부분은 다른 이유가 있어서 짧게 한 건지, 당시에 프레임 길이에 대한 연구가 현재보다 덜 발달해서 그런 것인지는 확인을 해봐야 할 듯 하다.&lt;/p&gt;
&lt;h3&gt;Language Model&lt;/h3&gt;
&lt;p&gt;본 논문의 모델은 성능 테스트시에, 정확히 맞추거나 그럴싸하게 틀렸다고 한다.&lt;br&gt;
&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjNfOTEg/MDAxNTgyMzkwNTYzMjQ4.k-V_c6x3VVmGMV8CBoxQ4c-pDxUPDKVaYr0ibScfYmUg.cLZNNBPSqZJiRrMTRtX9PTyn6IMS0k4v5o2tp8Y4_4Qg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;performance-test&quot;&gt;&lt;/p&gt;
&lt;p&gt;arther =&gt; are there, n tickets =&gt; any tickets 등 꽤나 말이 되도록 틀린 것을 볼 수 있다.&lt;br&gt;
본 논문은 이보다 더 정확한 인식을 위하여 N-gram Language Model을 사용했다고 한다.&lt;/p&gt;
&lt;p&gt;매우 방대한 텍스트 Corpus로 N-gram language model을 학습시켰으며, 해당 언어 모델은 다음 공식에 사용됐다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjNfMTY1/MDAxNTgyMzkwNTcxMjQ0.cLKWNiiavUqrxHaBijz7yBT90bD-7QwqpA9My0qtc6Ug.67kMZDDcCx0HnvPj6hKRbAmAsuXhRhTT1Lihbx-TBKcg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;scoring&quot;&gt;&lt;/p&gt;
&lt;p&gt;여기서 알파, 베타는 설정 가능한 파라미터이다.&lt;br&gt;
본 논문에서는 성능을 향상시키기 위해 빔서치를 사용했는데, 이때 빔 사이즈를 1,000 - 8,000으로 상당히 크게 준 것을 볼 수 있었다.&lt;br&gt;
이후에 나온 논문들을 봤을 때, 빔 사이즈 단위는 기껏 해봐야 수십 정도였는데 본 논문은 상당히 큰 빔 사이즈를 사용한 것을 볼 수 있었다.&lt;/p&gt;
&lt;h2&gt;Optimizations&lt;/h2&gt;
&lt;p&gt;해당 장에서는 어떻게 최적화를 했는지에 대해 설명하고 있다.&lt;br&gt;
주로 빠른 학습을 시키기 위해 어떤 노력을 했는지를 설명했다.&lt;/p&gt;
&lt;h3&gt;Data Parallelism&lt;/h3&gt;
&lt;p&gt;데이터를 효과적으로 처리하기 위해 2-level data parallelism을 사용했다고 한다.&lt;/p&gt;
&lt;p&gt;미니배치 단위로 처리를 했는데, 이때 배치의 크기를 GPU 메모리 한계까지 사용했다고 한다.&lt;/p&gt;
&lt;p&gt;또한, 학습을 빨리하기 위해 NMT와 같은 Text-NLP에서 많이 사용되는, 길이 순으로 정렬해서 비슷한 길이끼리 배치로 묶었다고 한다. 이렇게 비슷한 길이끼리 배치로 묶게 되면, 배치 안에서 Max Length를 맞추기 위해 PAD token을 최소화 할 수 있다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;p&gt;본 논문은 음성 인식의 기반을 다진 논문인지라 논문에 소개된 대부분의 내용이 음성인식 튜토리얼 내용과 비슷했습니다.&lt;br&gt;
해당 논문 리뷰는 여기까지만 하겠습니다.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Listen, Attend and Spell Paper Review]]></title><description><![CDATA[「Listen, Attend and Spell」 Review title https://arxiv.org/abs/1508.01211  (William Chan et al. 2015)  Introduction 어텐션 기반 Seq2seq…]]></description><link>https://gatsby-casper.netlify.com/las/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/las/</guid><pubDate>Fri, 20 Sep 2019 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;「Listen, Attend and Spell」 Review&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMTY4/MDAxNTgyMTI0ODQ4MTQ4.atLsczYj39_WahxOcLp2eQAvAwbO_uFY3s57CwVoKwUg.z87Ok0avCEU6v0C7L-ymtGjvjQNcgLwsrs-nVi2p4rwg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;title&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.01211&quot;&gt;https://arxiv.org/abs/1508.01211&lt;/a&gt;  (William Chan et al. 2015)&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;어텐션 기반 Seq2seq 구조를 음성 인식에 적용한 논문이다.&lt;/p&gt;
&lt;p&gt;당시에는 CTC (Connectionist temporal classification) 이 음성 인식 분야를 점유하고 있던 시절이였던 터라, End-to-End 방식으로 본 논문 모델과 같은 성능을 낸 것은 굉장히 혁명적인 일이였다고 한다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMjQ2/MDAxNTgyMTI1Mjk5Njg0.9S4lvnYbptl5VZvmn2ju2k0Rlq0bjaebYS_oRGdyLfEg.1rqJhW8PZ_noH55AS2RCsDVYXWmDO3qwMl9Q41ZXqlsg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;end-to-end&quot;&gt;&lt;/p&gt;
&lt;p&gt;본 논문 이후 Speech 분야는 CTC와 LAS로 나뉜다고 한다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;모델의 전체적인 구조는 Listener (encoder) 와 Speller (decoder) 로 이루어져 있다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfNDEg/MDAxNTgyMTI1MDY0MjAz.I1hSxVSWm_YNA1c9EFqe6jetEsPnEePULmJqPWOi3tkg.P_qGbmj96CmKp6RFaCvY5qhu_c7mu8jkbC4s9_UYycog.PNG.sooftware/image.png?type=w773&quot; alt=&quot;las_model&quot;&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Listener&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfNDcg/MDAxNTgyMTI1NTIyMDI1.g0Hl_gv9eMqMtPNCJ7V3FFgAB4Ki0NEdm9bD3WB5ZOEg.SvRIOEMLXCR-0wikkqIl0J4yHehtYyHodr0PyntyLMwg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;listener&quot;&gt;&lt;/p&gt;
&lt;p&gt;데이터의 피쳐를 입력받는 Encoder&lt;br&gt;
입력 시퀀스 x를 High level feature인 h로 변형하는 역할을 담당한다&lt;br&gt;
(더 의미있는 시퀀스로 변형한다)&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Speller&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMjYw/MDAxNTgyMTI1NTI0OTg2.pg_moBT5kOqM9OsZGVlsJlTbcnm4S_yxICgbjhYu7qcg.rbNb4kqRfZT5d1UbQ5sgq9YXGezYai4pN4O4W1MNrCcg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;speller&quot;&gt;&lt;/p&gt;
&lt;p&gt;리스너가 변형한 High Level feature인 h를 어텐션을 사용하여 문자로 출력한다. (Decoder)&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h3&gt;&lt;strong&gt;pBLSTM&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMTkx/MDAxNTgyMTI1ODA1ODkw.8GOtCZNfdDXZuTO46FXUnHf1Fxis7zv-CSoUT6pCGU8g.mIZEx-VA4qEG0ELa0AzRZQ16vJkBvy1f6Cp8QiH9UnIg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;pBLSTM&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMTE3/MDAxNTgyMTI1NTMwNzg1.TIns05VCjY4QXexRHEg5s4os_ELoGylFCgIHCBdGMI0g.tBiGMeTPxS60YofPiqMItgvdwzOLu6jt__Y_E8BPq1Ig.PNG.sooftware/image.png?type=w773&quot; alt=&quot;pblstm_math&quot;&gt;&lt;/p&gt;
&lt;p&gt;모델의 첫 번째 특징으로는 Pyrimidal Bidirectional LSTM을 사용했다.&lt;br&gt;
이전 레이어의 2i, 2i+1 번째 시퀀스를 Concatenate하여 다음 레이어의 i번째 RNN 셀의 입력으로 넣는 구조이다.&lt;/p&gt;
&lt;p&gt;상대적으로 매우 긴 시퀀스 길이를 가지는 Speech 모델의 단점을 완화 해주는 역할을 한다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMjUx/MDAxNTgyMTI2MDA1NDQy.gdw6LQR1ZZlEm8C5bl9SMqNSiaH8LrK-mU5N9yB1GPQg.uthp2jVUQdBFX5rRLDGoPLyQOp1RpZurRyeulPbMUCog.PNG.sooftware/image.png?type=w773&quot; alt=&quot;blstm_pblstm&quot;&gt;&lt;/p&gt;
&lt;p&gt;위의 그림처럼 기존 BLSTM 구조에 반해, 시퀀스의 길이가 상당히 줄어드는 것을 확인할 수 있다.&lt;/p&gt;
&lt;p&gt;pBLSTM 레이어 1층 당 시퀀스 길이가 반씩 줄어들게 되는데, 본 논문에서는 이러한 레이어를 3개를 둠으로써, 총 시퀀스 길이를 8 분의 1로 줄였다고 한다.&lt;/p&gt;
&lt;p&gt;이러한 시퀀스 길이의 감소는 디코딩 &amp;#x26; 어텐션 과정에서 연산량 감소를 가능하게한다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h3&gt;&lt;strong&gt;Exposure Bias Problem&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Seq2seq에서 티쳐 포싱이라는 개념이 있다.&lt;br&gt;
티쳐 포싱의 개념은 &lt;a href=&quot;https://blog.naver.com/sooftware/221790750668&quot;&gt;이글&lt;/a&gt;을 참고하기를 바랍니다.&lt;/p&gt;
&lt;p&gt;당시에는 티쳐 포싱은 Seq2seq 아키텍쳐에서 디폴트 100%로 사용됐던 것 같다.&lt;br&gt;
티쳐 포싱은 학습을 빠르게 해준다는 장점이 있지만, Exposure Bias Problem이란게 존재한다.&lt;/p&gt;
&lt;p&gt;티쳐 포싱은 학습 시에 레이블을 제공받지만, 실제 추론 시에는 레이블을 제공 받을 수가 없다.&lt;br&gt;
이러한 차이점이 실제 추론과 학습시의 성능에 차이가 있을 수 있다는 점이다.&lt;br&gt;
이를 Exposure Bias Problem이라고 한다.&lt;/p&gt;
&lt;p&gt;본 논문에서는 이러한 문제점을 완화 및 실험해보기 위하여 티쳐 포싱 비율이 100%인 모델과 90%인 모델 2개를 학습시켰다고 한다.&lt;/p&gt;
&lt;p&gt;( 2019년 5월에 나온 「Exposure Bias for Neural Language Generation」논문에서는 이런 노출 편향 문제가 생각만큼 큰 영향을 미치지는 않는다는 연구 결과를 냈다고 한다 )&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h3&gt;&lt;strong&gt;Decoding&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;본 논문에서는 역시 빔서치를 사용했다고 한다. (빔사이즈 = 32)&lt;br&gt;
빔서치에 대한 설명은 &lt;a href=&quot;https://blog.naver.com/sooftware/221809101199&quot;&gt;이글&lt;/a&gt;을 참고하길 바랍니다.&lt;/p&gt;
&lt;p&gt;본 논문에서 설명하기를, 기존 음성 인식 모델들은 모든 빔이 &lt;eos&gt;를 만나고 나면, 가장 높은 점수를 받은 빔을 선택한 후, Dictionary (사전) 을 통해 언어 교정을 하는 방식을 많이 사용했다고 한다.&lt;/p&gt;
&lt;p&gt;하지만, 본 논문에서 실험시에, 이 DIctionary 방식은 별로 필요가 없다고 주장한다.&lt;/p&gt;
&lt;p&gt;본 논문의 모델로 실험해본 결과, 어느 정도 학습 후에는 거의 항상 단어 단위에서는 완벽한 단어를 내놓기 때문에, 이러한 교정 과정이 필요가 없다는 것이다.&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;Rescoring&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;그래서 본 논문에서는 Dictionary 방식이 아닌 Rescoring 방식을 제안한다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMjg5/MDAxNTgyMTI3MDQ3Mzg0.IwmLo-utBNEc-BMIkuECTa6OCU2YwCAN5pLszMvmsYEg.aiJDdfZk9tLz-MokkndcGfBqOXdjnk8T4fXx7oU5ws4g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;rescoring&quot;&gt;&lt;/p&gt;
&lt;p&gt;빔 사이즈 만큼의 후보를 뽑아놓은 후에, 여기에 Language Model을 이용하여 점수를 매긴 뒤, 기존 점수와 LM에서 나온 점수를 적절히 결합하여 새로 점수를 매기는 것이다.&lt;br&gt;
해서, 최종적으로 가장 높은 점수를 받은 빔을 최종 선택지로 사용하는 것이다.&lt;/p&gt;
&lt;h1&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;본 논문에서 진행한 실험 환경은 아래와 같다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMTA3/MDAxNTgyMTI3MjE3ODEx.gsza0dxPeS7OsNdTBGjqqmSpYuIAlDISegj6U2khtiIg.bIp8Ru57DtM58V7q1W3ESzltTbSogWSMnQZyM_SKo5cg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;experiment_environ&quot;&gt;&lt;/p&gt;
&lt;p&gt;위의 환경에서 진행한 실험 결과는 아래와 같다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMzAg/MDAxNTgyMTI3MjI3ODA3.FF88MejgiXadN5VqA68HiSFx8NJ6dQ0RtblcaN6M2uUg.LShKDPYxbrbwcPOKoK32JCvbV1_CHnkJmjyaO-vNWH0g.PNG.sooftware/image.png?type=w773&quot; alt=&quot;result&quot;&gt;&lt;/p&gt;
&lt;p&gt;Language Model을 적용하기 전에는 노이즈가 없는 환경에서는 14.1%의 WER (Word Error Rate), 노이즈가 있는 환경에서는 16.5%의 WER을 기록했다.&lt;/p&gt;
&lt;p&gt;결과를 보면, 100%의 티쳐 포싱 비율을 가진 모델보다, 90%의 티쳐 포싱 비율을 가진 모델이 더 좋은 결과를 기록한 것을 알 수 있다.&lt;/p&gt;
&lt;p&gt;그리고 본 논문에서 제안한 &lt;strong&gt;Beam Search + Language Model&lt;/strong&gt;의 퍼포먼스는 매우 훌륭했다.&lt;/p&gt;
&lt;p&gt;모든 면에서 약 4%의 성능을 개선한 것을 확인할 수 있다.&lt;br&gt;
인식률이 85%가 넘어가는 상황에서의 4%는 엄청난 발전이라고 볼 수 있다.&lt;/p&gt;
&lt;p&gt;본 논문은 위의 결과를 통해 새로 제안한 Rescoring 방식이 의미 있는 결과를 냈다는 것을 검증했다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://postfiles.pstatic.net/MjAyMDAyMjBfMTA5/MDAxNTgyMTI3MjM3NDYx.LR6q0OJM0HSqOotAz38FBPg6rTssjBg_rcEkXqQYrGIg.VGG1b4X6-RQdxp0JhZ-gFb7-Ge7fe4UzwFyPiz4ftbIg.PNG.sooftware/image.png?type=w773&quot; alt=&quot;sota&quot;&gt;&lt;/p&gt;
&lt;p&gt;또한, 본 논문의 모델은 당시 SOTA (State-Of-The-Art) 모델인 CLDNN-HMM 모델과 비교하여 2.3% WER 정도만의 차이를 기록했다.&lt;br&gt;
CTC를 사용하지 않고도 이 정도의 성능을 낼 수 있다는 것을 보여준 셈이다.&lt;/p&gt;
&lt;p&gt;본 논문에서는 SOTA 모델과 자신들의 모델의 차이를 Convolution filter의 유무에 의해 생겼다고 추정하고 있다.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[MFCC (Mel-Frequency Cepstral Coefficient)]]></title><description><![CDATA[MFCC (Mel-Frequency Cepstral Coefficient) ‘Voice Recognition Using MFCC Algorithm’ 논문 참고 MFCC란? 음성인식에서 MFCC, Mel-Spectrogram…]]></description><link>https://gatsby-casper.netlify.com/mfcc/</link><guid isPermaLink="false">https://gatsby-casper.netlify.com/mfcc/</guid><pubDate>Tue, 18 Jun 2019 10:00:00 GMT</pubDate><content:encoded>&lt;h1&gt;MFCC (Mel-Frequency Cepstral Coefficient)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;‘Voice Recognition Using MFCC Algorithm’ 논문 참고&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;MFCC란?&lt;/h2&gt;
&lt;p&gt;음성인식에서 MFCC, Mel-Spectrogram는 빼놓고 얘기할 수 없는 부분이다.&lt;br&gt;
간단히 말하면, MFCC는 ‘음성데이터’를 ‘특징벡터’ (Feature) 화 해주는 알고리즘이다.&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/133935462-acaf3951-da9b-42ad-bc24-4ea96b742736.png&quot;&gt;
&lt;p&gt;머신러닝에서 어떠한 데이터를 벡터화 한다는 것은 곧 학습이 가능하다는 의미이기 때문에 상당히 중요한 부분이라고 할 수 있다.
데이터에서 Feature를 어떤 방법으로 뽑느냐에 따라 모델의 성능이 상당히 좌우될 수 있기 때문에 굉장히 중요하다.&lt;/p&gt;
&lt;p&gt;이러한 MFCC Feature는 파이썬에서는 제공되는 librosa라는 라이브러리를 이용해서 간단하게 뽑아올 수 있다.&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot; data-language=&quot;python&quot;&gt;&lt;pre class=&quot;language-python&quot;&gt;&lt;code class=&quot;language-python&quot;&gt;&lt;span class=&quot;token keyword&quot;&gt;import&lt;/span&gt; librosa

&lt;span class=&quot;token keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;token function&quot;&gt;get_librosa_mfcc&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;path&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; n_mfcc&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;token builtin&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;token punctuation&quot;&gt;:&lt;/span&gt;
    SAMPLE_RATE &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;16000&lt;/span&gt;
    HOP_LENGTH &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;128&lt;/span&gt;
    N_FFT &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;token number&quot;&gt;512&lt;/span&gt;

    signal&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; sr &lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt; librosa&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;core&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;load&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;path&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; SAMPLE_RATE&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;token keyword&quot;&gt;return&lt;/span&gt; librosa&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;feature&lt;span class=&quot;token punctuation&quot;&gt;.&lt;/span&gt;mfcc&lt;span class=&quot;token punctuation&quot;&gt;(&lt;/span&gt;signal&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; sr&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; hop_length&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;HOP_LENGTH&lt;span class=&quot;token punctuation&quot;&gt;,&lt;/span&gt; n_fft&lt;span class=&quot;token operator&quot;&gt;=&lt;/span&gt;N_FFT&lt;span class=&quot;token punctuation&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;위 코드는 음성데이터의 파일 경로를 넘겨받아 해당 음성데이터의 MFCC Feature를 뽑아주는 함수이다.&lt;br&gt;
여기서 SAMPLE_RATE는 음성데이터의 형식에 따라 다를 수 있다.&lt;br&gt;
( Ex MP4 : 44100, PCM, WAV 16000 etc.. )&lt;/p&gt;
&lt;p&gt;밑으로는 MFCC Algorithm이 어떤 식으로 Feature를 뽑는지에 대해 수식 없이 직관적인 내용만으로 설명하겠다.&lt;/p&gt;
&lt;h2&gt;Mel-Scale&lt;/h2&gt;
&lt;p&gt;MFCC를 알기 위해서 먼저 Mel이 뭔지를 알아야 한다. Mel은 사람의 달팽이관을 모티브로 따온 값이라고 생각하면 된다! 기계에게 음성을 인식시키기 전에, 사람은 어떤 식으로 음성을 인식하는지를 살펴보자.&lt;/p&gt;
&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/20150421_279/wyepark_14296057390206BzFo_JPEG/image043.jpg?type=w2&quot;&gt;  
&lt;p&gt;사람은 소리를 달팽이관을 통해 인식한다.&lt;/p&gt;
&lt;p&gt;그럼 달팽이관은 어떤 식으로 소리를 인식할까??&lt;/p&gt;
&lt;p&gt;달팽이관을 똘똘 말려있지만, 실제로 길게 펴서 보면 달팽이관의 각 부분은 각기 다른 진동수(주파수)를 감지한다.&lt;br&gt;
이 달팽이관이 감지하는 진동수를 기반으로 하여 사람은 소리를 인식한다.&lt;/p&gt;
&lt;p&gt;그렇기 때문에 이 주파수(Frequency)를 Feature로 쓰는 것은 어떻게 보면 당연한 얘기이다.
하지만, 달팽이관은 특수한 성질이 있다.&lt;/p&gt;
&lt;p&gt;주파수가 낮은 대역에서는 주파수의 변화를 잘 감지하는데,
주파수가 높은 대역에서는 주파수의 변화를 잘 감지하지 못한다는 것이다.&lt;/p&gt;
&lt;p&gt;예를 들어, 실제로 사람은 2000Hz에서 3000Hz로 변하는 소리는 기가막히게 감지하는데, 12000Hz에서 13000Hz로 변하는 소리는 잘 감지를 하지 못한다.&lt;br&gt;
이 이유를 달팽이관의 구조로 살펴보면, 달팽이관에서 저주파 대역을 감지하는 부분은 굵지만 고주파 대역을 감지하는 부분으로 갈수록 얇아진다&lt;/p&gt;
&lt;p&gt;그렇다면, 특징벡터로 그냥 주파수를 쓰기 보다는 이러한 달팽이관의 특성에 맞춰서 특징을 뽑아주는 것이 더욱 효과적인 피쳐를 뽑는 방법일 것이다.&lt;/p&gt;
&lt;p&gt;그래서 위와 같이 사람 달팽이관 특성을 고려한 값을 Mel-scale이라고 한다.&lt;/p&gt;
&lt;h2&gt;Short-Time Fourier Transform&lt;/h2&gt;
&lt;p&gt;그리고 두 번째로 고려해야 할 사항이 있다.
음성데이터에서 주파수(frequency)를 성분을 뽑아내야 한다면 당연히 Fourier Transform을 해야 할 것이다. 그렇다면 음성데이터에 대해서 전체를 퓨리에 변환을 했다고 생각해보자.&lt;/p&gt;
&lt;p&gt;사람이 발성하는 음성은 그 길이가 천차만별일 것이다.
“안녕하세요”라고 하더라도, 어떤 사람은 1초, 어떤 사람은 3초가 걸릴 수도 있다.
그래서 음성 데이터에서 한 번에 Mel-Scale을 뽑게 되면, 이 천차만별인 길이에 대하여 같은 “안녕하세요”라는 음성이라고 학습시키기는 어려울 것이다.&lt;/p&gt;
&lt;p&gt;위와 같은 문제를 해결하기 위해 음성데이터를 모두 20~40ms로 쪼갠다. 여기서 사람의 음성은 20~40ms 사이에서는 음소(현재 내고 있는 발음)가 바뀔 수 없다는 연구결과들을 기반으로 음소는 해당 시간내에 바뀔 수 없다고 가정한다.&lt;/p&gt;
&lt;p&gt;그래서 MFCC에서는 음성데이터를 모두 20~40ms 단위로 쪼개고, 쪼갠 단위에 대해서 Mel 값을 뽑아서 Feature로 사용하는 것이다.&lt;/p&gt;
&lt;h2&gt;MFCC (Mel Frequency Cepstral Coefficient)&lt;/h2&gt;
&lt;p&gt;위에서까지 MFCC가 어떤 건지에 대해 대략적으로 이해했다면, 이제 MFCC 내부에서 어떤 식으로 동작하는지를 살펴보자.&lt;br&gt;
다음은 MFCC 추출 과정을 잘 설명한 블록 다이어그램이다.&lt;/p&gt;
&lt;p&gt;(출처 : ‘Voice Recognition Using MFCC Algorithm’ 논문)&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/133935469-e3cd70cd-9095-481c-b0c0-224cdb6584c3.png&quot;&gt;  
&lt;p&gt;위의 과정을 하나하나 직관적으로 살펴보자.&lt;/p&gt;
&lt;h3&gt;Pre-Emphasis&lt;/h3&gt;
&lt;p&gt;간단히 말하면 High-pass Filter이다. 사람이 발성 시 몸의 구조 때문에 실제로 낸 소리에서 고주파 성분은 많이 줄어들게 되서 나온다고 한다. (이게 본인이 생각하는 본인 목소리와 다른 사람이 생각하는 본인 목소리가 다른 이유라고 한다) 그래서 먼저 줄어든 고주파 성분을 변조가 강하게 걸리도록 High-pass Filter를 적용해주는 과정이다.&lt;/p&gt;
&lt;h3&gt;Sampling and Windowing&lt;/h3&gt;
&lt;p&gt;Pre-emphasis 된 신호에 대해서 앞에서 언급했던 이유 때문에 신호를 20~40ms 단위의 프레임으로 분할한다. 여기서 주의할 점은, 이 때 프레임을 50%겹치게 분할한다는 것이다. 프레임끼리 서로 뚝뚝 떨어지는 것이 아니라 프레임끼리 연속성을 만들어주기 위해 프레임을 50% 겹치게 분할한다. (물론 겹치는 정도는 조정가능한 파라미터이다)&lt;/p&gt;
&lt;p&gt;여기서 왜 연속성이 필요한지 궁금할 수 있다. 만약 프레임이 서로 뚝뚝 떨어지게 샘플링을 한다면, 프레임과 프레임의 접합 부분에서 순간 변화율이 ∞ (무한대) 가 될 수 있다.   이러한 부분을 방지하기 위한 과정이다.&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/133935493-64bca70f-8966-4035-9bfc-11b0c929eb17.png&quot;&gt;
&lt;p&gt;그리고 이 프레임들에 대해 Window를 각각 적용한다. 보통 Hamming Window를 많이 사용한다. 여기서 각각의 프레임들에 대해서 window를 적용하는 이유는, A frame과 B frame이 서로 연속되지 않는다면, 프레임이 접합하는 부분에서의 주파수 성분이 무한대가 되어버린다. 이러한 일을 방지하기 위해 프레임의 시작점과 끝점을 똑같이 유지해주기 위해서 Hamming Window를 적용한다.(Window 종류는 굉장히 다양한데, Hammin window가 Default라고 생각하면 된다)&lt;/p&gt;
&lt;h3&gt;Fast Fourier Transform&lt;/h3&gt;
&lt;p&gt;각각의 프레임들에 대하여 Fourier Transform을 통하여 주파수 성분을 얻어낸다. 여기 FFT 까지만 적용하더라도 충분히 학습 가능한 피쳐를 뽑을 수 있다. 하지만 사람 몸의 구조를 고려한 Mel-Scale을 적용한 feature가 보통 더 나은 성능을 보이기 때문에 아래의 과정을 진행한다.&lt;/p&gt;
&lt;h3&gt;Mel Filter Bank&lt;/h3&gt;
&lt;p&gt;가장 중요한 부분이다. 각각의 프레임에 대해 얻어낸 주파수들에 대해서 Mel 값을 얻어내기 위한 Filter를 적용한다. 아래의 그림으로 보면 쉽게 이해가 쉬울 듯 하다.&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/133935498-99001844-3bf9-4e76-ba29-5f7bf18552ff.png&quot;&gt;  
&lt;p&gt;앞에서 언급했듯이, 달팽이관의 특성을 고려해서 낮은 주파수에서는 작은 삼각형 Filter를 가지고, 고주파 대역으로 갈수록 넓은 삼각형 Filter를 가진다고 생각하면 된다.&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/133935503-fb449744-9f98-49e7-bc56-07e14f147674.png&quot;&gt;
&lt;p&gt;그래서 위와 같은 삼각형 필터 N개를 모두 적용한 필터를 Mel-filter Bank 라고 부른다. 하여, 퓨리에 변환을 통과한 신호를 위의 Mel-filter Bank를 통과하게 되면 Mel-Spectrogram이라는 피쳐가 뽑히게 된다. 최근에는 뒤의 과정을 거치지 않고 여기까지 구한 Mel-Spectrogram을 사용하는 경우가 많다.&lt;/p&gt;
&lt;img src=&quot;https://user-images.githubusercontent.com/42150335/133935537-132f1ba0-c327-48cf-a334-000403541797.png&quot;&gt;
&lt;h3&gt;Discrete Cosine Transform (DCT) 연산&lt;/h3&gt;
&lt;p&gt;앞에서 나온 Mel-Spectrogram이라는 피쳐에 대해 행렬을 압축해서 표현해주는 DCT 연산을 수행한다. 여기까지 해주면, Output으로 MFCC (Mel-Frequency Cepstral Coefficient)가 나오게 된다. 앞의 Mel-Spectrogram은 주파수끼리 Correlation이 형성되어 있는데, 이러한 상관관계를 De-Correlate해주는 역할 또한 수행한다. 위의 과정을 파이썬 NumPy를 이용해서 구현한 블로그가 있다. 코드를 하나하나 보면서 자세히 이해하고 싶은 분들은 아래의 링크를 추천한다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html&quot;&gt;https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html&lt;/a&gt;&lt;/p&gt;</content:encoded></item></channel></rss>