---
title: LLM Paper Abstract - 2023.12
author:
  - Soohwan Kim
tags:
  - nlp
image: img/llm-abs-202310.jpg
date: 2024-01-05T10:00:00.000Z
draft: false
---
  
# LLM Paper Abstract - 2023.12
  
LLM 관련해서 워낙 많은 논문들이 나와서, 최근에 읽은 논문들에 대해 간단하게 요약한 리스트입니다.  
아래 리스트중에는 가볍게 읽어본 논문들이 포함되어 있어서 요약에 틀린 내용이 있을 수 있습니다.
  
## Abstract

### 12월 4주차

- [**`SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling`**](https://arxiv.org/abs/2312.15166) : Solar를 어떻게 만들었는지에 대한 논문. Depth up-scaling (DUS) 이라고 명명한 기술을 활용했다고 함. 2개의 모델을 처음 세트의 끝과 두번째 세트의 첫 레이어 부분들을 일정 부분 떼어내고 합쳐서 활용했다고 함.

### 12월 3주차

- [**`Mixture of Experts Explained`**](https://huggingface.co/blog/moe) : Mixture of Experts (MoE)에 대해 자세히 설명한 글.

### 12월 2주차

- [**`Efficient Memory Management for Large Language Model Serving with PagedAttention`**](https://arxiv.org/abs/2309.06180) : [vLLM](https://github.com/vllm-project/vllm) 에 사용된 PagedAttention에 대한 논문. OS에서 쓰는 Paging 기법을 참고하여 적용함. 현재 State-Of-The-Art인 FasterTransformer, ORCA와 걑은 수준이라고 함.

- [**`Mixtral of experts`**](https://mistral.ai/news/mixtral-of-experts/) : Mistral 팀에서 sparse Mixture of Experts model (SMoE) 방법으로 내놓은 모델. GPT-4처럼 7B짜리 모델 8개로 구성되어 있음. Feed-forward network가 8개의 그룹 중 하나를 선택해서 추론하며, 총 46.7B 사이즈지만, 실제로 토큰당 사용되는 파라미터는 12.9B이라고 함. 몇 벤치마크서 LLaMA2 70B을 이겼다고..

- [**`Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models`**](https://arxiv.org/abs/2312.06585) : 리워드를 줄 수 있는 태스크에 대해서 RL/Self training을 하는 것으로 성능을 개선하는 방법에 대한 논문.

- [**`Mamba: Linear-Time Sequence Modeling with Selective State Spaces`**](https://arxiv.org/abs/2312.00752) : 트랜스포머 아키텍처의 한계점, 그리고 이를 해결하기 위해 나왔던 수많은 아키텍처의 한계점을 개선하고자 나온 논문. Mamba 3B가 트랜스포머 3B 대비 우수한 성능을 보인다고 함.

### 12월 1주차

- [**`Gemini: A Family of Highly Capable Multimodal Models`**](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf) : 구글 & 딥마인드에서 공개한 **Gemini** 테크니컬 리포트. MMLU 벤치마크에서 32개 중 30개의 SOTA를 달성했으며, GPT-4를 능가했다고 함. 멀티모달 모델이여서 텍스트, 음성, 이미지, 비디오 모두 입력 가능한 형태..
  
- [**`AlphaCode 2 Technical Report`**](https://storage.googleapis.com/deepmind-media/AlphaCode2/AlphaCode2_Tech_Report.pdf) : AlphaCode 2 테크니컬 리포트. 12.07 공개된 Gemini를 이용한 Code Generation 모델. 85%의 참가자들보다 더 나은 실력을 보여줬다고..

- [**`Never Lost in the Middle: Improving Large Language Models via Attention Strengthening Question Answering`**](https://arxiv.org/abs/2311.09198) : 'Lost in the Middle' 현상을 완화하기 위한 방법을 제시한 논문.

- [**`Lost in the Middle: How Language Models Use Long Contexts`**](https://arxiv.org/abs/2307.03172) : QA와 같이 long context가 주어지는 태스크에서 관련 정보가 context의 앞쪽이나 뒤쪽에 있을때는 성능이 좋은데, 'middle'에 있을 때 유독 성능이 떨어지는 현상을 관찰하고 분석한 논문. 이러한 현상을 'Lost in the Middle'이라고 표현

- [**`Long context prompting for Claude 2.1`**](https://www.anthropic.com/index/claude-2-1-prompting) : 긴 문맥에서 관련 내용을 찾아내 답변해야할 때, “Here is the most relevant sentence in the context:“를 그림과 같이 마지막에 추가했더니 성능이 27%에서 98%로 크게 올랐다고 함.
