---
title: 'Large Scale LM (2) Distributed Programming'
author: [Soohwan Kim]
tags: [nlp, parallelism, large-scale, lm]
image: img/big-model2.png
date: '2021-11-22T11:00:00.000Z'
draft: false
---

# Large Scale LM (2) Distributed Programming (ì‘ì„±ì¤‘)

ì´ ìë£ŒëŠ” [[í•´ë‹¹ link]](https://github.com/tunib-ai/large-scale-lm-tutorials) ë¥¼ ì°¸ê³ í•˜ë©° ì œ ì–¸ì–´ë¡œ ì¬ì‘ì„±í•œ ê¸€ì…ë‹ˆë‹¤.  
ì €ì˜ ì¶”ê°€ì ì¸ ë©”ëª¨ë‚˜ ì˜ê²¬ì´ ì‚½ì…ë˜ê±°ë‚˜ ì‚­ì œëœ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤.  
ë” í€„ë¦¬í‹°ê°€ ì¢‹ì€ ìë£ŒëŠ” ìœ„ì˜ ë§í¬ë¥¼ ì°¸ê³ í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤.
  
***
  
Large-Scale ëª¨ë¸ì€ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ë¨¹ê¸° ë•Œë¬¸ì— ì–´ëŠ ì •ë„ ì»¤ì§€ê²Œ ë˜ë©´ í•˜ë‚˜ì˜ GPUì— ì˜¬ë¦´ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. 
Big Model í•™ìŠµì´ ì–´ë ¤ìš´ ì£¼ëœ ì´ìœ ì£ . ê·¸ë˜ì„œ ì´ëŸ° Large-Scale ëª¨ë¸ì˜ ê²½ìš° ì—¬ëŸ¬ëŒ€ì˜ GPUì— ëª¨ë¸ì„ ìª¼ê°œì„œ ì˜¬ë ¤ì•¼ í•©ë‹ˆë‹¤. 
ê·¸ë¦¬ê³  ìª¼ê°œì§„ ëª¨ë¸ì„ ë°›ì€ GPUë“¤ê°„ì— ë„¤íŠ¸ì›Œí¬ë¡œ í†µì‹ ì„ í•˜ë©´ì„œ ê°’ì„ ì£¼ê³  ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ì—¬ëŸ¬ëŒ€ì˜ ì¥ë¹„ë¡œ ë¶„ì‚°ì‹œì¼œì„œ 
ì²˜ë¦¬í•˜ëŠ” ì‘ì—…ì„ ë¶„ì‚°ì²˜ë¦¬ë¼ê³  í•©ë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” PyTorch í”„ë ˆì„ì›Œí¬ë¥¼ ì´ìš©í•œ ë¶„ì‚° í”„ë¡œê·¸ë˜ë° ê¸°ì´ˆì— ëŒ€í•´ì„œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.  
  
## Multi-processing with PyTorch
  
ë¶„ì‚° í”„ë¡œê·¸ë˜ë°ì˜ ì›í™œí•œ ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ PyTorchì˜ Multi-processing ì• í”Œë¦¬ì¼€ì´ì…˜ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼ì„ ë¨¼ì € ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.  
  
### Multi-process Terms
  
- Node: ì»´í“¨í„° í˜¹ì€ ì„œë²„ì™€ ê°™ì€ ì¥ë¹„ë¥¼ ë§í•©ë‹ˆë‹¤. AI ìª½ì—ì„œëŠ” ë³´í†µ GPU ì—¬ëŸ¬ëŒ€ê°€ ë¬¶ì—¬ìˆëŠ” í•˜ë‚˜ì˜ ì»´í“¨í„° or ì„œë²„ë¥¼ ì¹­í•©ë‹ˆë‹¤.
- Global Rank: ì›ë˜ëŠ” í”„ë¡œì„¸ìŠ¤ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì˜ë¯¸ëŠ” ì£¼ë¡œ **GPUì˜ ID**ë¼ê³  ë³´ë©´ ë©ë‹ˆë‹¤.
- Local Rank: ì›ë˜ëŠ” í•œ ë…¸ë“œë‚´ì—ì„œì˜ í”„ë¡œì„¸ìŠ¤ ìš°ì„ ìˆœìœ„ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” **í•œ ë…¸ë“œë‚´ì˜ GPU ID**ë¼ê³  ë³´ë©´ ë©ë‹ˆë‹¤.
- World Size: í”„ë¡œì„¸ìŠ¤ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì£¼ë¡œ GPUì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

<img src="https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/process_terms.png" width="500">  
  
### Multi-process Application ì‹¤í–‰ ë°©ë²•
  
PyTorch Multi-process ì–´í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ë°©ë²•ì€ ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.

1. ìœ¼ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ íŠ¹ì • í•¨ìˆ˜ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
2. PyTorch ëŸ°ì³ê°€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
  
### 1) ì‚¬ìš©ìì˜ ì½”ë“œê°€ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ íŠ¹ì • í•¨ìˆ˜ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
  
<img src="https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/multi_process_1.png" width="500"> 
  
ì¼ë°˜ì ìœ¼ë¡œ `Spawn`ê³¼ `Fork` ë“± ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë¶„ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  
- `Spawn`
  - ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ìì›ì„ ë¬¼ë ¤ì£¼ì§€ ì•Šê³  í•„ìš”í•œ ë§Œí¼ì˜ ìì›ë§Œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ê²Œ ìƒˆë¡œ í• ë‹¹
  - ì†ë„ê°€ ëŠë¦¬ì§€ë§Œ ì•ˆì „í•œ ë°©ì‹
- `Fork`
  - ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì˜ ëª¨ë“  ìì›ì„ ì„œë¸Œ í”„ë¡œì„¸ìŠ¤ì™€ ê³µìœ í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘
  - ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ ìœ„í—˜í•œ ë°©ì‹
    
```python
import torch.multiprocessing as mp


def fn(rank, param1, param2):
    print(f"{param1} {param2} - rank: {rank}")


processes = list()
mp.set_start_method("spawn")

for rank in range(4):
    process = mp.Process(target=fn, args=(rank, "A0", "B1"))
    process.daemon = False
    process.start()
    processes.append(process)

for process in processes:
    process.join()
```
  
```
A0 B1 - rank: 0
A0 B1 - rank: 2
A0 B1 - rank: 3
A0 B1 - rank: 1
```

### 2) PyTorch ëŸ°ì²˜ê°€ ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ê°€ ë˜ì–´ ì‚¬ìš©ì ì½”ë“œ ì „ì²´ë¥¼ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ë¡œ ë¶„ê¸°í•œë‹¤.
  
<img src="https://github.com/tunib-ai/large-scale-lm-tutorials/raw/ca29ff9f945a59abcc3e3f1000c4d83de97973d4/images/multi_process_2.png" width="500">
  
ì´ ë°©ì‹ì€ `python -m torch.distributed.launch --nproc_per_node=n OOO.py`ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰í•´ì¤˜ì•¼ ë™ì‘í•©ë‹ˆë‹¤.  
  
```python
import os

print(f"hello world, {os.environ['RANK']}")
```

```
hello world, 0
hello world, 1
hello world, 2
hello world, 3
```
  
## Distributed Programming with PyTorch
  
#### Concept of Message Passing
  
OS ê³¼ëª©ì—ì„œ ë°°ìš°ëŠ” ê°œë…ì´ì£ . ëª‡ ë…„ ì „ì— OS ê³¼ëª©ì„ ë°°ìš¸ ë•Œ Message Passingì€ ë¶„ì‚° í™˜ê²½ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ëœë‹¤ê³  ë°°ìš´ ê¸°ì–µì´ ìˆìŠµë‹ˆë‹¤. 
Message Passingì´ë€ Shared Memory(ê³µìœ  ë©”ëª¨ë¦¬) ì—†ì´ í”„ë¡œì„¸ìŠ¤ê°„ì— ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. 
íŠ¹ì • íƒœê·¸ê°€ ë‹¬ë¦° ë°ì´í„°ë¥¼ ë„¤íŠ¸ì›Œí¬ì— ë³´ë‚´ë©´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°„ í•´ë‹¹ ë°ì´í„°ë¥¼ ë¦¬ì‹œë¸Œë¥¼ í•˜ë„ë¡ í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. 
ì½”ë“œ ë ˆë²¨ì—ì„œ íŠ¹ì • íƒœê·¸ë¥¼ ì´ìš©í•˜ì—¬ í”„ë¡œê·¸ë˜ë° í•´ë‘ë©´ ì›í•˜ëŠ”ëŒ€ë¡œ ì›í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì— ë°ì´í„°ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
Large-scale ëª¨ë¸ ê°œë°œì‹œ ì´ìš©ë˜ëŠ” ë¶„ì‚° í†µì‹  ì—­ì‹œ ëŒ€ë¶€ë¶„ ì´ëŸ° Message Passing ê¸°ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.  
  
<img src="https://user-images.githubusercontent.com/42150335/147876208-04481ccb-e115-41c4-9722-9639c185c498.png" width="400">  
  
#### MPI (Message Passing Interface)
  
MPIëŠ” Message Passingì— ëŒ€í•œ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. MPIëŠ” Message Passingì— ì‚¬ìš©ë˜ëŠ” ì—¬ëŸ¬ ì—°ì‚° (e.g. broadcast, reduce, scatter, gather, ...) 
ë“±ì´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©° ëŒ€í‘œì ìœ¼ë¡œ OpenMPIë¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.   
  
https://www.open-mpi.org/  
  
#### NCCL & GLOO
  
í•˜ì§€ë§Œ ì‹¤ì œ ì‚¬ìš©ì—ì„œëŠ” openmpië³´ë‹¤ëŠ” ncclì´ë‚˜ gloo ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•©ë‹ˆë‹¤.
  
- NCCL (NVIDIA COllective Communication Library)
  - NVIDIAì—ì„œ ê°œë°œí•œ GPU íŠ¹í™” Message Passing ë¼ì´ë¸ŒëŸ¬ë¦¬ (`nickel`ë¼ê³  ì½ëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤.)
  - NVIDIA GPUì—ì„œ ì‚¬ìš©ì‹œ, ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë¹„í•´ ì›”ë“±íˆ ë¹ ë¥´ë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.
- GLOO (Facebook's Collective Communication Library)
  - Facebookì—ì„œ ê°œë°œëœ Message Passing ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
  - `torch`ì—ì„œ ì£¼ë¡œ CPU ë¶„ì‚° ì²˜ë¦¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
- ì¼ë°˜ì ìœ¼ë¡œëŠ” CPUëŠ” GLOO, GPUëŠ” NCCLì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

#### torch.distributed íŒ¨í‚¤ì§€
  
torch.distributed íŒ¨í‚¤ì§€ëŠ” gloo, nccl, openmpi ë“±ì„ í•˜ì´ë ˆë²¨ì—ì„œ ë˜í•‘í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, 
ì¼ë°˜ì ìœ¼ë¡œëŠ” torch.distributedë¥¼ ì´ìš©í•´ì„œ í”„ë¡œê·¸ë˜ë°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤.  
  
#### Process Group
  
í”„ë¡œì„¸ìŠ¤ê°€ ë§ì€ ê²½ìš°, ê´€ë¦¬í•˜ê¸°ê°€ ì–´ë µìŠµë‹ˆë‹¤. ì´ëŸ´ë•ŒëŠ” ë³´í†µ í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ì„ ë§Œë“¤ì–´ì„œ ê´€ë¦¬ë¥¼ í•©ë‹ˆë‹¤. 
`torch.distributed`ì˜ `init_process_group`ì„ í˜¸ì¶œí•˜ë©´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ê°€ ì†í•œ default groupì´ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. 
  
ì£¼ì˜í•  ì ì€ `init_process_group` í•¨ìˆ˜ëŠ” ë°˜ë“œì‹œ ì„œë¸Œí”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ë©°, ì¶”ê°€ë¡œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë“¤ë§Œ ëª¨ì•„ì„œ 
ê·¸ë£¹ì„ ìƒì„±í•˜ë ¤ë©´ `new_group`ì„ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.  
  
- ì˜ˆì œ 1
  
```python
import os
import torch.distributed as dist

os.environ["RANK"] = "0"
os.environ["LOCAL_RANK"] = "0"
os.environ["WORLD_SIZE"] = "1"

os.environ["MASTER_ADDR"] = "localhost"
os.environ["MASTER_PORT"] = "29500"

dist.init_process_group('nccl', rank=0, wirld_size=1)
process_group = dist.new_group([0])
```
  
- ì˜ˆì œ 2
  
```python
import os
import torch.multiprocessing as mp
import torch.distributed as dist


def fn(rank, world_size):
    dist.init_process_group('nccl', rank=rank, world_size=world_size)
    group = dist.new_group([i for i in range(world_size)])


os.environ["MASTER_ADDR"] = "localhost"
os.environ["MASTER_PORT"] = "29500"
os.environ["WORLD_SIZE"] = "4"

mp.spawn(
  fn=fn,
  args=(4, 1),
  nprocs=4,
  join=True,
  daemon=False,
  start_method="spawn",
)
```
  
ìœ„ ì½”ë“œì˜ ê²½ìš° python3 ***.pyì™€ ê°™ì´ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.  
  
- ì˜ˆì œ 3

```python
import torch.distributed as dist

dist.init_process_group(backend="nccl")
group = dist.new_group([i for i in range(dist.get_world_size())])
```
  
ìœ„ ì½”ë“œëŠ” python3 -m torch.distributed.launch --nproc_per_node=N ***.pyì™€ ê°™ì´ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  
#### P2P Communication (Point to Point)
  
<img src="https://user-images.githubusercontent.com/42150335/147877032-e1439e42-8db2-451a-9098-43063ac914e1.png" width="300">  
  
P2P í†µì‹ ì€ íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ë°ì´í„°ë¥¼ ì „ì†¡í•˜ëŠ” í†µì‹ ì…ë‹ˆë‹¤. torch.distributed íŒ¨í‚¤ì§€ì˜ `send`, `recv` í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í†µì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  
```python
import torch
import torch.distributed as dist

dist.init_process_group("gloo")

if dist.get_rank() == 0:
    tensor = torch.randn(2, 2)
    dist.send(tensor, dst=1)

elif dist.get_rank() == 1:
    tensor = torch.zeros(2, 2)
    print(f"rank 1 before: {tensor}\n")
    dist.recv(tensor, src=0)
    print(f"rank 1 after: {tensor}\n")

else:
    raise RuntimeError("wrong rank")
```
  
`send`, `recv`ëŠ” ë™ê¸°ì ìœ¼ë¡œ í†µì‹ í•©ë‹ˆë‹¤. ë¹„ë™ê¸° ë°©ì‹ (non-blocking)ìœ¼ë¡œ ì‚¬ìš©í•˜ë ¤ë©´ `isend`, `irecv`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. 
ë¹„ë™ê¸° ë°©ì‹ì—ì„œëŠ” `wait()` ë©”ì„œë“œë¥¼ í†µí•´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì˜ í†µì‹ ì´ ëë‚ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° ë’¤ì— ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤. 
ë©€í‹°ìŠ¤ë ˆë”© í”„ë¡œê·¸ë˜ë° í•  ë•Œê°€ ê¸°ì–µë‚˜ë„¤ìš” ğŸ˜…
  
```python
import torch
import torch.distributed as dist

dist.init_process_group("gloo")

if dist.get_rank() == 0:
    tensor = torch.randn(2, 2)
    request = dist.isend(tensor, dst=1)
elif dist.get_rank() == 1:
    tensor = torch.zeros(2, 2)
    request = dist.irecv(tensor, src=0)
else:
    raise RuntimeError("wrong rank")

request.wait()

print(f"rank {dist.get_rank()}: {tensor}")
```
  
### Collective Communication
  
Collective Communicationì€ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ì°¸ì—¬í•˜ëŠ” í†µì‹ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì—°ì‚°ë“¤ì´ ìˆì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ 
ì•„ë˜ 4ê°œì˜ ì—°ì‚°ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.  
  
<img src="https://user-images.githubusercontent.com/42150335/147877159-26b24204-de88-416b-9ad0-5c1061a82c26.png" width="450">
  
ì—¬ê¸° 4ê°œì— ì¶”ê°€ë¡œ `all-reduce`, `all-gather`, `reduce-scatter` ë“±ì˜ ë³µí•© ì—°ì‚°ê³¼ ë™ê¸°í™” ì—°ì‚°ì¸ `barrier`ê¹Œì§€ ì´ 8ê°œ ì—°ì‚°ì— ëŒ€í•´ ì•„ë˜ì—ì„œ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.  
  
#### Broadcast
  
BroadcastëŠ” íŠ¹ì • í”„ë¡œì„¸ìŠ¤ì˜ ë°ì´í„°ë¥¼ ê·¸ë£¹ë‚´ì˜ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì— ë³µì‚¬í•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.  
  
<img src="https://user-images.githubusercontent.com/42150335/147877187-10e16fb5-620f-496f-b52f-04769d47ad69.png" width="400">  
  
`torch.distributed.broadcast`ë¡œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. `broadcast`ëŠ” ìƒí™©ì— ë”°ë¼ì„œ P2P í†µì‹  ìš©ë„ë¡œë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. 
  
- ì°¸ê³  ì˜ˆì œ (deepspeed/runtime/pipe/p2p.py)
  
```python
def send(tensor, dest_stage, async_op=False):
    global _groups
    assert async_op == False, "Doesnt support async_op true"
    src_stage = _grid.get_stage_id()
    _is_valid_send_recv(src_stage, dest_stage)

    dest_rank = _grid.stage_to_global(stage_id=dest_stage)
    if async_op:
        global _async
        op = dist.isend(tensor, dest_rank)
        _async.append(op)
    else:

        if can_send_recv():
            return dist.send(tensor, dest_rank)
        else:
            group = _get_send_recv_group(src_stage, dest_stage)
            src_rank = _grid.stage_to_global(stage_id=src_stage)
            return dist.broadcast(tensor, src_rank, group=group, async_op=async_op)
```
  
#### Reduce
  
ReduceëŠ” ê° í”„ë¡œì„¸ìŠ¤ê°€ ê°€ì§„ ë°ì´í„°ë¡œ íŠ¹ì • ì—°ì‚°ì„ ìˆ˜í–‰í•´ì„œ ì¶œë ¥ì„ í•˜ë‚˜ì˜ ë””ë°”ì´ìŠ¤ë¡œ ëª¨ì•„ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. 
ì£¼ë¡œ sum, max, min ë“±ì˜ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  
  
<img src="https://user-images.githubusercontent.com/42150335/147877296-57e4f5e4-9d50-4424-a674-08e3d5ecc20a.png" width="400">  
  
- Reduce sum ì˜ˆì‹œ
  
```python
import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank
# rank==0 => [[0, 0], [0, 0]]
# rank==1 => [[1, 1], [1, 1]]
# rank==2 => [[2, 2], [2, 2]]
# rank==3 => [[3, 3], [3, 3]]

dist.reduce(tensor, op=torch.distributed.ReduceOp.SUM, dst=0)

if rank == 0:
    print(tensor)

# tensor([[6., 6.],
#         [6., 6.]]
```
  
#### Scatter
  
ScatterëŠ” ì—¬ëŸ¬ elementë¥¼ ìª¼ê°œì„œ ê° deviceì— ë¿Œë ¤ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.  
  
<img src="https://user-images.githubusercontent.com/42150335/147877358-f444a3e4-de35-4bd9-bb1d-9825b505add2.png" width="400">
  
- ì˜ˆì‹œ  
  
```python
import torch
import torch.distributed as dist

dist.init_process_group("gloo")
# ncclì€ scatterë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
rank = dist.get_rank()
torch.cuda.set_device(rank)


output = torch.zeros(1)
print(f"before rank {rank}: {output}\n")

if rank == 0:
    inputs = torch.tensor([10.0, 20.0, 30.0, 40.0])
    inputs = torch.split(inputs, dim=0, split_size_or_sections=1)
    # (tensor([10]), tensor([20]), tensor([30]), tensor([40]))
    dist.scatter(output, scatter_list=list(inputs), src=0)
else:
    dist.scatter(output, src=0)

print(f"after rank {rank}: {output}\n")

# before rank 0: tensor([0.])
# before rank 3: tensor([0.])
# after rank 3: tensor([40.])
# before rank 1: tensor([0.])
# before rank 2: tensor([0.])
# after rank 0: tensor([10.])
# after rank 1: tensor([20.])
# after rank 2: tensor([30.])
```
  
`nccl`ì—ì„œëŠ” scatter ì—°ì‚°ì´ ì§€ì›ë˜ì§€ ì•Šì•„ì„œ ì•„ë˜ ê°™ì€ ë°©ë²•ìœ¼ë¡œ scatter ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  
  
```python
import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

inputs = torch.tensor([10.0, 20.0, 30.0, 40.0])
inputs = torch.split(tensor=inputs, dim=-1, split_size_or_sections=1)
output = inputs[rank].contiguous().to(torch.cuda.current_device())
print(f"after rank {rank}: {output}\n")

# after rank 2: tensor([30.], device='cuda:2')
# after rank 3: tensor([40.], device='cuda:3') 
# after rank 0: tensor([10.], device='cuda:0')
# after rank 1: tensor([20.], device='cuda:1')
```
  
- Megatron-LM Scatter ì˜ˆì‹œ
  
```python
def _split(input_):
    """Split the tensor along its last dimension and keep the
    corresponding slice."""

    world_size = get_tensor_model_parallel_world_size()
    # Bypass the function if we are using only 1 GPU.
    if world_size==1:
        return input_

    # Split along last dimension.
    input_list = split_tensor_along_last_dim(input_, world_size)

    # Note: torch.split does not create contiguous tensors by default.
    rank = get_tensor_model_parallel_rank()
    output = input_list[rank].contiguous()

    return output

class _ScatterToModelParallelRegion(torch.autograd.Function):
    """Split the input and keep only the corresponding chuck to the rank."""

    @staticmethod
    def symbolic(graph, input_):
        return _split(input_)

    @staticmethod
    def forward(ctx, input_):
        return _split(input_)

    @staticmethod
    def backward(ctx, grad_output):
        return _gather(grad_output)
```
  
#### Gather
  
GatherëŠ” ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” í…ì„œë¥¼ í•˜ë‚˜ë¡œ ëª¨ì•„ì£¼ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.  
  
<img src="https://user-images.githubusercontent.com/42150335/147877443-3e479d6f-c7e0-4da4-9f77-723e7e3208a6.png" width="400">
  
- gather ì˜ˆì‹œ
  
```python
import torch
import torch.distributed as dist

dist.init_process_group("gloo")
# ncclì€ gatherë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
rank = dist.get_rank()
torch.cuda.set_device(rank)

input = torch.ones(1) * rank
# rank==0 => [0]
# rank==1 => [1]
# rank==2 => [2]
# rank==3 => [3]

if rank == 0:
    outputs_list = [torch.zeros(1), torch.zeros(1), torch.zeros(1), torch.zeros(1)]
    dist.gather(input, gather_list=outputs_list, dst=0)
    print(outputs_list)
else:
    dist.gather(input, dst=0)

# [tensor([0.]), tensor([1.]), tensor([2.]), tensor([3.])]
```
  
#### All-reduce  
  
ì´ë¦„ ì•ì— Allì´ ë¶™ì€ ì—°ì‚°ë“¤ì€ í•´ë‹¹ ì—°ì‚°ì„ ìˆ˜í–‰í•œ ë’¤, ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ broadcastí•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. 
ì•„ë˜ ê·¸ë¦¼ì€ All-reduceì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.
  
<img src="https://user-images.githubusercontent.com/42150335/147877565-20269bae-5962-4fbb-a392-77999b0812a2.png" width="400">
  
- All-reduce sum ì˜ˆì‹œ

```python
import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

tensor = torch.ones(2, 2).to(torch.cuda.current_device()) * rank
# rank==0 => [[0, 0], [0, 0]]
# rank==1 => [[1, 1], [1, 1]]
# rank==2 => [[2, 2], [2, 2]]
# rank==3 => [[3, 3], [3, 3]]

dist.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)

print(f"rank {rank}: {tensor}\n")

# rank 1: tensor([[6., 6.],
#         [6., 6.]], device='cuda:1')
# rank 2: tensor([[6., 6.],
#         [6., 6.]], device='cuda:2')
# rank 0: tensor([[6., 6.],
#         [6., 6.]], device='cuda:0')
# rank 3: tensor([[6., 6.],
#         [6., 6.]], device='cuda:3')
```
  
#### All-gather
  
All-gatherëŠ” gatherë¥¼ ìˆ˜í–‰í•œ ë’¤, ëª¨ì•„ì§„ ê²°ê³¼ë¥¼ ëª¨ë“  ë””ë°”ì´ìŠ¤ë¡œ ë³µì‚¬í•©ë‹ˆë‹¤. 
All-reduceì™€ ë¹„ìŠ·í•´ë³´ì´ì§€ë§Œ ê²°ê³¼ë¥¼ ë³´ë©´ ë‹¤ë¥¸ ì—°ì‚°ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  
```python
import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

input = torch.ones(1).to(torch.cuda.current_device()) * rank
# rank==0 => [0]
# rank==1 => [1]
# rank==2 => [2]
# rank==3 => [3]

outputs_list = [
    torch.zeros(1, device=torch.device(torch.cuda.current_device())),
    torch.zeros(1, device=torch.device(torch.cuda.current_device())),
    torch.zeros(1, device=torch.device(torch.cuda.current_device())),
    torch.zeros(1, device=torch.device(torch.cuda.current_device())),
]

dist.all_gather(tensor_list=outputs_list, tensor=input)
print(outputs_list)

# [tensor([0.], device='cuda:1'), tensor([1.], device='cuda:1'), tensor([2.], device='cuda:1'), tensor([3.], device='cuda:1')]
# [tensor([0.], device='cuda:0'), tensor([1.], device='cuda:0'), tensor([2.], device='cuda:0'), tensor([3.], device='cuda:0')]
# [tensor([0.], device='cuda:2'), tensor([1.], device='cuda:2'), tensor([2.], device='cuda:2'), tensor([3.], device='cuda:2')]
# [tensor([0.], device='cuda:3'), tensor([1.], device='cuda:3'), tensor([2.], device='cuda:3'), tensor([3.], device='cuda:3')]
```
  
#### Reduce-scatter
  
Reduce scatterëŠ” Reduceë¥¼ ìˆ˜í–‰í•œ ë’¤, ê²°ê³¼ë¥¼ ìª¼ê°œì„œ ë””ë°”ì´ìŠ¤ì— ë°˜í™˜í•©ë‹ˆë‹¤.  
  
<img src="https://user-images.githubusercontent.com/42150335/147877668-57d1728c-1451-4f6a-a37a-6c59bcb42d68.png" width="400">
  
- Reduce scatter ì˜ˆì œ  
  
```python
import torch
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()
torch.cuda.set_device(rank)

input_list = torch.tensor([1, 10, 100, 1000]).to(torch.cuda.current_device()) * rank
input_list = torch.split(input_list, dim=0, split_size_or_sections=1)
# rank==0 => [0, 00, 000, 0000]
# rank==1 => [1, 10, 100, 1000]
# rank==2 => [2, 20, 200, 2000]
# rank==3 => [3, 30, 300, 3000]

output = torch.tensor([0], device=torch.device(torch.cuda.current_device()),)

dist.reduce_scatter(
    output=output,
    input_list=list(input_list),
    op=torch.distributed.ReduceOp.SUM,
)

print(f"rank {rank}: {output}\n")

# rank 0: tensor([6], device='cuda:0')
# rank 2: tensor([600], device='cuda:2')
# rank 1: tensor([60], device='cuda:1')
# rank 3: tensor([6000], device='cuda:3')
```
  
#### Barrier
  
BarrierëŠ” í”„ë¡œì„¸ìŠ¤ ë™ê¸°í™”ë¥¼ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ë¨¼ì € barrierì— ë„ì°©í•œ í”„ë¡œì„¸ìŠ¤ëŠ” ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ í•´ë‹¹ ì§€ì ê¹Œì§€ ì‹¤í–‰ë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤. 
  
```python
import time
import torch.distributed as dist

dist.init_process_group("nccl")
rank = dist.get_rank()

if rank == 0:
    seconds = 0
    while seconds <= 3:
        time.sleep(1)
        seconds += 1
        print(f"rank 0 - seconds: {seconds}\n")

print(f"rank {rank}: no-barrier\n")
dist.barrier()
print(f"rank {rank}: barrier\n")

# rank 2: no-barrier
# rank 1: no-barrier
# rank 3: no-barrier
# rank 0 - seconds: 1
# rank 0 - seconds: 2
# rank 0 - seconds: 3
# rank 0 - seconds: 4
# rank 0: no-barrier
# rank 0: barrier
# rank 1: barrier
# rank 3: barrier
# rank 2: barrier
```