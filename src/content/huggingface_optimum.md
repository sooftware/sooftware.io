---
layout: post
title: 'Sooftware Serving - Huggingface Optimum'
author: [Soohwan Kim]
tags: ['huggingface', 'nlp', 'serving']
image: img/optimum.png
date: '2022-10-05T15:11:55.000Z'
draft: false
---

# Sooftware Serving - Huggingface Optimum  
  
í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ë‚˜ì˜¨ Transformersì˜ Extension ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. ëª©ì ì€ ëª¨ë¸ í•™ìŠµ ë° ì¸í¼ëŸ°ìŠ¤ë¥¼ ë”ìš± ë¹ ë¥´ê²Œ í•´ì£¼ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. 
  
## Exporting Transformers models to ONNX
  
ëª¨ë¸ ì„œë¹™ì„ ìœ„í•´ì„œëŠ” ë§ì´ë“¤ Transformers ëª¨ë¸ì„ ONNXë¡œ ì»¨ë²„íŒ…í•˜ê³¤ í•œë‹¤.  
ONNX(Open Neural Network Exchange)ëŠ” ì‰½ê²Œ ë§í•˜ë©´, Tensorflow, PyTorchì™€ ê°™ì´ ì„œë¡œ ë‹¤ë¥¸ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ í™˜ê²½ì—ì„œ ë§Œë“¤ì–´ì§„ ëª¨ë¸ë“¤ì„ 
ì„œë¡œ í˜¸í™˜í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ì´ë‹¤.  
  
ì›ë˜ Transformers ëª¨ë¸ì„ ONNXë¡œ ëª¨ë¸ë¡œ ì»¨ë²„íŒ…í•˜ëŠ”ê²Œ ë§ˆëƒ¥ ê°„ë‹¨í•˜ì§€ë§Œì€ ì•Šì•˜ëŠ”ë°, ì´ Optimum ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì•„ë˜ ì½”ë“œë¡œ ì‰½ê²Œ ì»¨ë²„íŒ… ê°€ëŠ¥í•´ì¡Œë‹¤. 
ì•„ë§ˆ Optimumì„ ê°€ì¥ ë§ì´ ì“°ê²Œë˜ëŠ” ì´ìœ ì¼ ê²ƒ ê°™ë‹¤.  
  
```python
from optimum.onnxruntime import ORTModelForSequenceClassification

pretrain_model_name_or_path = "pretrain_model_name_or_path"
save_directory = "tmp/onnx/"

# Load a model from transformers and export it to ONNX
ort_model = ORTModelForSequenceClassification.from_pretrained(pretrain_model_name_or_path, from_transformers=True)

# Save the onnx model
ort_model.save_pretrained(save_directory)
```
  
## Quantization
  
Model Quantizationë„ ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ê°€ëŠ¥í•˜ë‹¤! Huggingface ë§Œì„¸!  
  
```python
from optimum.onnxruntime.configuration import AutoQuantizationConfig
from optimum.onnxruntime import ORTQuantizer

# Define the quantization methodology
qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)
quantizer = ORTQuantizer.from_pretrained(ort_model)

# Apply dynamic quantization on the model
quantizer.quantize(save_dir=save_directory, quantization_config=qconfig)
```
  
### Example of how to load an ONNX Runtime model and generate predictions with Optimum ğŸ¤—:  
  
```python
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import pipeline, AutoTokenizer

model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name="model_quantized.onnx")
tokenizer = AutoTokenizer.from_pretrained(save_directory)

cls_pipeline = pipeline("text-classification", model=model, tokenizer=tokenizer)

results = cls_pipeline("I love burritos!")
```
  
## Graph Optimization
  
Graph Optimizationì„ í†µí•´ ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤ë¥¼ ë”ìš± ë¹ ë¥´ê²Œë„ ê°€ëŠ¥í•˜ë‹¤.  
  
```python
from optimum.onnxruntime import ORTModelForSequenceClassification, ORTOptimizer

ort_model = ORTModelForSequenceClassification.from_pretrained(pretrain_model_name_or_path, from_transformers=True)
optimizer = ORTOptimizer.from_pretrained(ort_model)

# Optimize the model
optimizer.optimize(save_dir=save_directory, optimization_config=optimization_config)
```
  
ì´ë ‡ê²Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ê°„ë‹¨í•˜ê²Œ, Quantization, Optimizationì´ ê°€ëŠ¥í•´ì¡Œë‹¤. ì •ë§ ê°€ë­„ì˜ ë‹¨ë¹„ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤.  
  
## Training
  
ì¸í¼ëŸ°ìŠ¤ ìš©ë„ ì™¸ì—ë„, ONNXë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµì—ë„ ì ìš© ê°€ëŠ¥í•˜ë‹¤ê³  í•œë‹¤. ê°œì¸ì ìœ¼ë¡œëŠ” Transformersì—ì„œ ì œê³µí•˜ëŠ” Trainerë¥¼ ì˜ 
ì‚¬ìš©í•˜ì§„ ì•ŠëŠ”ë°, Transformersì˜ Trainerë¥¼ ì‚¬ìš©í•˜ëŠ” ì½”ë“œë¼ë©´ ì ìš©í•´ë³´ê³  í•™ìŠµ ì†ë„ë¥¼ ë¹„êµí•´ë³´ê³  ì‹¶ë‹¤.
  
<img width="440" alt="image" src="https://user-images.githubusercontent.com/42150335/193879406-6b86724b-98de-4bf0-8346-33cd497d82ac.png">

  
### Reference
- Optimum github: https://github.com/huggingface/optimum
- ONNX: https://github.com/onnx/onnx
  
